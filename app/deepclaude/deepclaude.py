"""DeepClaude æœåŠ¡ï¼Œç”¨äºåè°ƒ DeepSeek å’Œ Claude API çš„è°ƒç”¨

ä¸»è¦åŠŸèƒ½ï¼š
1. é›†æˆ DeepSeek å’Œ Claude ä¸¤ä¸ªå¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›
2. æ”¯æŒæµå¼å’Œéæµå¼ä¸¤ç§è¾“å‡ºæ¨¡å¼
3. å®ç° DeepSeek æ¨ç†ç»“æœä½œä¸º Claude è¾“å…¥çš„ä¸²è”è°ƒç”¨
4. æä¾›ç¬¦åˆ OpenAI API æ ¼å¼çš„æ ‡å‡†è¾“å‡º

å·¥ä½œæµç¨‹ï¼š
1. æ¥æ”¶ç”¨æˆ·è¾“å…¥çš„æ¶ˆæ¯å’Œæ¨¡å‹å‚æ•°
2. è°ƒç”¨ DeepSeek è¿›è¡Œæ¨ç†ï¼Œè·å–æ¨ç†è¿‡ç¨‹
3. å°†æ¨ç†ç»“æœä¼ é€’ç»™ Claude è¿›è¡Œå¤„ç†
4. æ•´åˆè¾“å‡ºç»“æœå¹¶è¿”å›ç»™ç”¨æˆ·

æŠ€æœ¯ç‰¹ç‚¹ï¼š
1. ä½¿ç”¨å¼‚æ­¥ç¼–ç¨‹æé«˜å¹¶å‘æ€§èƒ½
2. é‡‡ç”¨é˜Ÿåˆ—æœºåˆ¶å®ç°æ•°æ®æµè½¬
3. æ”¯æŒæµå¼è¾“å‡ºæå‡ç”¨æˆ·ä½“éªŒ
4. å®Œå–„çš„é”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•
"""
import json
import time
import tiktoken
import asyncio
from typing import AsyncGenerator
from app.utils.logger import logger
from app.clients import DeepSeekClient, ClaudeClient, OllamaR1Client
from app.utils.message_processor import MessageProcessor
import aiohttp
import os
from dotenv import load_dotenv

load_dotenv()

class DeepClaude:
    """å¤„ç† DeepSeek å’Œ Claude API çš„æµå¼è¾“å‡ºè¡”æ¥
    
    è¯¥ç±»è´Ÿè´£åè°ƒ DeepSeek å’Œ Claude ä¸¤ä¸ªæ¨¡å‹çš„è°ƒç”¨è¿‡ç¨‹ï¼Œä¸»è¦ç‰¹ç‚¹ï¼š
    1. æ”¯æŒæµå¼å’Œéæµå¼ä¸¤ç§è¾“å‡ºæ¨¡å¼
    2. å®ç° DeepSeek æ¨ç†å’Œ Claude å›ç­”çš„ä¸²è”è°ƒç”¨
    3. æä¾›æ ‡å‡†çš„ OpenAI æ ¼å¼è¾“å‡º
    4. æ”¯æŒå¤šç§ API æä¾›å•†é…ç½®
    
    ä¸»è¦ç»„ä»¶ï¼š
    - DeepSeekå®¢æˆ·ç«¯ï¼šè´Ÿè´£è°ƒç”¨ DeepSeek API è·å–æ¨ç†è¿‡ç¨‹
    - Claudeå®¢æˆ·ç«¯ï¼šè´Ÿè´£è°ƒç”¨ Claude API ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
    - å¼‚æ­¥é˜Ÿåˆ—ï¼šç”¨äºæ•°æ®æµè½¬å’Œä»»åŠ¡åè°ƒ
    - OllamaR1Clientï¼šè´Ÿè´£è°ƒç”¨ OllamaR1 API ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
    - å¼‚æ­¥é˜Ÿåˆ—ï¼šç”¨äºæ•°æ®æµè½¬å’Œä»»åŠ¡åè°ƒ
    
    å·¥ä½œæ¨¡å¼ï¼š
    1. æµå¼æ¨¡å¼ï¼šå®æ—¶è¿”å›æ¨ç†è¿‡ç¨‹å’Œç”Ÿæˆç»“æœ
    2. éæµå¼æ¨¡å¼ï¼šç­‰å¾…å®Œæ•´ç»“æœåä¸€æ¬¡æ€§è¿”å›
    """
    def __init__(self, **kwargs):
        """åˆå§‹åŒ– DeepClaude
        
        å·¥ä½œæµç¨‹:
        1. æ€è€ƒè€…(DeepSeek/Ollama)æä¾›æ€è€ƒè¿‡ç¨‹
        2. å›ç­”è€…(Claude)æ ¹æ®æ€è€ƒè¿‡ç¨‹å’ŒåŸé¢˜ç”Ÿæˆç­”æ¡ˆ
        """
        # 1. åˆå§‹åŒ–å›ç­”è€…(Claude)
        self.claude_client = ClaudeClient(
            api_key=kwargs.get('claude_api_key'),
            api_url=kwargs.get('claude_api_url'),
            provider=kwargs.get('claude_provider')
        )
        
        # 2. é…ç½®æ€è€ƒè€…æ˜ å°„
        self.reasoning_providers = {
            'deepseek': lambda: DeepSeekClient(
                api_key=kwargs.get('deepseek_api_key'),
                api_url=kwargs.get('deepseek_api_url'),
                provider=kwargs.get('deepseek_provider')
            ),
            'ollama': lambda: OllamaR1Client(
                api_url=kwargs.get('ollama_api_url')
            )
        }

    def _get_reasoning_provider(self):
        """è·å–æ€è€ƒè€…å®ä¾‹"""
        provider = os.getenv('REASONING_PROVIDER', 'deepseek').lower()
        
        if provider not in self.reasoning_providers:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨ç†æä¾›è€…: {provider}")
            
        return self.reasoning_providers[provider]()

    async def _handle_stream_response(self, response_queue: asyncio.Queue, 
                                    chat_id: str, created_time: int, model: str) -> AsyncGenerator[bytes, None]:
        """å¤„ç†æµå¼å“åº”"""
        try:
            while True:
                item = await response_queue.get()
                if item is None:
                    break
                    
                content_type, content = item
                if content_type == "reasoning":
                    response = {
                        "id": chat_id,
                        "object": "chat.completion.chunk",
                        "created": created_time,
                        "model": model,
                        "choices": [{
                            "index": 0,
                            "delta": {
                                "role": "assistant",
                                "content": f"ğŸ¤” æ€è€ƒè¿‡ç¨‹:\n{content}\n"
                            }
                        }]
                    }
                elif content_type == "content":
                    response = {
                        "id": chat_id,
                        "object": "chat.completion.chunk",
                        "created": created_time,
                        "model": model,
                        "choices": [{
                            "index": 0,
                            "delta": {
                                "role": "assistant",
                                "content": f"\n\n---\næ€è€ƒå®Œæ¯•ï¼Œå¼€å§‹å›ç­”ï¼š\n\n{content}"
                            }
                        }]
                    }
                yield f"data: {json.dumps(response)}\n\n".encode('utf-8')
        except Exception as e:
            logger.error(f"å¤„ç†æµå¼å“åº”æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
            raise

    async def chat_completions_with_stream(self, messages: list, model_arg: tuple = None, **kwargs):
        """æµå¼å¯¹è¯å®Œæˆ"""
        try:
            # 1. è·å–æ€è€ƒè¿‡ç¨‹
            reasoning_content = []
            provider = self._get_reasoning_provider()
            
            # æ ¹æ®æä¾›è€…ç±»å‹é€‰æ‹©æ­£ç¡®çš„æ¨¡å‹å’Œå‚æ•°
            provider_type = os.getenv('REASONING_PROVIDER', 'deepseek').lower()
            if provider_type == 'ollama':
                model = "deepseek-r1:32b"
                provider_kwargs = {}
            else:
                model = os.getenv('DEEPSEEK_MODEL', 'deepseek-reasoner')
                provider_kwargs = {'model_arg': model_arg} if model_arg else {}
            
            # å…ˆè¾“å‡ºæ€è€ƒæ ‡é¢˜
            yield self._format_stream_response("ğŸ¤” æ€è€ƒè¿‡ç¨‹:\n", **kwargs)
            
            try:
                async for content_type, content in provider.get_reasoning(
                    messages=messages,
                    model=model,
                    **provider_kwargs
                ):
                    if content_type == "reasoning":
                        reasoning_content.append(content)
                        # ç›´æ¥è¾“å‡ºæ€è€ƒå†…å®¹ï¼Œä¸å¸¦æ ‡é¢˜
                        yield self._format_stream_response(content, **kwargs)
            except Exception as e:
                logger.error(f"è·å–æ€è€ƒè¿‡ç¨‹å¤±è´¥: {e}")
            
            # 2. æ„é€ ç»™ Claude çš„è¾“å…¥
            reasoning = "\n".join(reasoning_content) if reasoning_content else "æ— æ³•è·å–æ€è€ƒè¿‡ç¨‹"
            
            # 3. ä½¿ç”¨ Claude ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
            try:
                prompt = self._format_claude_prompt(messages[-1]['content'], reasoning)
                claude_messages = [{"role": "user", "content": prompt}]
                
                # å…ˆè¾“å‡ºå›ç­”æ ‡é¢˜
                yield self._format_stream_response("\n\n---\næœ€ç»ˆç­”æ¡ˆï¼š\n\n", **kwargs)
                
                async for content_type, content in self.claude_client.stream_chat(
                    messages=claude_messages,
                    model=os.getenv('CLAUDE_MODEL', 'claude-3-5-sonnet-20241022'),
                    max_tokens=8192,
                    temperature=0.7,
                    top_p=0.9
                ):
                    if content_type == "answer":
                        # ç›´æ¥è¾“å‡ºå›ç­”å†…å®¹ï¼Œä¸å¸¦æ ‡é¢˜
                        yield self._format_stream_response(content, **kwargs)
            except Exception as e:
                logger.error(f"è·å–Claudeå›ç­”å¤±è´¥: {e}")
                yield self._format_stream_response("âŒ è·å–æœ€ç»ˆç­”æ¡ˆå¤±è´¥ï¼Œè¯·ç¨åé‡è¯•", **kwargs)
        except Exception as e:
            logger.error(f"å¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            yield self._format_stream_response("âŒ æœåŠ¡å‡ºç°é”™è¯¯ï¼Œè¯·ç¨åé‡è¯•", **kwargs)

    def _format_claude_prompt(self, original_question: str, reasoning: str) -> str:
        """æ ¼å¼åŒ–ç»™Claudeçš„æç¤ºè¯"""
        return f"""
åŸå§‹é—®é¢˜:
{original_question}

æ€è€ƒè¿‡ç¨‹:
{reasoning}

è¯·åŸºäºä»¥ä¸Šæ€è€ƒè¿‡ç¨‹å’ŒåŸå§‹é—®é¢˜,ç»™å‡ºè¯¦ç»†çš„ç­”æ¡ˆã€‚è¦æ±‚:
1. åˆ†æ­¥éª¤è¯¦ç»†è§£ç­”
2. ç¡®ä¿ç†è§£é—®é¢˜çš„æ¯ä¸ªéƒ¨åˆ†
3. ç»™å‡ºå®Œæ•´çš„è§£å†³æ–¹æ¡ˆ
"""

    async def chat_completions_without_stream(
        self,
        messages: list,
        model_arg: tuple[float, float, float, float],
        deepseek_model: str = "deepseek-reasoner",
        claude_model: str = "claude-3-5-sonnet-20241022"
    ) -> dict:
        """éæµå¼å¯¹è¯å®Œæˆ
        
        Args:
            messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
            model_arg: æ¨¡å‹å‚æ•°å…ƒç»„
            deepseek_model: DeepSeek æ¨¡å‹åç§°
            claude_model: Claude æ¨¡å‹åç§°
            
        Returns:
            dict: åŒ…å«å›ç­”å†…å®¹çš„å“åº”å­—å…¸
        """
        logger.info("å¼€å§‹å¤„ç†è¯·æ±‚...")
        logger.debug(f"è¾“å…¥æ¶ˆæ¯: {messages}")
        
        # 1. è·å–æ¨ç†å†…å®¹
        logger.info("æ­£åœ¨è·å–æ¨ç†å†…å®¹...")
        try:
            reasoning = await self._get_reasoning_content(
                messages=messages,
                model=deepseek_model,
                model_arg=model_arg
            )
        except Exception as e:
            logger.error(f"è·å–æ¨ç†å†…å®¹å¤±è´¥: {e}")
            reasoning = "æ— æ³•è·å–æ¨ç†å†…å®¹"
        
        logger.debug(f"è·å–åˆ°æ¨ç†å†…å®¹: {reasoning}")
        
        # 2. æ„é€  Claude çš„è¾“å…¥æ¶ˆæ¯
        combined_content = f"""
è¿™æ˜¯æˆ‘è‡ªå·±åŸºäºé—®é¢˜çš„æ€è€ƒè¿‡ç¨‹:\n{reasoning}\n\n
ä¸Šé¢æ˜¯æˆ‘è‡ªå·±çš„æ€è€ƒè¿‡ç¨‹ä¸ä¸€å®šå®Œå…¨æ­£ç¡®è¯·å€Ÿé‰´æ€è€ƒè¿‡ç¨‹å’ŒæœŸä¸­ä½ ä¹Ÿè®¤ä¸ºæ­£ç¡®çš„éƒ¨åˆ†ï¼ˆ1000% æƒé‡ï¼‰
ï¼Œç°åœ¨è¯·ç»™å‡ºè¯¦ç»†å’Œç»†è‡´çš„ç­”æ¡ˆï¼Œä¸è¦çœç•¥æ­¥éª¤å’Œæ­¥éª¤ç»†èŠ‚
ï¼Œè¦åˆ†è§£åŸé¢˜ç¡®ä¿ä½ ç†è§£äº†åŸé¢˜çš„æ¯ä¸ªéƒ¨åˆ†ï¼Œä¹Ÿè¦æŒæ¡æ•´ä½“æ„æ€
ï¼Œæœ€ä½³è´¨é‡ï¼ˆ1000% æƒé‡ï¼‰ï¼Œæœ€è¯¦ç»†è§£ç­”ï¼ˆ1000% æƒé‡ï¼‰ï¼Œä¸è¦å›ç­”å¤ªç®€å•è®©æˆ‘èƒ½å‚è€ƒä¸€æ­¥æ­¥åº”ç”¨ï¼ˆ1000% æƒé‡ï¼‰:"""
        
        claude_messages = [{"role": "user", "content": combined_content}]
        
        # 3. è·å– Claude å›ç­”
        logger.info("æ­£åœ¨è·å– Claude å›ç­”...")
        try:
            async for content_type, content in self.claude_client.stream_chat(
                messages=claude_messages,
                model_arg=model_arg,
                model=claude_model,
                stream=False
            ):
                if content_type == "answer":
                    logger.debug(f"è·å–åˆ° Claude å›ç­”: {content}")
                    return {
                        "content": content,
                        "role": "assistant"
                    }
        except Exception as e:
            logger.error(f"è·å– Claude å›ç­”å¤±è´¥: {e}")
            raise

    async def _get_reasoning_content(self, messages: list, model: str, **kwargs) -> str:
        """è·å–æ¨ç†å†…å®¹
        
        1. é¦–å…ˆå°è¯•ä½¿ç”¨é…ç½®çš„æ¨ç†æä¾›è€…
        2. å¦‚æœå¤±è´¥åˆ™å°è¯•åˆ‡æ¢åˆ°å¤‡ç”¨æä¾›è€…
        """
        try:
            provider = self._get_reasoning_provider()
            reasoning_content = []
            
            async for content_type, content in provider.get_reasoning(
                messages=messages,
                model=model,
                model_arg=kwargs.get('model_arg')  # åªä¼ é€’å¿…è¦çš„å‚æ•°
            ):
                if content_type == "reasoning":
                    reasoning_content.append(content)
                
            return "\n".join(reasoning_content)
        except Exception as e:
            logger.error(f"ä¸»è¦æ¨ç†æä¾›è€…å¤±è´¥: {e}")
            # å¦‚æœé…ç½®äº† Ollama ä½œä¸ºå¤‡ç”¨ï¼Œåˆ™å°è¯•åˆ‡æ¢
            if hasattr(self, 'ollama_api_url'):
                logger.info("å°è¯•åˆ‡æ¢åˆ° Ollama æ¨ç†æä¾›è€…")
                try:
                    provider = OllamaR1Client(self.ollama_api_url)
                    reasoning_content = []
                    async for content_type, content in provider.get_reasoning(
                        messages=messages,
                        model="deepseek-r1:32b"
                    ):
                        if content_type == "reasoning":
                            reasoning_content.append(content)
                    return "\n".join(reasoning_content)
                except Exception as e:
                    logger.error(f"å¤‡ç”¨æ¨ç†æä¾›è€…ä¹Ÿå¤±è´¥: {e}")
                
            return "æ— æ³•è·å–æ¨ç†å†…å®¹"

    async def _retry_operation(self, operation, max_retries=3):
        """é€šç”¨é‡è¯•æœºåˆ¶"""
        for i in range(max_retries):
            try:
                return await operation()
            except Exception as e:
                if i == max_retries - 1:
                    raise
                logger.warning(f"æ“ä½œå¤±è´¥ï¼Œæ­£åœ¨é‡è¯• ({i+1}/{max_retries}): {str(e)}")
                await asyncio.sleep(1 * (i + 1))  # æŒ‡æ•°é€€é¿

    def _validate_model_names(self, deepseek_model: str, claude_model: str):
        """éªŒè¯æ¨¡å‹åç§°çš„æœ‰æ•ˆæ€§"""
        if not deepseek_model or not isinstance(deepseek_model, str):
            raise ValueError("æ— æ•ˆçš„ DeepSeek æ¨¡å‹åç§°")
        
        if not claude_model or not isinstance(claude_model, str):
            raise ValueError("æ— æ•ˆçš„ Claude æ¨¡å‹åç§°")

    def _validate_messages(self, messages: list) -> None:
        """éªŒè¯æ¶ˆæ¯åˆ—è¡¨çš„æœ‰æ•ˆæ€§"""
        if not messages:
            raise ValueError("æ¶ˆæ¯åˆ—è¡¨ä¸èƒ½ä¸ºç©º")
        
        for msg in messages:
            if not isinstance(msg, dict):
                raise ValueError("æ¶ˆæ¯å¿…é¡»æ˜¯å­—å…¸æ ¼å¼")
            if "role" not in msg or "content" not in msg:
                raise ValueError("æ¶ˆæ¯å¿…é¡»åŒ…å« role å’Œ content å­—æ®µ")
            if msg["role"] not in ["user", "assistant", "system"]:
                raise ValueError(f"ä¸æ”¯æŒçš„æ¶ˆæ¯è§’è‰²: {msg['role']}")

    async def _get_reasoning_with_fallback(
        self, 
        messages: list,
        model: str,
        model_arg: tuple[float, float, float, float] = None
    ) -> str:
        """è·å–æ¨ç†å†…å®¹ï¼Œå¸¦æœ‰å¤‡ç”¨æ–¹æ¡ˆ
        
        Args:
            messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
            model: ä½¿ç”¨çš„æ¨¡å‹åç§°
            model_arg: æ¨¡å‹å‚æ•°å…ƒç»„
            
        Returns:
            str: æ¨ç†å†…å®¹
        """
        try:
            provider = self._get_reasoning_provider()
            reasoning_content = []
            
            async for content_type, content in provider.get_reasoning(
                messages=messages,
                model=model,
                model_arg=model_arg
            ):
                if content_type == "reasoning":
                    reasoning_content.append(content)
                
            return "".join(reasoning_content)
        except Exception as e:
            logger.error(f"ä¸»è¦æ¨ç†æä¾›è€…å¤±è´¥: {e}")
            if isinstance(provider, DeepSeekClient):
                logger.info("å°è¯•åˆ‡æ¢åˆ° Ollama æ¨ç†æä¾›è€…")
                provider = OllamaR1Client(self.ollama_api_url)
                # é‡è¯•ä½¿ç”¨ Ollama
                reasoning_content = []
                async for content_type, content in provider.get_reasoning(
                    messages=messages,
                    model="deepseek-r1:32b"
                ):
                    if content_type == "reasoning":
                        reasoning_content.append(content)
                return "".join(reasoning_content)
            raise

    def _validate_config(self):
        """éªŒè¯é…ç½®çš„å®Œæ•´æ€§"""
        provider = os.getenv('REASONING_PROVIDER', 'deepseek').lower()
        
        if provider == 'deepseek':
            if not self.deepseek_api_key:
                raise ValueError("ä½¿ç”¨ DeepSeek æ—¶å¿…é¡»æä¾› API KEY")
            if not self.deepseek_api_url:
                raise ValueError("ä½¿ç”¨ DeepSeek æ—¶å¿…é¡»æä¾› API URL")
        elif provider == 'ollama':
            if not self.ollama_api_url:
                raise ValueError("ä½¿ç”¨ Ollama æ—¶å¿…é¡»æä¾› API URL")

    def _format_stream_response(self, content: str, **kwargs) -> bytes:
        """æ ¼å¼åŒ–æµå¼å“åº”ä¸º OpenAI æ ¼å¼"""
        if not content:  # é˜²æ­¢ç©ºå†…å®¹
            return b""
        
        response = {
            "id": kwargs.get('chat_id', 'chatcmpl-default'),
            "object": "chat.completion.chunk",
            "created": kwargs.get('created_time', int(time.time())),
            "model": kwargs.get('model', 'deepclaude'),
            "choices": [{
                "index": 0,
                "delta": {
                    "role": "assistant",
                    "content": content
                },
                "finish_reason": None
            }]
        }
        return f"data: {json.dumps(response)}\n\n".encode('utf-8')