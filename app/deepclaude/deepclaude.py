"""DeepClaude æœåŠ¡ï¼Œç”¨äºåè°ƒ DeepSeek å’Œ Claude API çš„è°ƒç”¨

ä¸»è¦åŠŸèƒ½ï¼š
1. é›†æˆ DeepSeek å’Œ Claude ä¸¤ä¸ªå¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›
2. æ”¯æŒæµå¼å’Œéæµå¼ä¸¤ç§è¾“å‡ºæ¨¡å¼
3. å®ç° DeepSeek æ¨ç†ç»“æœä½œä¸º Claude è¾“å…¥çš„ä¸²è”è°ƒç”¨
4. æä¾›ç¬¦åˆ OpenAI API æ ¼å¼çš„æ ‡å‡†è¾“å‡º

å·¥ä½œæµç¨‹ï¼š
1. æ¥æ”¶ç”¨æˆ·è¾“å…¥çš„æ¶ˆæ¯å’Œæ¨¡å‹å‚æ•°
2. è°ƒç”¨ DeepSeek è¿›è¡Œæ¨ç†ï¼Œè·å–æ¨ç†è¿‡ç¨‹
3. å°†æ¨ç†ç»“æœä¼ é€’ç»™ Claude è¿›è¡Œå¤„ç†
4. æ•´åˆè¾“å‡ºç»“æœå¹¶è¿”å›ç»™ç”¨æˆ·

æŠ€æœ¯ç‰¹ç‚¹ï¼š
1. ä½¿ç”¨å¼‚æ­¥ç¼–ç¨‹æé«˜å¹¶å‘æ€§èƒ½
2. é‡‡ç”¨é˜Ÿåˆ—æœºåˆ¶å®ç°æ•°æ®æµè½¬
3. æ”¯æŒæµå¼è¾“å‡ºæå‡ç”¨æˆ·ä½“éªŒ
4. å®Œå–„çš„é”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•
"""
import json
import time
import tiktoken
import asyncio
from typing import AsyncGenerator
from app.utils.logger import logger
from app.clients import DeepSeekClient, ClaudeClient, OllamaR1Client
from app.utils.message_processor import MessageProcessor
import aiohttp
import os
from dotenv import load_dotenv

load_dotenv()

class DeepClaude:
    """å¤„ç† DeepSeek å’Œ Claude API çš„æµå¼è¾“å‡ºè¡”æ¥
    
    è¯¥ç±»è´Ÿè´£åè°ƒ DeepSeek å’Œ Claude ä¸¤ä¸ªæ¨¡å‹çš„è°ƒç”¨è¿‡ç¨‹ï¼Œä¸»è¦ç‰¹ç‚¹ï¼š
    1. æ”¯æŒæµå¼å’Œéæµå¼ä¸¤ç§è¾“å‡ºæ¨¡å¼
    2. å®ç° DeepSeek æ¨ç†å’Œ Claude å›ç­”çš„ä¸²è”è°ƒç”¨
    3. æä¾›æ ‡å‡†çš„ OpenAI æ ¼å¼è¾“å‡º
    4. æ”¯æŒå¤šç§ API æä¾›å•†é…ç½®
    
    ä¸»è¦ç»„ä»¶ï¼š
    - DeepSeekå®¢æˆ·ç«¯ï¼šè´Ÿè´£è°ƒç”¨ DeepSeek API è·å–æ¨ç†è¿‡ç¨‹
    - Claudeå®¢æˆ·ç«¯ï¼šè´Ÿè´£è°ƒç”¨ Claude API ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
    - å¼‚æ­¥é˜Ÿåˆ—ï¼šç”¨äºæ•°æ®æµè½¬å’Œä»»åŠ¡åè°ƒ
    - OllamaR1Clientï¼šè´Ÿè´£è°ƒç”¨ OllamaR1 API ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
    - å¼‚æ­¥é˜Ÿåˆ—ï¼šç”¨äºæ•°æ®æµè½¬å’Œä»»åŠ¡åè°ƒ
    
    å·¥ä½œæ¨¡å¼ï¼š
    1. æµå¼æ¨¡å¼ï¼šå®æ—¶è¿”å›æ¨ç†è¿‡ç¨‹å’Œç”Ÿæˆç»“æœ
    2. éæµå¼æ¨¡å¼ï¼šç­‰å¾…å®Œæ•´ç»“æœåä¸€æ¬¡æ€§è¿”å›
    """
    def __init__(self, **kwargs):
        """åˆå§‹åŒ– DeepClaude
        
        å·¥ä½œæµç¨‹:
        1. æ€è€ƒè€…(DeepSeek/Ollama)æä¾›æ€è€ƒè¿‡ç¨‹
        2. å›ç­”è€…(Claude)æ ¹æ®æ€è€ƒè¿‡ç¨‹å’ŒåŸé¢˜ç”Ÿæˆç­”æ¡ˆ
        """
        # ä¿å­˜é…ç½®å‚æ•°ï¼Œç”¨äºéªŒè¯
        self.deepseek_api_key = kwargs.get('deepseek_api_key')
        self.deepseek_api_url = kwargs.get('deepseek_api_url')
        self.ollama_api_url = kwargs.get('ollama_api_url')
        
        # 1. åˆå§‹åŒ–å›ç­”è€…(Claude)
        self.claude_client = ClaudeClient(
            api_key=kwargs.get('claude_api_key'),
            api_url=kwargs.get('claude_api_url'),
            provider=kwargs.get('claude_provider')
        )
        
        # ä¿å­˜providerå±æ€§ï¼Œç”¨äºæ¨¡å‹é€‰æ‹©
        self.provider = kwargs.get('deepseek_provider', 'deepseek')
        
        # 2. é…ç½®æ€è€ƒè€…æ˜ å°„
        self.reasoning_providers = {
            'deepseek': lambda: DeepSeekClient(
                api_key=kwargs.get('deepseek_api_key'),
                api_url=kwargs.get('deepseek_api_url'),
                provider=kwargs.get('deepseek_provider')
            ),
            'ollama': lambda: OllamaR1Client(
                api_url=kwargs.get('ollama_api_url')
            ),
            'siliconflow': lambda: DeepSeekClient(
                api_key=kwargs.get('deepseek_api_key'),
                api_url=kwargs.get('deepseek_api_url'),
                provider='siliconflow'
            ),
            'nvidia': lambda: DeepSeekClient(
                api_key=kwargs.get('deepseek_api_key'),
                api_url=kwargs.get('deepseek_api_url'),
                provider='nvidia'
            )
        }
        
        # 3. æ¨ç†æå–é…ç½®
        self.min_reasoning_chars = int(os.getenv('MIN_REASONING_CHARS', '50'))
        self.max_retries = int(os.getenv('REASONING_MAX_RETRIES', '2'))
        self.reasoning_modes = os.getenv('REASONING_MODE_SEQUENCE', 'auto,think_tags,early_content,any_content').split(',')

    def _get_reasoning_provider(self):
        """è·å–æ€è€ƒè€…å®ä¾‹"""
        provider = os.getenv('REASONING_PROVIDER', 'deepseek').lower()
        
        if provider not in self.reasoning_providers:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨ç†æä¾›è€…: {provider}")
            
        return self.reasoning_providers[provider]()

    async def _handle_stream_response(self, response_queue: asyncio.Queue, 
                                    chat_id: str, created_time: int, model: str) -> AsyncGenerator[bytes, None]:
        """å¤„ç†æµå¼å“åº”"""
        try:
            while True:
                item = await response_queue.get()
                if item is None:
                    break
                    
                content_type, content = item
                if content_type == "reasoning":
                    response = {
                        "id": chat_id,
                        "object": "chat.completion.chunk",
                        "created": created_time,
                        "model": model,
                        "is_reasoning": True,  # æ·»åŠ é¡¶å±‚æ ‡è®°
                        "choices": [{
                            "index": 0,
                            "delta": {
                                "role": "assistant",
                                "content": f"ğŸ¤” æ€è€ƒè¿‡ç¨‹:\n{content}\n",
                                "reasoning": True  # åœ¨deltaä¸­æ·»åŠ æ ‡è®°
                            }
                        }]
                    }
                elif content_type == "content":
                    response = {
                        "id": chat_id,
                        "object": "chat.completion.chunk",
                        "created": created_time,
                        "model": model,
                        "choices": [{
                            "index": 0,
                            "delta": {
                                "role": "assistant",
                                "content": f"\n\n---\næ€è€ƒå®Œæ¯•ï¼Œå¼€å§‹å›ç­”ï¼š\n\n{content}"
                            }
                        }]
                    }
                yield f"data: {json.dumps(response)}\n\n".encode('utf-8')
        except Exception as e:
            logger.error(f"å¤„ç†æµå¼å“åº”æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
            raise

    async def _handle_api_error(self, e: Exception) -> str:
        """å¤„ç† API é”™è¯¯"""
        if isinstance(e, aiohttp.ClientError):
            return "ç½‘ç»œè¿æ¥é”™è¯¯ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥"
        elif isinstance(e, asyncio.TimeoutError):
            return "è¯·æ±‚è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•"
        elif isinstance(e, ValueError):
            return f"å‚æ•°é”™è¯¯: {str(e)}"
        else:
            return f"æœªçŸ¥é”™è¯¯: {str(e)}"

    async def chat_completions_with_stream(self, messages: list, **kwargs):
        try:
            logger.info("å¼€å§‹æµå¼å¤„ç†è¯·æ±‚...")
            
            # è·å–æ€è€ƒè€…å®ä¾‹
            provider = self._get_reasoning_provider()
            
            # ç”¨äºæ”¶é›†æ€è€ƒå†…å®¹
            reasoning_content = []
            thought_complete = False
            
            # è®°å½•å‚æ•°ä¿¡æ¯ï¼Œä¾¿äºè°ƒè¯•
            logger.info(f"æ€è€ƒè€…æä¾›å•†: {self.provider}")
            logger.info(f"æ€è€ƒæ¨¡å¼: {os.getenv('DEEPSEEK_REASONING_MODE', 'auto')}")
            
            # 1. æ€è€ƒé˜¶æ®µ - ç›´æ¥è½¬å‘ token å¹¶æ”¶é›†å†…å®¹
            try:
                # è·å–æ¨ç†å†…å®¹å¹¶è®¾ç½®é‡è¯•é€»è¾‘
                reasoning_success = False
                is_first_reasoning = True  # æ–°å¢æ ‡è®°ï¼Œè¡¨ç¤ºæ˜¯å¦æ˜¯é¦–æ¬¡å‘é€æ€è€ƒå†…å®¹
                
                # é¦–å…ˆå‘å‰ç«¯å‘é€å¼€å§‹æ€è€ƒçš„æç¤º
                yield self._format_stream_response(
                    "å¼€å§‹æ€è€ƒé—®é¢˜...",
                    content_type="reasoning",
                    is_first_thought=True,  # æ ‡è®°è¿™æ˜¯é¦–ä¸ªæ€è€ƒå†…å®¹
                    **kwargs
                )
                is_first_reasoning = False  # å‘é€å®Œé¦–ä¸ªæç¤ºåè®¾ä¸ºFalse
                
                # éå†ä¸åŒçš„æ¨ç†æ¨¡å¼ç›´åˆ°æˆåŠŸ
                for retry_count, reasoning_mode in enumerate(self.reasoning_modes):
                    if reasoning_success:
                        logger.info("æ¨ç†æˆåŠŸï¼Œé€€å‡ºæ¨¡å¼é‡è¯•å¾ªç¯")
                        break
                        
                    # å¦‚æœæ€è€ƒå®Œæˆä¸”å·²æ”¶é›†è¶³å¤Ÿæ¨ç†å†…å®¹ï¼Œç›´æ¥è¿›å…¥å›ç­”é˜¶æ®µ
                    if thought_complete and len("".join(reasoning_content)) > self.min_reasoning_chars:
                        logger.info("æ€è€ƒé˜¶æ®µå·²å®Œæˆï¼Œé€€å‡ºæ‰€æœ‰é‡è¯•")
                        reasoning_success = True
                        break
                        
                    if retry_count > 0:
                        logger.info(f"å°è¯•ä½¿ç”¨ä¸åŒçš„æ¨ç†æ¨¡å¼: {reasoning_mode} (å°è¯• {retry_count+1}/{len(self.reasoning_modes)})")
                        # è®¾ç½®ç¯å¢ƒå˜é‡ä»¥æ›´æ”¹æ¨ç†æ¨¡å¼
                        os.environ["DEEPSEEK_REASONING_MODE"] = reasoning_mode
                        # é‡æ–°åˆå§‹åŒ–æä¾›è€…ï¼Œä»¥åŠ è½½æ–°çš„æ¨ç†æ¨¡å¼
                        provider = self._get_reasoning_provider()
                        
                        # é€šçŸ¥å‰ç«¯æ­£åœ¨åˆ‡æ¢æ¨¡å¼
                        yield self._format_stream_response(
                            f"åˆ‡æ¢æ€è€ƒæ¨¡å¼: {reasoning_mode}...",
                            content_type="reasoning",
                            is_first_thought=False,  # éé¦–æ¬¡æ€è€ƒå†…å®¹
                            **kwargs
                        )
                
                    # å‡†å¤‡æ€è€ƒå‚æ•°
                    thinking_kwargs = self._prepare_thinker_kwargs(kwargs)
                    logger.info(f"ä½¿ç”¨æ€è€ƒæ¨¡å‹: {thinking_kwargs.get('model')}")
                    
                    # è·å–æ¨ç†å†…å®¹
                    try:
                        async for content_type, content in provider.get_reasoning(
                            messages=messages,
                            **thinking_kwargs
                        ):
                            if content_type == "reasoning":
                                # ä¿å­˜æ€è€ƒå†…å®¹
                                reasoning_content.append(content)
                                # å¦‚æœæ”¶é›†äº†è¶³å¤Ÿå¤šçš„æ¨ç†å†…å®¹ï¼Œæ ‡è®°ä¸ºæˆåŠŸ
                                if len("".join(reasoning_content)) > self.min_reasoning_chars:
                                    reasoning_success = True
                                # ç›´æ¥è½¬å‘æ€è€ƒ tokenï¼Œæ˜ç¡®æ ‡è®°ä¸ºæ¨ç†å†…å®¹
                                yield self._format_stream_response(
                                    content, 
                                    content_type="reasoning",
                                    is_first_thought=False,  # éé¦–æ¬¡æ€è€ƒå†…å®¹
                                    **kwargs
                                )
                            elif content_type == "content":
                                # å¦‚æœæ”¶åˆ°å¸¸è§„å†…å®¹ï¼Œè¯´æ˜æ€è€ƒé˜¶æ®µå¯èƒ½å·²ç»“æŸ
                                logger.debug(f"æ”¶åˆ°å¸¸è§„å†…å®¹: {content[:50]}...")
                                thought_complete = True
                                
                                # å¦‚æœè¿˜æ²¡æœ‰è¶³å¤Ÿçš„æ¨ç†å†…å®¹ï¼Œä½†æ”¶åˆ°äº†å¸¸è§„å†…å®¹ï¼Œå¯ä»¥å°†å…¶è½¬åŒ–ä¸ºæ¨ç†å†…å®¹
                                if not reasoning_success and reasoning_mode in ['early_content', 'any_content']:
                                    logger.info("å°†å¸¸è§„å†…å®¹è½¬åŒ–ä¸ºæ¨ç†å†…å®¹")
                                    reasoning_content.append(f"åˆ†æ: {content}")
                                    yield self._format_stream_response(
                                        f"åˆ†æ: {content}", 
                                        content_type="reasoning",
                                        is_first_thought=False,  # éé¦–æ¬¡æ€è€ƒå†…å®¹
                                        **kwargs
                                    )
                                    
                                # é‡è¦: å¦‚æœå·²æ”¶é›†è¶³å¤Ÿçš„æ¨ç†å†…å®¹æˆ–å¤„äºç‰¹å®šæ¨¡å¼ï¼Œåˆ™é€€å‡ºå¾ªç¯
                                if len("".join(reasoning_content)) > self.min_reasoning_chars or reasoning_mode in ['early_content', 'any_content']:
                                    logger.info("æ”¶åˆ°å¸¸è§„å†…å®¹ä¸”å·²æ”¶é›†è¶³å¤Ÿæ¨ç†å†…å®¹ï¼Œç»ˆæ­¢æ¨ç†è¿‡ç¨‹")
                                    reasoning_success = True
                                    break
                    except Exception as reasoning_e:
                        logger.error(f"ä½¿ç”¨æ¨¡å¼ {reasoning_mode} è·å–æ¨ç†å†…å®¹æ—¶å‘ç”Ÿé”™è¯¯: {reasoning_e}")
                        # é€šçŸ¥å‰ç«¯å½“å‰æ¨¡å¼å¤±è´¥
                        yield self._format_stream_response(
                            f"æ€è€ƒæ¨¡å¼ {reasoning_mode} å¤±è´¥ï¼Œå°è¯•å…¶ä»–æ–¹å¼...",
                            content_type="reasoning",
                            is_first_thought=False,  # éé¦–æ¬¡æ€è€ƒå†…å®¹
                            **kwargs
                        )
                        continue
                
                logger.info(f"æ€è€ƒè¿‡ç¨‹{'æˆåŠŸ' if reasoning_success else 'å¤±è´¥'}ï¼Œå…±æ”¶é›† {len(reasoning_content)} ä¸ªæ€è€ƒç‰‡æ®µ")
            except Exception as e:
                logger.error(f"æ€è€ƒé˜¶æ®µå‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
                # è®°å½•é”™è¯¯ä½†ç»§ç»­å°è¯•ä½¿ç”¨å·²æ”¶é›†çš„å†…å®¹
                yield self._format_stream_response(
                    f"æ€è€ƒè¿‡ç¨‹å‡ºé”™: {str(e)}ï¼Œå°è¯•ç»§ç»­...",
                    content_type="reasoning",
                    is_first_thought=False,  # éé¦–æ¬¡æ€è€ƒå†…å®¹
                    **kwargs
                )
                
            # ç¡®ä¿æ€è€ƒå†…å®¹ä¸ä¸ºç©ºä¸”æœ‰è¶³å¤Ÿçš„å†…å®¹
            if not reasoning_content or len("".join(reasoning_content)) < self.min_reasoning_chars:
                logger.warning(f"æœªè·å–åˆ°è¶³å¤Ÿçš„æ€è€ƒå†…å®¹ï¼Œå½“å‰å†…å®¹é•¿åº¦: {len(''.join(reasoning_content))}")
                
                # å¦‚æœæ¥è¿‘ä½†ä¸æ»¡è¶³æœ€å°éœ€æ±‚ï¼Œä»ç„¶ä½¿ç”¨å®ƒ
                if not reasoning_content or len("".join(reasoning_content)) < self.min_reasoning_chars // 2:
                    logger.warning("æœªè·å–åˆ°æœ‰æ•ˆæ€è€ƒå†…å®¹ï¼Œä½¿ç”¨åŸå§‹é—®é¢˜ä½œä¸ºæ›¿ä»£")
                    message_content = messages[-1]['content'] if messages and isinstance(messages[-1], dict) and 'content' in messages[-1] else "æœªèƒ½è·å–é—®é¢˜å†…å®¹"
                    reasoning_content = [f"é—®é¢˜åˆ†æï¼š{message_content}"]
                    # ä¹Ÿå‘ç”¨æˆ·å‘é€æç¤ºï¼Œæ˜ç¡®æ ‡è®°ä¸ºæ¨ç†å†…å®¹
                    yield self._format_stream_response(
                        "æ— æ³•è·å–æ€è€ƒè¿‡ç¨‹ï¼Œå°†ç›´æ¥å›ç­”é—®é¢˜",
                        content_type="reasoning",
                        is_first_thought=True,  # è¿™æ˜¯æ–°çš„æ€è€ƒè¿‡ç¨‹çš„å¼€å§‹
                        **kwargs
                    )
            
            # è¿›å…¥å›ç­”é˜¶æ®µå‰å‘é€åˆ†éš”ç¬¦
            yield self._format_stream_response(
                "\n\n---\næ€è€ƒå®Œæ¯•ï¼Œå¼€å§‹å›ç­”ï¼š\n\n",
                content_type="separator",
                is_first_thought=False,  # éæ€è€ƒå†…å®¹
                **kwargs
            )
            
            # 2. å›ç­”é˜¶æ®µ - ä½¿ç”¨æ ¼å¼åŒ–çš„ prompt å¹¶è½¬å‘ token
            full_reasoning = "\n".join(reasoning_content)
            if 'content' in messages[-1]:
                original_question = messages[-1]['content']
            else:
                logger.warning("æ— æ³•ä»æ¶ˆæ¯ä¸­è·å–é—®é¢˜å†…å®¹")
                original_question = "æœªæä¾›é—®é¢˜å†…å®¹"
                
            prompt = self._format_claude_prompt(
                original_question,
                full_reasoning
            )
            
            logger.debug(f"å‘é€ç»™Claudeçš„æç¤ºè¯: {prompt[:500]}...")
            
            try:
                answer_begun = False
                async for content_type, content in self.claude_client.stream_chat(
                    messages=[{"role": "user", "content": prompt}],
                    **self._prepare_answerer_kwargs(kwargs)
                ):
                    if content_type == "content" and content:
                        if not answer_begun and content.strip():
                            # æ ‡è®°å›ç­”å¼€å§‹
                            answer_begun = True
                            
                        # è½¬å‘å›ç­” tokenï¼Œæ˜ç¡®æ ‡è®°ä¸ºæ™®é€šå†…å®¹
                        yield self._format_stream_response(
                            content,
                            content_type="content",
                            is_first_thought=False,  # éæ€è€ƒå†…å®¹
                            **kwargs
                        )
            except Exception as e:
                logger.error(f"å›ç­”é˜¶æ®µå‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
                yield self._format_stream_response(
                    f"\n\nâš ï¸ è·å–å›ç­”æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}",
                    content_type="error",
                    is_first_thought=False,  # éæ€è€ƒå†…å®¹
                    **kwargs
                )
                
        except Exception as e:
            error_msg = await self._handle_api_error(e)
            logger.error(f"æµå¼å¤„ç†é”™è¯¯: {error_msg}", exc_info=True)
            yield self._format_stream_response(
                f"é”™è¯¯: {error_msg}", 
                content_type="error",
                is_first_thought=False,  # éæ€è€ƒå†…å®¹
                **kwargs
            )

    def _prepare_thinker_kwargs(self, kwargs: dict) -> dict:
        """å‡†å¤‡æ€è€ƒè€…å‚æ•°"""
        provider_type = os.getenv('REASONING_PROVIDER', 'deepseek').lower()
        
        if provider_type == 'ollama':
            model = "deepseek-r1:32b"
        else:
            # ä¸å†ä½¿ç”¨kwargsä¸­ä¼ å…¥çš„modelå‚æ•°ï¼Œä»¥é¿å…ä½¿ç”¨ä¸å…¼å®¹çš„æ¨¡å‹åç§°
            # è€Œæ˜¯ä½¿ç”¨ç¯å¢ƒå˜é‡æˆ–é»˜è®¤çš„DeepSeekæ¨¡å‹åç§°
            model = os.getenv('DEEPSEEK_MODEL', 'deepseek-reasoner')
            
            # æ ¹æ®providerè¿›è¡Œç‰¹å®šå¤„ç†
            if self.provider == 'deepseek':
                model = 'deepseek-reasoner'  # ä½¿ç”¨ç¡®å®šå¯ç”¨çš„æ¨¡å‹
            elif self.provider == 'siliconflow':
                model = 'deepseek-ai/DeepSeek-R1'
            elif self.provider == 'nvidia':
                model = 'deepseek-ai/deepseek-r1'
            
        return {
            'model': model,
            'temperature': kwargs.get('temperature', 0.7),
            'top_p': kwargs.get('top_p', 0.9)
        }
        
    def _prepare_answerer_kwargs(self, kwargs: dict) -> dict:
        """å‡†å¤‡å›ç­”è€…å‚æ•°"""
        return {
            'model': 'claude-3-5-sonnet-20241022',
            'temperature': kwargs.get('temperature', 0.7),
            'top_p': kwargs.get('top_p', 0.9)
        }

    def _chunk_content(self, content: str, chunk_size: int = 3) -> list[str]:
        """å°†å†…å®¹åˆ†å‰²æˆå°å—ä»¥å®ç°æ›´ç»†ç²’åº¦çš„æµå¼è¾“å‡º
        
        Args:
            content: è¦åˆ†å‰²çš„å†…å®¹
            chunk_size: æ¯ä¸ªå—çš„å¤§å°ï¼Œé»˜è®¤ä¸º3ä¸ªå­—ç¬¦
            
        Returns:
            list[str]: åˆ†å‰²åçš„å†…å®¹å—åˆ—è¡¨
        """
        return [content[i:i+chunk_size] for i in range(0, len(content), chunk_size)]

    def _format_claude_prompt(self, original_question: str, reasoning: str) -> str:
        """æ ¼å¼åŒ–ç»™Claudeçš„æç¤ºè¯"""
        return f"""
åŸå§‹é—®é¢˜:
{original_question}

æ€è€ƒè¿‡ç¨‹:
{reasoning}

è¯·åŸºäºä»¥ä¸Šæ€è€ƒè¿‡ç¨‹å’ŒåŸå§‹é—®é¢˜,ç»™å‡ºè¯¦ç»†çš„ç­”æ¡ˆã€‚è¦æ±‚:
1. åˆ†æ­¥éª¤è¯¦ç»†è§£ç­”
2. ç¡®ä¿ç†è§£é—®é¢˜çš„æ¯ä¸ªéƒ¨åˆ†
3. ç»™å‡ºå®Œæ•´çš„è§£å†³æ–¹æ¡ˆ
4. å¦‚æœæ€è€ƒè¿‡ç¨‹æœ‰é”™è¯¯æˆ–ä¸å®Œæ•´ï¼Œè¯·æŒ‡å‡ºå¹¶è¡¥å……æ­£ç¡®çš„è§£ç­”
5. ä¿æŒå›ç­”çš„ä¸“ä¸šæ€§å’Œå‡†ç¡®æ€§
"""

    async def chat_completions_without_stream(
        self,
        messages: list,
        model_arg: tuple[float, float, float, float],
        deepseek_model: str = "deepseek-reasoner",
        claude_model: str = "claude-3-5-sonnet-20241022"
    ) -> dict:
        """éæµå¼å¯¹è¯å®Œæˆ
        
        Args:
            messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
            model_arg: æ¨¡å‹å‚æ•°å…ƒç»„
            deepseek_model: DeepSeek æ¨¡å‹åç§°
            claude_model: Claude æ¨¡å‹åç§°
            
        Returns:
            dict: åŒ…å«å›ç­”å†…å®¹çš„å“åº”å­—å…¸
        """
        logger.info("å¼€å§‹å¤„ç†è¯·æ±‚...")
        logger.debug(f"è¾“å…¥æ¶ˆæ¯: {messages}")
        
        # 1. è·å–æ¨ç†å†…å®¹
        logger.info("æ­£åœ¨è·å–æ¨ç†å†…å®¹...")
        try:
            reasoning = await self._get_reasoning_content(
                messages=messages,
                model=deepseek_model,
                model_arg=model_arg
            )
        except Exception as e:
            logger.error(f"è·å–æ¨ç†å†…å®¹å¤±è´¥: {e}")
            reasoning = "æ— æ³•è·å–æ¨ç†å†…å®¹"
            
            # å°è¯•ä½¿ç”¨ä¸åŒçš„æ¨ç†æ¨¡å¼é‡è¯•
            for reasoning_mode in self.reasoning_modes[1:]:  # è·³è¿‡ç¬¬ä¸€ä¸ªå·²ä½¿ç”¨çš„æ¨¡å¼
                try:
                    logger.info(f"å°è¯•ä½¿ç”¨ä¸åŒçš„æ¨ç†æ¨¡å¼è·å–å†…å®¹: {reasoning_mode}")
                    os.environ["DEEPSEEK_REASONING_MODE"] = reasoning_mode
                    reasoning = await self._get_reasoning_content(
                        messages=messages,
                        model=deepseek_model,
                        model_arg=model_arg
                    )
                    if reasoning and len(reasoning) > self.min_reasoning_chars:
                        logger.info(f"ä½¿ç”¨æ¨ç†æ¨¡å¼ {reasoning_mode} æˆåŠŸè·å–æ¨ç†å†…å®¹")
                        break
                except Exception as retry_e:
                    logger.error(f"ä½¿ç”¨æ¨ç†æ¨¡å¼ {reasoning_mode} é‡è¯•å¤±è´¥: {retry_e}")
        
        logger.debug(f"è·å–åˆ°æ¨ç†å†…å®¹: {reasoning[:min(500, len(reasoning))]}...")
        
        # 2. æ„é€  Claude çš„è¾“å…¥æ¶ˆæ¯
        combined_content = f"""
è¿™æ˜¯æˆ‘è‡ªå·±åŸºäºé—®é¢˜çš„æ€è€ƒè¿‡ç¨‹:\n{reasoning}\n\n
ä¸Šé¢æ˜¯æˆ‘è‡ªå·±çš„æ€è€ƒè¿‡ç¨‹ä¸ä¸€å®šå®Œå…¨æ­£ç¡®è¯·å€Ÿé‰´æ€è€ƒè¿‡ç¨‹å’ŒæœŸä¸­ä½ ä¹Ÿè®¤ä¸ºæ­£ç¡®çš„éƒ¨åˆ†ï¼ˆ1000% æƒé‡ï¼‰
ï¼Œç°åœ¨è¯·ç»™å‡ºè¯¦ç»†å’Œç»†è‡´çš„ç­”æ¡ˆï¼Œä¸è¦çœç•¥æ­¥éª¤å’Œæ­¥éª¤ç»†èŠ‚
ï¼Œè¦åˆ†è§£åŸé¢˜ç¡®ä¿ä½ ç†è§£äº†åŸé¢˜çš„æ¯ä¸ªéƒ¨åˆ†ï¼Œä¹Ÿè¦æŒæ¡æ•´ä½“æ„æ€
ï¼Œæœ€ä½³è´¨é‡ï¼ˆ1000% æƒé‡ï¼‰ï¼Œæœ€è¯¦ç»†è§£ç­”ï¼ˆ1000% æƒé‡ï¼‰ï¼Œä¸è¦å›ç­”å¤ªç®€å•è®©æˆ‘èƒ½å‚è€ƒä¸€æ­¥æ­¥åº”ç”¨ï¼ˆ1000% æƒé‡ï¼‰:"""
        
        claude_messages = [{"role": "user", "content": combined_content}]
        
        # 3. è·å– Claude å›ç­”
        logger.info("æ­£åœ¨è·å– Claude å›ç­”...")
        try:
            full_content = ""
            async for content_type, content in self.claude_client.stream_chat(
                messages=claude_messages,
                model_arg=model_arg,
                model=claude_model,
                stream=False
            ):
                if content_type in ["answer", "content"]:
                    logger.debug(f"è·å–åˆ° Claude å›ç­”: {content}")
                    full_content += content
            
            # è¿”å›å®Œæ•´çš„å›ç­”å†…å®¹
            return {
                "content": full_content,
                "role": "assistant"
            }
        except Exception as e:
            logger.error(f"è·å– Claude å›ç­”å¤±è´¥: {e}")
            raise

    async def _get_reasoning_content(self, messages: list, model: str, **kwargs) -> str:
        """è·å–æ¨ç†å†…å®¹
        
        1. é¦–å…ˆå°è¯•ä½¿ç”¨é…ç½®çš„æ¨ç†æä¾›è€…
        2. å¦‚æœå¤±è´¥åˆ™å°è¯•åˆ‡æ¢åˆ°å¤‡ç”¨æä¾›è€…
        3. æ”¯æŒå¤šç§æ¨ç†æ¨¡å¼é‡è¯•
        """
        try:
            provider = self._get_reasoning_provider()
            reasoning_content = []
            content_received = False
            
            logger.info(f"å¼€å§‹è·å–æ€è€ƒå†…å®¹ï¼Œæ¨¡å‹: {model}, æ¨ç†æ¨¡å¼: {os.getenv('DEEPSEEK_REASONING_MODE', 'auto')}")
            
            async for content_type, content in provider.get_reasoning(
                messages=messages,
                model=model,
                model_arg=kwargs.get('model_arg')  # åªä¼ é€’å¿…è¦çš„å‚æ•°
            ):
                if content_type == "reasoning":
                    reasoning_content.append(content)
                    logger.debug(f"æ”¶åˆ°æ¨ç†å†…å®¹ï¼Œå½“å‰é•¿åº¦: {len(''.join(reasoning_content))}")
                elif content_type == "content" and not reasoning_content:
                    # å¦‚æœæ²¡æœ‰æ”¶é›†åˆ°æ¨ç†å†…å®¹ï¼Œä½†æ”¶åˆ°äº†å†…å®¹ï¼Œå°†å…¶ä¹Ÿè§†ä¸ºæ¨ç†
                    logger.info("æœªæ”¶é›†åˆ°æ¨ç†å†…å®¹ï¼Œå°†æ™®é€šå†…å®¹è§†ä¸ºæ¨ç†")
                    reasoning_content.append(f"åˆ†æ: {content}")
                    logger.debug(f"æ™®é€šå†…å®¹è½¬ä¸ºæ¨ç†å†…å®¹ï¼Œå½“å‰é•¿åº¦: {len(''.join(reasoning_content))}")
                elif content_type == "content":
                    # è®°å½•æ”¶åˆ°æ™®é€šå†…å®¹ï¼Œè¿™é€šå¸¸è¡¨ç¤ºæ¨ç†é˜¶æ®µç»“æŸ
                    content_received = True
                    logger.info("æ”¶åˆ°æ™®é€šå†…å®¹ï¼Œæ¨ç†é˜¶æ®µå¯èƒ½å·²ç»“æŸ")
                
            result = "\n".join(reasoning_content)
            
            # å¦‚æœå·²æ”¶åˆ°æ™®é€šå†…å®¹ä¸”æ¨ç†å†…å®¹é•¿åº¦è¶³å¤Ÿï¼Œç›´æ¥è¿”å›
            if content_received and len(result) > self.min_reasoning_chars:
                logger.info(f"å·²æ”¶åˆ°æ™®é€šå†…å®¹ä¸”æ¨ç†å†…å®¹é•¿åº¦è¶³å¤Ÿ ({len(result)}å­—ç¬¦)ï¼Œç»“æŸè·å–æ¨ç†")
                return result
            
            # å¦‚æœå†…å®¹ä¸è¶³ï¼Œå°è¯•åˆ‡æ¢æ¨¡å¼é‡è¯•
            if not result or len(result) < self.min_reasoning_chars:
                current_mode = os.getenv('DEEPSEEK_REASONING_MODE', 'auto')
                logger.warning(f"ä½¿ç”¨æ¨¡å¼ {current_mode} è·å–çš„æ¨ç†å†…å®¹ä¸è¶³ï¼Œå°è¯•åˆ‡æ¢æ¨¡å¼")
                
                # å°è¯•ä¸‹ä¸€ä¸ªæ¨ç†æ¨¡å¼
                for reasoning_mode in self.reasoning_modes:
                    if reasoning_mode == current_mode:
                        continue
                    
                    logger.info(f"å°è¯•ä½¿ç”¨æ¨ç†æ¨¡å¼: {reasoning_mode}")
                    os.environ["DEEPSEEK_REASONING_MODE"] = reasoning_mode
                    provider = self._get_reasoning_provider()  # é‡æ–°åˆå§‹åŒ–æä¾›è€…
                    
                    reasoning_content = []
                    async for content_type, content in provider.get_reasoning(
                        messages=messages,
                        model=model,
                        model_arg=kwargs.get('model_arg')
                    ):
                        if content_type == "reasoning":
                            reasoning_content.append(content)
                        elif content_type == "content" and not reasoning_content:
                            reasoning_content.append(f"åˆ†æ: {content}")
                    
                    retry_result = "\n".join(reasoning_content)
                    if retry_result and len(retry_result) > self.min_reasoning_chars:
                        logger.info(f"ä½¿ç”¨æ¨ç†æ¨¡å¼ {reasoning_mode} æˆåŠŸè·å–è¶³å¤Ÿçš„æ¨ç†å†…å®¹")
                        return retry_result
            
            return result or "æ— æ³•è·å–è¶³å¤Ÿçš„æ¨ç†å†…å®¹"
        except Exception as e:
            logger.error(f"ä¸»è¦æ¨ç†æä¾›è€…å¤±è´¥: {e}")
            if isinstance(provider, DeepSeekClient):
                # è·å–å½“å‰æä¾›å•†åç§°
                current_provider = getattr(provider, 'provider', 'unknown')
                logger.info(f"ä» {current_provider} æä¾›å•†åˆ‡æ¢åˆ° Ollama æ¨ç†æä¾›è€…")
                try:
                    provider = OllamaR1Client(api_url=os.getenv('OLLAMA_API_URL'))
                    # é‡è¯•ä½¿ç”¨ Ollama
                    reasoning_content = []
                    async for content_type, content in provider.get_reasoning(
                        messages=messages,
                        model="deepseek-r1:32b"
                    ):
                        if content_type == "reasoning":
                            reasoning_content.append(content)
                    return "\n".join(reasoning_content)
                except Exception as e:
                    logger.error(f"å¤‡ç”¨æ¨ç†æä¾›è€…ä¹Ÿå¤±è´¥: {e}")
                
            return "æ— æ³•è·å–æ¨ç†å†…å®¹"

    async def _retry_operation(self, operation, max_retries=3):
        """é€šç”¨é‡è¯•æœºåˆ¶"""
        for i in range(max_retries):
            try:
                return await operation()
            except Exception as e:
                if i == max_retries - 1:
                    raise
                logger.warning(f"æ“ä½œå¤±è´¥ï¼Œæ­£åœ¨é‡è¯• ({i+1}/{max_retries}): {str(e)}")
                await asyncio.sleep(1 * (i + 1))  # æŒ‡æ•°é€€é¿

    def _validate_model_names(self, deepseek_model: str, claude_model: str):
        """éªŒè¯æ¨¡å‹åç§°çš„æœ‰æ•ˆæ€§"""
        if not deepseek_model or not isinstance(deepseek_model, str):
            raise ValueError("æ— æ•ˆçš„ DeepSeek æ¨¡å‹åç§°")
        
        if not claude_model or not isinstance(claude_model, str):
            raise ValueError("æ— æ•ˆçš„ Claude æ¨¡å‹åç§°")

    def _validate_messages(self, messages: list) -> None:
        """éªŒè¯æ¶ˆæ¯åˆ—è¡¨çš„æœ‰æ•ˆæ€§"""
        if not messages:
            raise ValueError("æ¶ˆæ¯åˆ—è¡¨ä¸èƒ½ä¸ºç©º")
        
        for msg in messages:
            if not isinstance(msg, dict):
                raise ValueError("æ¶ˆæ¯å¿…é¡»æ˜¯å­—å…¸æ ¼å¼")
            if "role" not in msg or "content" not in msg:
                raise ValueError("æ¶ˆæ¯å¿…é¡»åŒ…å« role å’Œ content å­—æ®µ")
            if msg["role"] not in ["user", "assistant", "system"]:
                raise ValueError(f"ä¸æ”¯æŒçš„æ¶ˆæ¯è§’è‰²: {msg['role']}")

    async def _get_reasoning_with_fallback(
        self, 
        messages: list,
        model: str,
        model_arg: tuple[float, float, float, float] = None
    ) -> str:
        """è·å–æ¨ç†å†…å®¹ï¼Œå¸¦æœ‰å¤‡ç”¨æ–¹æ¡ˆ
        
        Args:
            messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
            model: ä½¿ç”¨çš„æ¨¡å‹åç§°
            model_arg: æ¨¡å‹å‚æ•°å…ƒç»„
            
        Returns:
            str: æ¨ç†å†…å®¹
        """
        try:
            provider = self._get_reasoning_provider()
            reasoning_content = []
            
            # å°è¯•ä¸åŒçš„æ¨ç†æ¨¡å¼
            for reasoning_mode in self.reasoning_modes:
                if reasoning_content and len("".join(reasoning_content)) > self.min_reasoning_chars:
                    # å¦‚æœå·²ç»æ”¶é›†åˆ°è¶³å¤Ÿçš„å†…å®¹ï¼Œç»“æŸå¾ªç¯
                    logger.info(f"å·²æ”¶é›†åˆ°è¶³å¤Ÿæ¨ç†å†…å®¹ ({len(''.join(reasoning_content))}å­—ç¬¦)ï¼Œä¸å†å°è¯•å…¶ä»–æ¨¡å¼")
                    break
                    
                logger.info(f"å°è¯•ä½¿ç”¨æ¨ç†æ¨¡å¼: {reasoning_mode}")
                os.environ["DEEPSEEK_REASONING_MODE"] = reasoning_mode
                provider = self._get_reasoning_provider()  # é‡æ–°åˆå§‹åŒ–æä¾›è€…
                
                temp_content = []
                content_received = False  # æ ‡è®°æ˜¯å¦æ”¶åˆ°æ™®é€šå†…å®¹
                
                try:
                    async for content_type, content in provider.get_reasoning(
                        messages=messages,
                        model=model,
                        model_arg=model_arg
                    ):
                        if content_type == "reasoning":
                            temp_content.append(content)
                            logger.debug(f"æ”¶åˆ°æ¨ç†å†…å®¹ï¼Œå½“å‰ä¸´æ—¶å†…å®¹é•¿åº¦: {len(''.join(temp_content))}")
                        elif content_type == "content" and not temp_content and reasoning_mode in ['early_content', 'any_content']:
                            # åœ¨æŸäº›æ¨¡å¼ä¸‹ï¼Œä¹Ÿå°†æ™®é€šå†…å®¹è§†ä¸ºæ¨ç†
                            temp_content.append(f"åˆ†æ: {content}")
                            logger.debug(f"æ™®é€šå†…å®¹è½¬ä¸ºæ¨ç†å†…å®¹ï¼Œå½“å‰ä¸´æ—¶å†…å®¹é•¿åº¦: {len(''.join(temp_content))}")
                        elif content_type == "content":
                            # æ”¶åˆ°æ™®é€šå†…å®¹ï¼Œå¯èƒ½è¡¨ç¤ºæ¨ç†é˜¶æ®µç»“æŸ
                            content_received = True
                            logger.info("æ”¶åˆ°æ™®é€šå†…å®¹ï¼Œæ¨ç†é˜¶æ®µå¯èƒ½å·²ç»“æŸ")
                            
                        # å¦‚æœæ”¶åˆ°æ™®é€šå†…å®¹ä¸”å·²æœ‰è¶³å¤Ÿæ¨ç†å†…å®¹ï¼Œæå‰ç»ˆæ­¢
                        if content_received and len("".join(temp_content)) > self.min_reasoning_chars:
                            logger.info("æ”¶åˆ°æ™®é€šå†…å®¹ä¸”ä¸´æ—¶æ¨ç†å†…å®¹è¶³å¤Ÿï¼Œæå‰ç»“æŸæ¨ç†è·å–")
                            break
                            
                    if temp_content and len("".join(temp_content)) > len("".join(reasoning_content)):
                        # å¦‚æœæœ¬æ¬¡è·å–çš„å†…å®¹æ›´å¤šï¼Œåˆ™æ›´æ–°ç»“æœ
                        reasoning_content = temp_content
                        if content_received:
                            # å¦‚æœå·²æ”¶åˆ°æ™®é€šå†…å®¹ï¼Œè¡¨ç¤ºæ¨ç†é˜¶æ®µå·²å®Œæˆï¼Œä¸å†å°è¯•å…¶ä»–æ¨¡å¼
                            logger.info("æ¨ç†é˜¶æ®µå·²ç»“æŸä¸”å†…å®¹è¶³å¤Ÿï¼Œåœæ­¢å°è¯•å…¶ä»–æ¨¡å¼")
                            break
                except Exception as mode_e:
                    logger.error(f"ä½¿ç”¨æ¨ç†æ¨¡å¼ {reasoning_mode} æ—¶å‘ç”Ÿé”™è¯¯: {mode_e}")
                    continue
            
            return "".join(reasoning_content) or "æ— æ³•è·å–æ¨ç†å†…å®¹"
        except Exception as e:
            logger.error(f"ä¸»è¦æ¨ç†æä¾›è€…å¤±è´¥: {e}")
            if isinstance(provider, DeepSeekClient):
                # è·å–å½“å‰æä¾›å•†åç§°
                current_provider = getattr(provider, 'provider', 'unknown')
                logger.info(f"ä» {current_provider} æä¾›å•†åˆ‡æ¢åˆ° Ollama æ¨ç†æä¾›è€…")
                try:
                    provider = OllamaR1Client(api_url=os.getenv('OLLAMA_API_URL'))
                    # é‡è¯•ä½¿ç”¨ Ollama
                    reasoning_content = []
                    async for content_type, content in provider.get_reasoning(
                        messages=messages,
                        model="deepseek-r1:32b"
                    ):
                        if content_type == "reasoning":
                            reasoning_content.append(content)
                    return "\n".join(reasoning_content)
                except Exception as e:
                    logger.error(f"å¤‡ç”¨æ¨ç†æä¾›è€…ä¹Ÿå¤±è´¥: {e}")
                
            return "æ— æ³•è·å–æ¨ç†å†…å®¹"

    def _validate_config(self):
        """éªŒè¯é…ç½®çš„å®Œæ•´æ€§"""
        provider = os.getenv('REASONING_PROVIDER', 'deepseek').lower()
        
        if provider == 'deepseek':
            if not self.deepseek_api_key:
                raise ValueError("ä½¿ç”¨ DeepSeek æ—¶å¿…é¡»æä¾› API KEY")
            if not self.deepseek_api_url:
                raise ValueError("ä½¿ç”¨ DeepSeek æ—¶å¿…é¡»æä¾› API URL")
        elif provider == 'ollama':
            if not self.ollama_api_url:
                raise ValueError("ä½¿ç”¨ Ollama æ—¶å¿…é¡»æä¾› API URL")
        elif provider == 'siliconflow':
            if not self.deepseek_api_key:
                raise ValueError("ä½¿ç”¨ ç¡…åŸºæµåŠ¨ æ—¶å¿…é¡»æä¾› DeepSeek API KEY")
            if not self.deepseek_api_url:
                raise ValueError("ä½¿ç”¨ ç¡…åŸºæµåŠ¨ æ—¶å¿…é¡»æä¾› DeepSeek API URL")
        elif provider == 'nvidia':
            if not self.deepseek_api_key:
                raise ValueError("ä½¿ç”¨ NVIDIA æ—¶å¿…é¡»æä¾› DeepSeek API KEY")
            if not self.deepseek_api_url:
                raise ValueError("ä½¿ç”¨ NVIDIA æ—¶å¿…é¡»æä¾› DeepSeek API URL")

    def _format_stream_response(self, content: str, content_type: str = "content", **kwargs) -> bytes:
        """æ ¼å¼åŒ–æµå¼å“åº”
        
        Args:
            content: è¦å‘é€çš„å†…å®¹
            content_type: å†…å®¹ç±»å‹ï¼Œå¯ä»¥æ˜¯ "reasoning"ã€"content" æˆ– "separator"
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            bytes: æ ¼å¼åŒ–çš„SSEå“åº”
        """
        # åŸºæœ¬å“åº”ç»“æ„
        response = {
            "id": kwargs.get("chat_id", f"chatcmpl-{int(time.time())}"),
            "object": "chat.completion.chunk",
            "created": kwargs.get("created_time", int(time.time())),
            "model": kwargs.get("model", "deepclaude"),
            "choices": [{
                "index": 0,
                "delta": {
                    "content": content
                }
            }]
        }
        
        # ä¸ºä¸åŒå†…å®¹ç±»å‹æ·»åŠ æ˜æ˜¾æ ‡è®°
        if content_type == "reasoning":
            # æ·»åŠ æ€è€ƒæ ‡è®° - åœ¨deltaä¸­å’Œresponseæ ¹çº§åˆ«éƒ½æ·»åŠ æ ‡è®°
            response["choices"][0]["delta"]["reasoning"] = True
            response["is_reasoning"] = True  # æ ¹çº§åˆ«æ·»åŠ æ ‡è®°ï¼Œæ–¹ä¾¿å‰ç«¯è¯†åˆ«
            
            # åªåœ¨é¦–ä¸ªtokenæ·»åŠ è¡¨æƒ…ç¬¦å·ï¼Œåç»­tokenä¿æŒåŸæ ·
            # æ£€æŸ¥æ˜¯å¦å·²ç»æ˜¯ä»¥è¡¨æƒ…ç¬¦å·å¼€å¤´ï¼Œå¦‚æœä¸æ˜¯ï¼Œå¹¶ä¸”æ˜¯é¦–æ¬¡å‘é€æ€è€ƒå†…å®¹(å¯ä»kwargsä¸­è·å–æ ‡å¿—)ï¼Œåˆ™æ·»åŠ è¡¨æƒ…ç¬¦å·
            is_first_thought = kwargs.get("is_first_thought", False)
            if is_first_thought and not content.startswith("ğŸ¤”"):
                response["choices"][0]["delta"]["content"] = f"ğŸ¤” {content}"
        elif content_type == "separator":
            # åˆ†éš”ç¬¦ç‰¹æ®Šæ ‡è®°
            response["is_separator"] = True
        elif content_type == "error":
            # é”™è¯¯ä¿¡æ¯ç‰¹æ®Šæ ‡è®°
            response["is_error"] = True
            response["choices"][0]["delta"]["content"] = f"âš ï¸ {content}"
        
        return f"data: {json.dumps(response)}\n\n".encode('utf-8')

    def _validate_kwargs(self, kwargs: dict) -> None:
        """éªŒè¯å‚æ•°çš„æœ‰æ•ˆæ€§"""
        # éªŒè¯æ¸©åº¦å‚æ•°
        temperature = kwargs.get('temperature')
        if temperature is not None:
            if not isinstance(temperature, (int, float)) or temperature < 0 or temperature > 1:
                raise ValueError("temperature å¿…é¡»åœ¨ 0 åˆ° 1 ä¹‹é—´")
            
        # éªŒè¯ top_p å‚æ•°
        top_p = kwargs.get('top_p')
        if top_p is not None:
            if not isinstance(top_p, (int, float)) or top_p < 0 or top_p > 1:
                raise ValueError("top_p å¿…é¡»åœ¨ 0 åˆ° 1 ä¹‹é—´")
            
        # éªŒè¯æ¨¡å‹å‚æ•°
        model = kwargs.get('model')
        if model and not isinstance(model, str):
            raise ValueError("model å¿…é¡»æ˜¯å­—ç¬¦ä¸²ç±»å‹")

    def _split_into_tokens(self, text: str) -> list[str]:
        """å°†æ–‡æœ¬åˆ†å‰²æˆæ›´å°çš„token
        
        Args:
            text: è¦åˆ†å‰²çš„æ–‡æœ¬
            
        Returns:
            list[str]: tokenåˆ—è¡¨
        """
        # å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´åˆ†å‰²ç²’åº¦
        # 1. æŒ‰å­—ç¬¦åˆ†å‰²
        return list(text)
        
        # æˆ–è€…æŒ‰è¯åˆ†å‰²
        # return text.split()
        
        # æˆ–è€…ä½¿ç”¨æ›´å¤æ‚çš„åˆ†è¯ç®—æ³•
        # return some_tokenizer(text)