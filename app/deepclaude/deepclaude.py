"""DeepClaude æœåŠ¡ï¼Œç”¨äºåè°ƒ DeepSeek å’Œ Claude API çš„è°ƒç”¨

ä¸»è¦åŠŸèƒ½ï¼š
1. é›†æˆ DeepSeek å’Œ Claude ä¸¤ä¸ªå¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›
2. æ”¯æŒæµå¼å’Œéæµå¼ä¸¤ç§è¾“å‡ºæ¨¡å¼
3. å®ç° DeepSeek æ¨ç†ç»“æœä½œä¸º Claude è¾“å…¥çš„ä¸²è”è°ƒç”¨
4. æä¾›ç¬¦åˆ OpenAI API æ ¼å¼çš„æ ‡å‡†è¾“å‡º

å·¥ä½œæµç¨‹ï¼š
1. æ¥æ”¶ç”¨æˆ·è¾“å…¥çš„æ¶ˆæ¯å’Œæ¨¡å‹å‚æ•°
2. è°ƒç”¨ DeepSeek è¿›è¡Œæ¨ç†ï¼Œè·å–æ¨ç†è¿‡ç¨‹
3. å°†æ¨ç†ç»“æœä¼ é€’ç»™ Claude è¿›è¡Œå¤„ç†
4. æ•´åˆè¾“å‡ºç»“æœå¹¶è¿”å›ç»™ç”¨æˆ·

æŠ€æœ¯ç‰¹ç‚¹ï¼š
1. ä½¿ç”¨å¼‚æ­¥ç¼–ç¨‹æé«˜å¹¶å‘æ€§èƒ½
2. é‡‡ç”¨é˜Ÿåˆ—æœºåˆ¶å®ç°æ•°æ®æµè½¬
3. æ”¯æŒæµå¼è¾“å‡ºæå‡ç”¨æˆ·ä½“éªŒ
4. å®Œå–„çš„é”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•
"""
import json
import time
import tiktoken
import asyncio
from typing import AsyncGenerator
from app.utils.logger import logger
from app.clients import DeepSeekClient, ClaudeClient
from app.utils.message_processor import MessageProcessor


class DeepClaude:
    """å¤„ç† DeepSeek å’Œ Claude API çš„æµå¼è¾“å‡ºè¡”æ¥
    
    è¯¥ç±»è´Ÿè´£åè°ƒ DeepSeek å’Œ Claude ä¸¤ä¸ªæ¨¡å‹çš„è°ƒç”¨è¿‡ç¨‹ï¼Œä¸»è¦ç‰¹ç‚¹ï¼š
    1. æ”¯æŒæµå¼å’Œéæµå¼ä¸¤ç§è¾“å‡ºæ¨¡å¼
    2. å®ç° DeepSeek æ¨ç†å’Œ Claude å›ç­”çš„ä¸²è”è°ƒç”¨
    3. æä¾›æ ‡å‡†çš„ OpenAI æ ¼å¼è¾“å‡º
    4. æ”¯æŒå¤šç§ API æä¾›å•†é…ç½®
    
    ä¸»è¦ç»„ä»¶ï¼š
    - DeepSeekå®¢æˆ·ç«¯ï¼šè´Ÿè´£è°ƒç”¨ DeepSeek API è·å–æ¨ç†è¿‡ç¨‹
    - Claudeå®¢æˆ·ç«¯ï¼šè´Ÿè´£è°ƒç”¨ Claude API ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
    - å¼‚æ­¥é˜Ÿåˆ—ï¼šç”¨äºæ•°æ®æµè½¬å’Œä»»åŠ¡åè°ƒ
    
    å·¥ä½œæ¨¡å¼ï¼š
    1. æµå¼æ¨¡å¼ï¼šå®æ—¶è¿”å›æ¨ç†è¿‡ç¨‹å’Œç”Ÿæˆç»“æœ
    2. éæµå¼æ¨¡å¼ï¼šç­‰å¾…å®Œæ•´ç»“æœåä¸€æ¬¡æ€§è¿”å›
    """

    def __init__(
        self,
        deepseek_api_key: str,
        claude_api_key: str,
        deepseek_api_url: str = None,
        claude_api_url: str = "https://api.anthropic.com/v1/messages",
        claude_provider: str = "anthropic",
        is_origin_reasoning: bool = True
    ):
        """åˆå§‹åŒ– DeepClaude
        
        Args:
            deepseek_api_key: DeepSeek APIå¯†é’¥
            claude_api_key: Claude APIå¯†é’¥
            deepseek_api_url: DeepSeek APIåœ°å€ï¼Œå¯é€‰
            claude_api_url: Claude APIåœ°å€ï¼Œé»˜è®¤ä¸ºAnthropicå®˜æ–¹åœ°å€
            claude_provider: ClaudeæœåŠ¡æä¾›å•†ï¼Œé»˜è®¤ä¸º"anthropic"
            is_origin_reasoning: æ˜¯å¦ä½¿ç”¨åŸå§‹æ¨ç†æ ¼å¼ï¼Œé»˜è®¤ä¸ºTrue
        """
        self.deepseek_client = DeepSeekClient(
            api_key=deepseek_api_key,
            api_url=deepseek_api_url if deepseek_api_url else "https://api.siliconflow.cn/v1/chat/completions"
        )
        self.claude_client = ClaudeClient(
            api_key=claude_api_key,
            api_url=claude_api_url,
            provider=claude_provider
        )
        self.is_origin_reasoning = is_origin_reasoning

    async def chat_completions_with_stream(
        self,
        messages: list,
        model_arg: tuple[float, float, float, float],
        deepseek_model: str = "deepseek-ai/DeepSeek-R1",
        claude_model: str = "claude-3-5-sonnet-20241022"
    ) -> AsyncGenerator[bytes, None]:
        """å¤„ç†å®Œæ•´çš„æµå¼è¾“å‡ºè¿‡ç¨‹
        
        è¯¥æ–¹æ³•å®ç°äº†å®Œæ•´çš„æµå¼å¤„ç†æµç¨‹ï¼ŒåŒ…æ‹¬ï¼š
        1. å¹¶å‘è°ƒç”¨ DeepSeek å’Œ Claude API
        2. å®æ—¶è¿”å›æ¨ç†è¿‡ç¨‹å’Œç”Ÿæˆç»“æœ
        3. ä½¿ç”¨é˜Ÿåˆ—æœºåˆ¶åè°ƒæ•°æ®æµè½¬
        4. æä¾›æ ‡å‡†æ ¼å¼çš„è¾“å‡ºæµ
        
        å¤„ç†æµç¨‹ï¼š
        1. åˆå§‹åŒ–ï¼šåˆ›å»ºä¼šè¯IDå’Œé˜Ÿåˆ—
        2. DeepSeekå¤„ç†ï¼š
           - è°ƒç”¨APIè·å–æ¨ç†æµ
           - æ”¶é›†æ¨ç†å†…å®¹
           - æ¨é€åˆ°è¾“å‡ºé˜Ÿåˆ—
        3. Claudeå¤„ç†ï¼š
           - ç­‰å¾…æ¨ç†å†…å®¹
           - æ„é€  Claude çš„è¾“å…¥æ¶ˆæ¯
           - è°ƒç”¨Claude API è·å–å›ç­”
           - æ¨é€åˆ°è¾“å‡ºé˜Ÿåˆ—
        4. è¾“å‡ºå¤„ç†ï¼š
           - ç›‘æ§ä»»åŠ¡å®ŒæˆçŠ¶æ€
           - æŒ‰åºè¿”å›æ•°æ®æµ
        
        Args:
            messages: åˆå§‹æ¶ˆæ¯åˆ—è¡¨ï¼ŒåŒ…å«å¯¹è¯å†å²
            model_arg: æ¨¡å‹å‚æ•°å…ƒç»„[temperature, top_p, presence_penalty, frequency_penalty]
                - temperature: æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶è¾“å‡ºçš„éšæœºæ€§
                - top_p: æ ¸é‡‡æ ·å‚æ•°ï¼Œæ§åˆ¶è¾“å‡ºçš„å¤šæ ·æ€§
                - presence_penalty: å­˜åœ¨æƒ©ç½šï¼Œé™ä½é‡å¤tokençš„æ¦‚ç‡
                - frequency_penalty: é¢‘ç‡æƒ©ç½šï¼Œé™ä½é«˜é¢‘tokençš„æ¦‚ç‡
            deepseek_model: DeepSeek æ¨¡å‹åç§°
            claude_model: Claude æ¨¡å‹åç§°
            
        Yields:
            å­—èŠ‚æµæ•°æ®ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
            {
                "id": "chatcmpl-xxx",
                "object": "chat.completion.chunk",
                "created": timestamp,
                "model": model_name,
                "choices": [{
                    "index": 0,
                    "delta": {
                        "role": "assistant",
                        "reasoning_content": reasoning_content,
                        "content": content
                    }
                }]
            }
            
        å¼‚å¸¸å¤„ç†ï¼š
        1. DeepSeek APIè°ƒç”¨å¼‚å¸¸ï¼šè®°å½•é”™è¯¯å¹¶ç»§ç»­Claudeå¤„ç†
        2. Claude APIè°ƒç”¨å¼‚å¸¸ï¼šè®°å½•é”™è¯¯å¹¶ç»“æŸå¤„ç†
        3. é˜Ÿåˆ—æ“ä½œå¼‚å¸¸ï¼šç¡®ä¿æ­£ç¡®å…³é—­å’Œæ¸…ç†
        """
        # éªŒè¯æ¶ˆæ¯æ ¼å¼
        if not messages:
            error_msg = "æ¶ˆæ¯åˆ—è¡¨ä¸ºç©º"
            logger.error(error_msg)
            raise ValueError(error_msg)
        
        # æ£€æŸ¥è¿ç»­æ¶ˆæ¯
        for i in range(1, len(messages)):
            if messages[i].get("role") == messages[i-1].get("role"):
                error_msg = f"æ£€æµ‹åˆ°è¿ç»­çš„{messages[i].get('role')}æ¶ˆæ¯"
                logger.warning(error_msg)
                raise ValueError(error_msg)
            
        # è½¬æ¢æ¶ˆæ¯æ ¼å¼
        message_processor = MessageProcessor()
        try:
            messages = message_processor.convert_to_deepseek_format(messages)
            logger.debug(f"è½¬æ¢åçš„æ¶ˆæ¯: {messages}")
        except Exception as e:
            error_msg = f"æ¶ˆæ¯æ ¼å¼è½¬æ¢å¤±è´¥: {str(e)}"
            logger.error(error_msg, exc_info=True)
            raise ValueError(error_msg)

        # ç”Ÿæˆå”¯ä¸€çš„ä¼šè¯IDå’Œæ—¶é—´æˆ³
        chat_id = f"chatcmpl-{hex(int(time.time() * 1000))[2:]}"
        created_time = int(time.time())

        # åˆ›å»ºé˜Ÿåˆ—ï¼Œç”¨äºæ”¶é›†è¾“å‡ºæ•°æ®
        output_queue = asyncio.Queue()  # å­˜å‚¨æœ€ç»ˆè¾“å‡ºçš„æ•°æ®æµ
        claude_queue = asyncio.Queue()  # ç”¨äºä¼ é€’ DeepSeek æ¨ç†å†…å®¹ç»™ Claude

        # ç”¨äºå­˜å‚¨ DeepSeek çš„æ¨ç†ç´¯ç§¯å†…å®¹
        reasoning_content = []  # å­˜å‚¨å®Œæ•´çš„æ¨ç†è¿‡ç¨‹

        async def process_deepseek():
            """å¤„ç† DeepSeek æµå¼çš„å¼‚æ­¥å‡½æ•°"""
            logger.info(f"å¼€å§‹å¤„ç† DeepSeek æµï¼Œä½¿ç”¨æ¨¡å‹ï¼š{deepseek_model}, æä¾›å•†: {self.deepseek_client.provider}")
            try:
                # æ·»åŠ æ€è€ƒå¼€å§‹æ ‡è®°
                start_response = {
                    "id": chat_id,
                    "object": "chat.completion.chunk",
                    "created": created_time,
                    "model": deepseek_model,
                    "choices": [{
                        "index": 0,
                        "delta": {
                            "role": "assistant",
                            "content": "ğŸ¤” æ€è€ƒè¿‡ç¨‹:\n"  # åªåœ¨å¼€å§‹æ—¶æ·»åŠ æ ‡è®°å’Œæ¢è¡Œ
                        }
                    }]
                }
                await output_queue.put(f"data: {json.dumps(start_response)}\n\n".encode('utf-8'))
                
                async for content_type, content in self.deepseek_client.stream_chat(
                    messages=messages, 
                    model=deepseek_model, 
                    is_origin_reasoning=self.is_origin_reasoning
                ):
                    if content_type == "reasoning":
                        # æ”¶é›†æ¨ç†å†…å®¹
                        reasoning_content.append(content)
                        # ç›´æ¥å‘é€å†…å®¹ï¼Œä¸æ·»åŠ é¢å¤–æ ‡è®°
                        response = {
                            "id": chat_id,
                            "object": "chat.completion.chunk",
                            "created": created_time,
                            "model": deepseek_model,
                            "choices": [{
                                "index": 0,
                                "delta": {
                                    "role": "assistant",
                                    "content": content  # ç›´æ¥å‘é€å†…å®¹ï¼Œä¸æ·»åŠ æ ‡è®°
                                }
                            }]
                        }
                        logger.debug(f"å‘é€æ¨ç†å“åº”: {response}")
                        await output_queue.put(f"data: {json.dumps(response)}\n\n".encode('utf-8'))
                    elif content_type == "content":
                        # æ·»åŠ æ€è€ƒç»“æŸåˆ†éš”ç¬¦
                        separator_response = {
                            "id": chat_id,
                            "object": "chat.completion.chunk",
                            "created": created_time,
                            "model": deepseek_model,
                            "choices": [{
                                "index": 0,
                                "delta": {
                                    "role": "assistant",
                                    "content": "\n\n---\næ€è€ƒå®Œæ¯•ï¼Œå¼€å§‹å›ç­”ï¼š\n\n"
                                }
                            }]
                        }
                        await output_queue.put(f"data: {json.dumps(separator_response)}\n\n".encode('utf-8'))
                        
                        # å‘é€ç´¯ç§¯çš„æ¨ç†å†…å®¹ç»™ Claude
                        logger.info(f"DeepSeek æ¨ç†å®Œæˆï¼Œæ”¶é›†åˆ°çš„æ¨ç†å†…å®¹é•¿åº¦ï¼š{len(''.join(reasoning_content))}")
                        await claude_queue.put("".join(reasoning_content))
                        break
            except Exception as e:
                logger.error(f"å¤„ç† DeepSeek æµæ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
                await claude_queue.put("")
            finally:
                logger.info("DeepSeek ä»»åŠ¡å¤„ç†å®Œæˆï¼Œæ ‡è®°ç»“æŸ")
                await output_queue.put(None)

        async def process_claude():
            """å¤„ç† Claude æµçš„å¼‚æ­¥å‡½æ•°
            
            ä¸»è¦èŒè´£ï¼š
            1. ç­‰å¾…å¹¶è·å– DeepSeek çš„æ¨ç†ç»“æœ
            2. æ„é€  Claude çš„è¾“å…¥æ¶ˆæ¯
            3. è°ƒç”¨ Claude API è·å–å›ç­”
            4. å®æ—¶æ¨é€å›ç­”å†…å®¹åˆ°è¾“å‡ºé˜Ÿåˆ—
            """
            try:
                # ç­‰å¾… DeepSeek çš„æ¨ç†ç»“æœ
                logger.info("ç­‰å¾…è·å– DeepSeek çš„æ¨ç†å†…å®¹...")
                reasoning = await claude_queue.get()
                logger.debug(f"è·å–åˆ°æ¨ç†å†…å®¹ï¼Œå†…å®¹é•¿åº¦ï¼š{len(reasoning) if reasoning else 0}")
                
                # å¤„ç†æ¨ç†å†…å®¹ç¼ºå¤±çš„æƒ…å†µ
                if not reasoning:
                    logger.warning("æœªèƒ½è·å–åˆ°æœ‰æ•ˆçš„æ¨ç†å†…å®¹ï¼Œå°†ä½¿ç”¨é»˜è®¤æç¤ºç»§ç»­")
                    reasoning = "è·å–æ¨ç†å†…å®¹å¤±è´¥"
                    
                # æ„é€  Claude çš„è¾“å…¥æ¶ˆæ¯
                claude_messages = messages.copy()
                combined_content = f"""
                è¿™æ˜¯æˆ‘è‡ªå·±åŸºäºé—®é¢˜çš„æ€è€ƒè¿‡ç¨‹:\n{reasoning}\n\n
                ä¸Šé¢æ˜¯æˆ‘è‡ªå·±çš„æ€è€ƒè¿‡ç¨‹ä¸ä¸€å®šå®Œå…¨æ­£ç¡®è¯·å€Ÿé‰´æ€è€ƒè¿‡ç¨‹å’ŒæœŸä¸­ä½ ä¹Ÿè®¤ä¸ºæ­£ç¡®çš„éƒ¨åˆ†ï¼ˆ1000% æƒé‡ï¼‰
                ï¼Œç°åœ¨è¯·ç»™å‡ºè¯¦ç»†å’Œç»†è‡´çš„ç­”æ¡ˆï¼Œä¸è¦çœç•¥æ­¥éª¤å’Œæ­¥éª¤ç»†èŠ‚
                ï¼Œè¦åˆ†è§£åŸé¢˜ç¡®ä¿ä½ ç†è§£äº†åŸé¢˜çš„æ¯ä¸ªéƒ¨åˆ†ï¼Œä¹Ÿè¦æŒæ¡æ•´ä½“æ„æ€
                ï¼Œæœ€ä½³è´¨é‡ï¼ˆ1000% æƒé‡ï¼‰ï¼Œæœ€è¯¦ç»†è§£ç­”ï¼ˆ1000% æƒé‡ï¼‰ï¼Œä¸è¦å›ç­”å¤ªç®€å•è®©æˆ‘èƒ½å‚è€ƒä¸€æ­¥æ­¥åº”ç”¨ï¼ˆ1000% æƒé‡ï¼‰:"""
                
                # å¤„ç†ç”¨æˆ·æ¶ˆæ¯ï¼Œå°†æ¨ç†ç»“æœæ·»åŠ åˆ°æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ä¸­
                last_message = claude_messages[-1]
                if last_message.get("role", "") == "user":
                    original_content = last_message["content"]
                    fixed_content = f"Here's my original input:\n{original_content}\n\n{combined_content}"
                    last_message["content"] = fixed_content
                    
                # ç§»é™¤ç³»ç»Ÿæ¶ˆæ¯ï¼Œå› ä¸ºæŸäº› API æä¾›å•†å¯èƒ½ä¸æ”¯æŒ
                claude_messages = [message for message in claude_messages if message.get("role", "") != "system"]

                logger.info(f"å¼€å§‹å¤„ç† Claude æµï¼Œä½¿ç”¨æ¨¡å‹: {claude_model}, æä¾›å•†: {self.claude_client.provider}")

                # è°ƒç”¨ Claude API è·å–å›ç­”
                async for content_type, content in self.claude_client.stream_chat(
                    messages=claude_messages,
                    model_arg=model_arg,
                    model=claude_model,
                ):
                    if content_type == "answer":
                        # æ„é€ è¾“å‡ºå“åº”
                        response = {
                            "id": chat_id,
                            "object": "chat.completion.chunk",
                            "created": created_time,
                            "model": claude_model,
                            "choices": [{
                                "index": 0,
                                "delta": {
                                    "role": "assistant",
                                    "content": content
                                }
                            }]
                        }
                        await output_queue.put(f"data: {json.dumps(response)}\n\n".encode('utf-8'))
            except Exception as e:
                logger.error(f"å¤„ç† Claude æµæ—¶å‘ç”Ÿé”™è¯¯: {e}")
            # æ ‡è®°ä»»åŠ¡å®Œæˆ
            logger.info("Claude ä»»åŠ¡å¤„ç†å®Œæˆï¼Œæ ‡è®°ç»“æŸ")
            await output_queue.put(None)
        
        # åˆ›å»ºå¹¶å‘ä»»åŠ¡
        deepseek_task = asyncio.create_task(process_deepseek())
        claude_task = asyncio.create_task(process_claude())
        
        # ç­‰å¾…ä¸¤ä¸ªä»»åŠ¡å®Œæˆï¼Œé€šè¿‡è®¡æ•°åˆ¤æ–­
        finished_tasks = 0
        while finished_tasks < 2:
            item = await output_queue.get()
            if item is None:
                finished_tasks += 1
                continue
            logger.debug(f"è‡ªå®šä¹‰apiå‘å¤–å‘é€ token: {item}")
            yield item

        # å‘é€å®Œæˆæ ‡è®°
        yield b'data: [DONE]\n\n'

    async def chat_completions_without_stream(
        self,
        messages: list,
        model_arg: tuple[float, float, float, float],
        deepseek_model: str = "deepseek-reasoner",
        claude_model: str = "claude-3-5-sonnet-20241022"
    ) -> dict:
        """å¤„ç†éæµå¼è¾“å‡ºè¿‡ç¨‹
        
        è¯¥æ–¹æ³•å®ç°äº†å®Œæ•´çš„éæµå¼å¤„ç†æµç¨‹ï¼Œä¸»è¦åŒ…æ‹¬ï¼š
        1. è·å– DeepSeek æ¨ç†å†…å®¹
        2. æ„é€  Claude è¾“å…¥æ¶ˆæ¯
        3. è®¡ç®—è¾“å…¥è¾“å‡ºçš„ Token æ•°é‡
        4. ç”Ÿæˆæ ‡å‡†çš„ OpenAI æ ¼å¼å“åº”
        
        å¤„ç†æµç¨‹ï¼š
        1. åˆå§‹åŒ–ï¼šç”Ÿæˆä¼šè¯IDå’Œæ—¶é—´æˆ³
        2. DeepSeekå¤„ç†ï¼š
           - ä½¿ç”¨æµå¼æ–¹å¼è·å–æ¨ç†å†…å®¹
           - ç´¯ç§¯æ¨ç†æ–‡æœ¬
           - å¤„ç†å¼‚å¸¸æƒ…å†µ
        3. Claudeå¤„ç†ï¼š
           - æ„é€ è¾“å…¥æ¶ˆæ¯
           - æ·»åŠ æ¨ç†å†…å®¹
           - ç§»é™¤ä¸æ”¯æŒçš„æ¶ˆæ¯ç±»å‹
        4. Tokenè®¡ç®—ï¼š
           - è®¡ç®—è¾“å…¥Tokenæ•°é‡
           - ç»Ÿè®¡è¾“å‡ºTokenæ•°é‡
        5. å“åº”å¤„ç†ï¼š
           - ç”ŸæˆOpenAIæ ¼å¼å“åº”
           - åŒ…å«å®Œæ•´çš„ä½¿ç”¨ç»Ÿè®¡
        
        Args:
            messages: åˆå§‹æ¶ˆæ¯åˆ—è¡¨ï¼ŒåŒ…å«å¯¹è¯å†å²
            model_arg: æ¨¡å‹å‚æ•°å…ƒç»„[temperature, top_p, presence_penalty, frequency_penalty]
            deepseek_model: DeepSeek æ¨¡å‹åç§°
            claude_model: Claude æ¨¡å‹åç§°
            
        Returns:
            dict: OpenAI æ ¼å¼çš„å®Œæ•´å“åº”ï¼ŒåŒ…å«ä»¥ä¸‹å†…å®¹ï¼š
            - id: ä¼šè¯å”¯ä¸€æ ‡è¯†
            - object: å“åº”å¯¹è±¡ç±»å‹
            - created: åˆ›å»ºæ—¶é—´æˆ³
            - model: ä½¿ç”¨çš„æ¨¡å‹åç§°
            - choices: å“åº”å†…å®¹åˆ—è¡¨
              - message: åŒ…å«roleã€contentå’Œreasoning_content
              - finish_reason: ç»“æŸåŸå› 
            - usage: Tokenä½¿ç”¨ç»Ÿè®¡
              - prompt_tokens: è¾“å…¥Tokenæ•°é‡
              - completion_tokens: è¾“å‡ºTokenæ•°é‡
              - total_tokens: æ€»Tokenæ•°é‡
        
        å¼‚å¸¸å¤„ç†ï¼š
        1. DeepSeekæ¨ç†å¼‚å¸¸ï¼šè®°å½•é”™è¯¯å¹¶ä½¿ç”¨é»˜è®¤å€¼
        2. Claudeå“åº”å¼‚å¸¸ï¼šè®°å½•é”™è¯¯å¹¶å‘ä¸Šä¼ é€’å¼‚å¸¸
        3. Tokenè®¡ç®—å¼‚å¸¸ï¼šè®°å½•è­¦å‘Šå¹¶ç»§ç»­å¤„ç†
        """
        # ç”Ÿæˆå”¯ä¸€çš„ä¼šè¯IDå’Œæ—¶é—´æˆ³
        chat_id = f"chatcmpl-{hex(int(time.time() * 1000))[2:]}"
        created_time = int(time.time())
        reasoning_content = []

        # 1. è·å– DeepSeek çš„æ¨ç†å†…å®¹ï¼ˆä½¿ç”¨æµå¼æ–¹å¼ï¼‰
        try:
            async for content_type, content in self.deepseek_client.stream_chat(messages, deepseek_model, self.is_origin_reasoning):
                if content_type == "reasoning":
                    # æ”¶é›†æ¨ç†å†…å®¹
                    reasoning_content.append(content)
                elif content_type == "content":
                    # æ¨ç†å®Œæˆï¼Œé€€å‡ºå¾ªç¯
                    break
        except Exception as e:
            # å¤„ç†å¼‚å¸¸æƒ…å†µï¼Œä½¿ç”¨é»˜è®¤å€¼
            logger.error(f"è·å– DeepSeek æ¨ç†å†…å®¹æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            reasoning_content = ["è·å–æ¨ç†å†…å®¹å¤±è´¥"]

        # 2. æ„é€  Claude çš„è¾“å…¥æ¶ˆæ¯
        reasoning = "".join(reasoning_content)  # åˆå¹¶æ¨ç†å†…å®¹
        claude_messages = messages.copy()  # å¤åˆ¶åŸå§‹æ¶ˆæ¯åˆ—è¡¨

        # æ„é€ åŒ…å«æ¨ç†å†…å®¹çš„æç¤ºæ–‡æœ¬
        combined_content = f"""
        Here's my another model's reasoning process:\n{reasoning}\n\n
        Based on this reasoning, provide your response directly to me:"""
        
        # å¤„ç†æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ï¼Œæ·»åŠ æ¨ç†å†…å®¹
        last_message = claude_messages[-1]
        if last_message.get("role", "") == "user":
            original_content = last_message["content"]
            fixed_content = f"Here's my original input:\n{original_content}\n\n{combined_content}"
            last_message["content"] = fixed_content

        # ç§»é™¤ç³»ç»Ÿæ¶ˆæ¯ï¼Œç¡®ä¿å…¼å®¹æ€§
        claude_messages = [message for message in claude_messages if message.get("role", "") != "system"]

        # è®¡ç®—è¾“å…¥Tokenæ•°é‡
        token_content = "\n".join([message.get("content", "") for message in claude_messages])
        encoding = tiktoken.encoding_for_model("gpt-4o")
        input_tokens = encoding.encode(token_content)
        logger.debug(f"è¾“å…¥ Tokens: {len(input_tokens)}")

        # è°ƒè¯•è¾“å‡ºå¤„ç†åçš„æ¶ˆæ¯
        logger.debug("claude messages: " + str(claude_messages))

        # 3. è·å– Claude çš„éæµå¼å“åº”
        try:
            answer = ""  # å­˜å‚¨å®Œæ•´å“åº”å†…å®¹
            # ä½¿ç”¨æµå¼æ–¹å¼è·å–å“åº”ï¼Œä½†è®¾ç½®stream=False
            async for content_type, content in self.claude_client.stream_chat(
                messages=claude_messages,
                model_arg=model_arg,
                model=claude_model,
                stream=False
            ):
                if content_type == "answer":
                    # ç´¯ç§¯å“åº”å†…å®¹
                    answer += content
                # è®¡ç®—è¾“å‡ºTokenæ•°é‡
                output_tokens = encoding.encode(answer)
                logger.debug(f"è¾“å‡º Tokens: {len(output_tokens)}")

            # 4. æ„é€  OpenAI æ ¼å¼çš„å®Œæ•´å“åº”
            return {
                "id": chat_id,
                "object": "chat.completion",
                "created": created_time,
                "model": claude_model,
                "choices": [{
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": answer,
                        "reasoning_content": reasoning
                    },
                    "finish_reason": "stop"
                }],
                "usage": {
                    "prompt_tokens": len(input_tokens),
                    "completion_tokens": len(output_tokens),
                    "total_tokens": len(input_tokens + output_tokens)
                }
            }
        except Exception as e:
            # è®°å½•é”™è¯¯å¹¶å‘ä¸Šä¼ é€’å¼‚å¸¸
            logger.error(f"è·å– Claude å“åº”æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            raise e