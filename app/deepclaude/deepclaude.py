"""DeepClaude æœåŠ¡ï¼Œç”¨äºåè°ƒ DeepSeek å’Œ Claude API çš„è°ƒç”¨

ä¸»è¦åŠŸèƒ½ï¼š
1. é›†æˆ DeepSeek å’Œ Claude ä¸¤ä¸ªå¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›
2. æ”¯æŒæµå¼å’Œéæµå¼ä¸¤ç§è¾“å‡ºæ¨¡å¼
3. å®ç° DeepSeek æ¨ç†ç»“æœä½œä¸º Claude è¾“å…¥çš„ä¸²è”è°ƒç”¨
4. æä¾›ç¬¦åˆ OpenAI API æ ¼å¼çš„æ ‡å‡†è¾“å‡º

å·¥ä½œæµç¨‹ï¼š
1. æ¥æ”¶ç”¨æˆ·è¾“å…¥çš„æ¶ˆæ¯å’Œæ¨¡å‹å‚æ•°
2. è°ƒç”¨ DeepSeek è¿›è¡Œæ¨ç†ï¼Œè·å–æ¨ç†è¿‡ç¨‹
3. å°†æ¨ç†ç»“æœä¼ é€’ç»™ Claude è¿›è¡Œå¤„ç†
4. æ•´åˆè¾“å‡ºç»“æœå¹¶è¿”å›ç»™ç”¨æˆ·

æŠ€æœ¯ç‰¹ç‚¹ï¼š
1. ä½¿ç”¨å¼‚æ­¥ç¼–ç¨‹æé«˜å¹¶å‘æ€§èƒ½
2. é‡‡ç”¨é˜Ÿåˆ—æœºåˆ¶å®ç°æ•°æ®æµè½¬
3. æ”¯æŒæµå¼è¾“å‡ºæå‡ç”¨æˆ·ä½“éªŒ
4. å®Œå–„çš„é”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•
"""
import json
import time
import tiktoken
import asyncio
from typing import AsyncGenerator
from app.utils.logger import logger
from app.clients import DeepSeekClient, ClaudeClient, OllamaR1Client
from app.utils.message_processor import MessageProcessor
import aiohttp
import os
from dotenv import load_dotenv

load_dotenv()

class DeepClaude:
    """å¤„ç† DeepSeek å’Œ Claude API çš„æµå¼è¾“å‡ºè¡”æ¥
    
    è¯¥ç±»è´Ÿè´£åè°ƒ DeepSeek å’Œ Claude ä¸¤ä¸ªæ¨¡å‹çš„è°ƒç”¨è¿‡ç¨‹ï¼Œä¸»è¦ç‰¹ç‚¹ï¼š
    1. æ”¯æŒæµå¼å’Œéæµå¼ä¸¤ç§è¾“å‡ºæ¨¡å¼
    2. å®ç° DeepSeek æ¨ç†å’Œ Claude å›ç­”çš„ä¸²è”è°ƒç”¨
    3. æä¾›æ ‡å‡†çš„ OpenAI æ ¼å¼è¾“å‡º
    4. æ”¯æŒå¤šç§ API æä¾›å•†é…ç½®
    
    ä¸»è¦ç»„ä»¶ï¼š
    - DeepSeekå®¢æˆ·ç«¯ï¼šè´Ÿè´£è°ƒç”¨ DeepSeek API è·å–æ¨ç†è¿‡ç¨‹
    - Claudeå®¢æˆ·ç«¯ï¼šè´Ÿè´£è°ƒç”¨ Claude API ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
    - å¼‚æ­¥é˜Ÿåˆ—ï¼šç”¨äºæ•°æ®æµè½¬å’Œä»»åŠ¡åè°ƒ
    - OllamaR1Clientï¼šè´Ÿè´£è°ƒç”¨ OllamaR1 API ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
    - å¼‚æ­¥é˜Ÿåˆ—ï¼šç”¨äºæ•°æ®æµè½¬å’Œä»»åŠ¡åè°ƒ
    
    å·¥ä½œæ¨¡å¼ï¼š
    1. æµå¼æ¨¡å¼ï¼šå®æ—¶è¿”å›æ¨ç†è¿‡ç¨‹å’Œç”Ÿæˆç»“æœ
    2. éæµå¼æ¨¡å¼ï¼šç­‰å¾…å®Œæ•´ç»“æœåä¸€æ¬¡æ€§è¿”å›
    """
    def __init__(
        self,
        deepseek_api_key: str,
        claude_api_key: str,
        deepseek_api_url: str = None,
        claude_api_url: str = None,
        claude_provider: str = None,
        is_origin_reasoning: bool = None,
        ollama_api_url: str = None
    ):
        # éªŒè¯å¿…è¦çš„é…ç½®
        if not deepseek_api_key and os.getenv('REASONING_PROVIDER') == 'deepseek':
            raise ValueError("ä½¿ç”¨ DeepSeek æ¨ç†æ—¶å¿…é¡»æä¾› DEEPSEEK_API_KEY")
        
        if not claude_api_key:
            raise ValueError("å¿…é¡»æä¾› CLAUDE_API_KEY")
        
        # éªŒè¯ Ollama é…ç½®
        if os.getenv('REASONING_PROVIDER') == 'ollama':
            ollama_url = ollama_api_url or os.getenv('OLLAMA_API_URL')
            if not ollama_url:
                raise ValueError("ä½¿ç”¨ Ollama æ¨ç†æ—¶å¿…é¡»æä¾› OLLAMA_API_URL")
        
        # åˆå§‹åŒ–æ¨ç†æä¾›è€…
        self.reasoning_providers = {
            'deepseek': lambda: DeepSeekClient(deepseek_api_key, deepseek_api_url),
            'ollama': lambda: OllamaR1Client(ollama_api_url)
        }
        
        # åˆå§‹åŒ– Claude å®¢æˆ·ç«¯
        self.claude_client = ClaudeClient(claude_api_key, claude_api_url, claude_provider)
        
        self.is_origin_reasoning = (
            is_origin_reasoning 
            if is_origin_reasoning is not None 
            else os.getenv('IS_ORIGIN_REASONING', 'true').lower() == 'true'
        )

    def _get_reasoning_provider(self):
        """è·å–å½“å‰é…ç½®çš„æ¨ç†æä¾›è€…"""
        provider = os.getenv('REASONING_PROVIDER', 'deepseek').lower()
        if provider not in self.reasoning_providers:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨ç†æä¾›è€…: {provider}")
        return self.reasoning_providers[provider]()

    async def _handle_stream_response(self, response_queue: asyncio.Queue, 
                                    chat_id: str, created_time: int, model: str) -> AsyncGenerator[bytes, None]:
        """å¤„ç†æµå¼å“åº”"""
        try:
            while True:
                item = await response_queue.get()
                if item is None:
                    break
                    
                content_type, content = item
                if content_type == "reasoning":
                    response = {
                        "id": chat_id,
                        "object": "chat.completion.chunk",
                        "created": created_time,
                        "model": model,
                        "choices": [{
                            "index": 0,
                            "delta": {
                                "role": "assistant",
                                "content": f"ğŸ¤” æ€è€ƒè¿‡ç¨‹:\n{content}\n"
                            }
                        }]
                    }
                elif content_type == "content":
                    response = {
                        "id": chat_id,
                        "object": "chat.completion.chunk",
                        "created": created_time,
                        "model": model,
                        "choices": [{
                            "index": 0,
                            "delta": {
                                "role": "assistant",
                                "content": f"\n\n---\næ€è€ƒå®Œæ¯•ï¼Œå¼€å§‹å›ç­”ï¼š\n\n{content}"
                            }
                        }]
                    }
                yield f"data: {json.dumps(response)}\n\n".encode('utf-8')
        except Exception as e:
            logger.error(f"å¤„ç†æµå¼å“åº”æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
            raise

    async def chat_completions_with_stream(
        self,
        messages: list,
        model_arg: tuple[float, float, float, float],
        deepseek_model: str = None,
        claude_model: str = None
    ) -> AsyncGenerator[bytes, None]:
        """å¤„ç†æµå¼å¯¹è¯"""
        self._validate_messages(messages)
        deepseek_model = deepseek_model or os.getenv('DEEPSEEK_MODEL', 'deepseek-ai/DeepSeek-R1')
        claude_model = claude_model or os.getenv('CLAUDE_MODEL', 'claude-3-sonnet-20240229')
        
        # æ·»åŠ æ¨¡å‹åç§°éªŒè¯
        self._validate_model_names(deepseek_model, claude_model)
        
        chat_id = f"chatcmpl-{hex(int(time.time() * 1000))[2:]}"
        created_time = int(time.time())
        response_queue = asyncio.Queue()
        
        try:
            provider = self._get_reasoning_provider()
            model = deepseek_model if isinstance(provider, DeepSeekClient) else (
                "deepseek-r1:32b" if isinstance(provider, OllamaR1Client) else claude_model
            )
            
            kwargs = {
                "messages": messages,
                "model": model,
            }
            
            if not isinstance(provider, OllamaR1Client):
                kwargs["model_arg"] = model_arg
            
            if isinstance(provider, DeepSeekClient):
                kwargs["is_origin_reasoning"] = self.is_origin_reasoning
            
            reasoning_content = []
            
            # 1. å¼€å§‹æ€è€ƒæç¤º
            yield self._format_stream_response(
                "ğŸ¤” æ€è€ƒè¿‡ç¨‹:\n",
                chat_id,
                created_time,
                model
            )
            
            # 2. è·å–å¹¶å®æ—¶æ˜¾ç¤ºæ¨ç†è¿‡ç¨‹
            current_reasoning = ""
            try:
                async for content_type, content in provider.stream_chat(**kwargs):
                    if content_type == "reasoning":
                        # æ”¶é›†å®Œæ•´æ¨ç†å†…å®¹ç”¨äºåç»­ Claude
                        reasoning_content.append(content)
                        
                        # å¤„ç†å¢é‡å†…å®¹ï¼Œå®ç°å­—ç¬¦çº§æµå¼è¾“å‡º
                        for char in content:
                            yield self._format_stream_response(
                                char,
                                chat_id,
                                created_time,
                                model
                            )
                            # é€‚å½“å»¶è¿Ÿï¼Œé¿å…è¾“å‡ºå¤ªå¿«
                            await asyncio.sleep(0.01)
                        
                        # æ¯æ®µæ¨ç†åæ·»åŠ æ¢è¡Œ
                        yield self._format_stream_response(
                            "\n",
                            chat_id,
                            created_time,
                            model
                        )
                        
            except Exception as e:
                logger.error(f"è·å–æ¨ç†å†…å®¹å¤±è´¥: {e}")
                yield self._format_stream_response(
                    "\nâŒ æ€è€ƒè¿‡ç¨‹è·å–å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•\n",
                    chat_id,
                    created_time,
                    model
                )
                return
            
            # 3. åˆ†éš”ç¬¦
            yield self._format_stream_response(
                "\n=============== æ€è€ƒå®Œæ¯•ï¼Œå¼€å§‹å›ç­” ===============\n\n",
                chat_id,
                created_time,
                model
            )
            
            # 4. è°ƒç”¨ Claude å¹¶å®æ—¶æ˜¾ç¤ºå›ç­”
            try:
                # æ„é€  Claude è¾“å…¥
                reasoning = "\n".join(reasoning_content)
                combined_content = f"""
è¿™æ˜¯æˆ‘è‡ªå·±åŸºäºé—®é¢˜çš„æ€è€ƒè¿‡ç¨‹:\n{reasoning}\n\n
ä¸Šé¢æ˜¯æˆ‘è‡ªå·±çš„æ€è€ƒè¿‡ç¨‹ä¸ä¸€å®šå®Œå…¨æ­£ç¡®è¯·å€Ÿé‰´æ€è€ƒè¿‡ç¨‹å’ŒæœŸä¸­ä½ ä¹Ÿè®¤ä¸ºæ­£ç¡®çš„éƒ¨åˆ†ï¼ˆ1000% æƒé‡ï¼‰
ï¼Œç°åœ¨è¯·ç»™å‡ºè¯¦ç»†å’Œç»†è‡´çš„ç­”æ¡ˆï¼Œä¸è¦çœç•¥æ­¥éª¤å’Œæ­¥éª¤ç»†èŠ‚
ï¼Œè¦åˆ†è§£åŸé¢˜ç¡®ä¿ä½ ç†è§£äº†åŸé¢˜çš„æ¯ä¸ªéƒ¨åˆ†ï¼Œä¹Ÿè¦æŒæ¡æ•´ä½“æ„æ€
ï¼Œæœ€ä½³è´¨é‡ï¼ˆ1000% æƒé‡ï¼‰ï¼Œæœ€è¯¦ç»†è§£ç­”ï¼ˆ1000% æƒé‡ï¼‰ï¼Œä¸è¦å›ç­”å¤ªç®€å•è®©æˆ‘èƒ½å‚è€ƒä¸€æ­¥æ­¥åº”ç”¨ï¼ˆ1000% æƒé‡ï¼‰:"""

                claude_messages = [{"role": "user", "content": combined_content}]
                
                # å­—ç¬¦çº§æµå¼è¾“å‡º Claude å›ç­”
                async for content_type, content in self.claude_client.stream_chat(
                    messages=claude_messages,
                    model_arg=model_arg,
                    model=claude_model
                ):
                    if content_type == "answer":
                        # ä¸€ä¸ªå­—ç¬¦ä¸€ä¸ªå­—ç¬¦åœ°è¾“å‡º
                        for char in content:
                            yield self._format_stream_response(
                                char,
                                chat_id,
                                created_time,
                                model
                            )
                            # é€‚å½“å»¶è¿Ÿï¼Œé¿å…è¾“å‡ºå¤ªå¿«
                            await asyncio.sleep(0.01)
                            
            except Exception as e:
                logger.error(f"è·å– Claude å›ç­”å¤±è´¥: {e}")
                yield self._format_stream_response(
                    "\nâŒ è·å–å›ç­”å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•\n",
                    chat_id,
                    created_time,
                    model
                )
            
        except Exception as e:
            logger.error(f"å¤„ç†æµå¼å¯¹è¯æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            yield self._format_stream_response(
                "\nâŒ æœåŠ¡å‡ºç°é”™è¯¯ï¼Œè¯·ç¨åé‡è¯•\n",
                chat_id,
                created_time,
                model
            )
        finally:
            yield b'data: [DONE]\n\n'

    async def chat_completions_without_stream(
        self,
        messages: list,
        model_arg: tuple[float, float, float, float],
        deepseek_model: str = "deepseek-reasoner",
        claude_model: str = "claude-3-5-sonnet-20241022"
    ) -> dict:
        """éæµå¼å¯¹è¯å®Œæˆ
        
        Args:
            messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
            model_arg: æ¨¡å‹å‚æ•°å…ƒç»„
            deepseek_model: DeepSeek æ¨¡å‹åç§°
            claude_model: Claude æ¨¡å‹åç§°
            
        Returns:
            dict: åŒ…å«å›ç­”å†…å®¹çš„å“åº”å­—å…¸
        """
        logger.info("å¼€å§‹å¤„ç†è¯·æ±‚...")
        logger.debug(f"è¾“å…¥æ¶ˆæ¯: {messages}")
        
        # 1. è·å–æ¨ç†å†…å®¹
        logger.info("æ­£åœ¨è·å–æ¨ç†å†…å®¹...")
        try:
            reasoning = await self._get_reasoning_with_fallback(
                messages=messages,
                model=deepseek_model,
                model_arg=model_arg
            )
        except Exception as e:
            logger.error(f"è·å–æ¨ç†å†…å®¹å¤±è´¥: {e}")
            reasoning = "æ— æ³•è·å–æ¨ç†å†…å®¹"
        
        logger.debug(f"è·å–åˆ°æ¨ç†å†…å®¹: {reasoning}")
        
        # 2. æ„é€  Claude çš„è¾“å…¥æ¶ˆæ¯
        combined_content = f"""
è¿™æ˜¯æˆ‘è‡ªå·±åŸºäºé—®é¢˜çš„æ€è€ƒè¿‡ç¨‹:\n{reasoning}\n\n
ä¸Šé¢æ˜¯æˆ‘è‡ªå·±çš„æ€è€ƒè¿‡ç¨‹ä¸ä¸€å®šå®Œå…¨æ­£ç¡®è¯·å€Ÿé‰´æ€è€ƒè¿‡ç¨‹å’ŒæœŸä¸­ä½ ä¹Ÿè®¤ä¸ºæ­£ç¡®çš„éƒ¨åˆ†ï¼ˆ1000% æƒé‡ï¼‰
ï¼Œç°åœ¨è¯·ç»™å‡ºè¯¦ç»†å’Œç»†è‡´çš„ç­”æ¡ˆï¼Œä¸è¦çœç•¥æ­¥éª¤å’Œæ­¥éª¤ç»†èŠ‚
ï¼Œè¦åˆ†è§£åŸé¢˜ç¡®ä¿ä½ ç†è§£äº†åŸé¢˜çš„æ¯ä¸ªéƒ¨åˆ†ï¼Œä¹Ÿè¦æŒæ¡æ•´ä½“æ„æ€
ï¼Œæœ€ä½³è´¨é‡ï¼ˆ1000% æƒé‡ï¼‰ï¼Œæœ€è¯¦ç»†è§£ç­”ï¼ˆ1000% æƒé‡ï¼‰ï¼Œä¸è¦å›ç­”å¤ªç®€å•è®©æˆ‘èƒ½å‚è€ƒä¸€æ­¥æ­¥åº”ç”¨ï¼ˆ1000% æƒé‡ï¼‰:"""
        
        claude_messages = [{"role": "user", "content": combined_content}]
        
        # 3. è·å– Claude å›ç­”
        logger.info("æ­£åœ¨è·å– Claude å›ç­”...")
        try:
            async for content_type, content in self.claude_client.stream_chat(
                messages=claude_messages,
                model_arg=model_arg,
                model=claude_model,
                stream=False
            ):
                if content_type == "answer":
                    logger.debug(f"è·å–åˆ° Claude å›ç­”: {content}")
                    return {
                        "content": content,
                        "role": "assistant"
                    }
        except Exception as e:
            logger.error(f"è·å– Claude å›ç­”å¤±è´¥: {e}")
            raise

    async def _get_reasoning_content(self, messages: list, model: str, **kwargs) -> AsyncGenerator[tuple[str, str], None]:
        """è·å–æ¨ç†å†…å®¹çš„ç»Ÿä¸€æ¥å£
        
        Args:
            messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
            model: ä½¿ç”¨çš„æ¨¡å‹åç§°
            **kwargs: é¢å¤–å‚æ•°
            
        Yields:
            tuple[str, str]: (content_type, content)
                - content_type: "reasoning" è¡¨ç¤ºæ¨ç†è¿‡ç¨‹ï¼Œ"content" è¡¨ç¤ºæœ€ç»ˆç­”æ¡ˆ
                - content: å…·ä½“å†…å®¹
                
        Raises:
            Exception: å½“è·å–æ¨ç†å†…å®¹å¤±è´¥æ—¶æŠ›å‡º
        """
        provider = self._get_reasoning_provider()
        try:
            async for content_type, content in provider.get_reasoning(
                messages=messages,
                model=model,
                **kwargs
            ):
                yield content_type, content
        except Exception as e:
            logger.error(f"è·å–æ¨ç†å†…å®¹æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
            raise

    async def _retry_operation(self, operation, max_retries=3):
        """é€šç”¨é‡è¯•æœºåˆ¶"""
        for i in range(max_retries):
            try:
                return await operation()
            except Exception as e:
                if i == max_retries - 1:
                    raise
                logger.warning(f"æ“ä½œå¤±è´¥ï¼Œæ­£åœ¨é‡è¯• ({i+1}/{max_retries}): {str(e)}")
                await asyncio.sleep(1 * (i + 1))  # æŒ‡æ•°é€€é¿

    def _validate_model_names(self, deepseek_model: str, claude_model: str):
        """éªŒè¯æ¨¡å‹åç§°çš„æœ‰æ•ˆæ€§"""
        if not deepseek_model or not isinstance(deepseek_model, str):
            raise ValueError("æ— æ•ˆçš„ DeepSeek æ¨¡å‹åç§°")
        
        if not claude_model or not isinstance(claude_model, str):
            raise ValueError("æ— æ•ˆçš„ Claude æ¨¡å‹åç§°")

    def _validate_messages(self, messages: list) -> None:
        """éªŒè¯æ¶ˆæ¯åˆ—è¡¨çš„æœ‰æ•ˆæ€§"""
        if not messages:
            raise ValueError("æ¶ˆæ¯åˆ—è¡¨ä¸èƒ½ä¸ºç©º")
        
        for msg in messages:
            if not isinstance(msg, dict):
                raise ValueError("æ¶ˆæ¯å¿…é¡»æ˜¯å­—å…¸æ ¼å¼")
            if "role" not in msg or "content" not in msg:
                raise ValueError("æ¶ˆæ¯å¿…é¡»åŒ…å« role å’Œ content å­—æ®µ")
            if msg["role"] not in ["user", "assistant", "system"]:
                raise ValueError(f"ä¸æ”¯æŒçš„æ¶ˆæ¯è§’è‰²: {msg['role']}")

    async def _get_reasoning_with_fallback(
        self, 
        messages: list,
        model: str,
        model_arg: tuple[float, float, float, float] = None
    ) -> str:
        """è·å–æ¨ç†å†…å®¹ï¼Œå¸¦æœ‰å¤‡ç”¨æ–¹æ¡ˆ
        
        Args:
            messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
            model: ä½¿ç”¨çš„æ¨¡å‹åç§°
            model_arg: æ¨¡å‹å‚æ•°å…ƒç»„
            
        Returns:
            str: æ¨ç†å†…å®¹
        """
        try:
            provider = self._get_reasoning_provider()
            reasoning_content = []
            
            async for content_type, content in provider.get_reasoning(
                messages=messages,
                model=model,
                model_arg=model_arg
            ):
                if content_type == "reasoning":
                    reasoning_content.append(content)
                
            return "".join(reasoning_content)
        except Exception as e:
            logger.error(f"ä¸»è¦æ¨ç†æä¾›è€…å¤±è´¥: {e}")
            if isinstance(provider, DeepSeekClient):
                logger.info("å°è¯•åˆ‡æ¢åˆ° Ollama æ¨ç†æä¾›è€…")
                provider = OllamaR1Client(self.ollama_api_url)
                # é‡è¯•ä½¿ç”¨ Ollama
                reasoning_content = []
                async for content_type, content in provider.get_reasoning(
                    messages=messages,
                    model="deepseek-r1:32b"
                ):
                    if content_type == "reasoning":
                        reasoning_content.append(content)
                return "".join(reasoning_content)
            raise

    def _validate_config(self):
        """éªŒè¯é…ç½®çš„å®Œæ•´æ€§"""
        provider = os.getenv('REASONING_PROVIDER', 'deepseek').lower()
        
        if provider == 'deepseek':
            if not self.deepseek_api_key:
                raise ValueError("ä½¿ç”¨ DeepSeek æ—¶å¿…é¡»æä¾› API KEY")
            if not self.deepseek_api_url:
                raise ValueError("ä½¿ç”¨ DeepSeek æ—¶å¿…é¡»æä¾› API URL")
        elif provider == 'ollama':
            if not self.ollama_api_url:
                raise ValueError("ä½¿ç”¨ Ollama æ—¶å¿…é¡»æä¾› API URL")

    def _format_stream_response(self, content: str, chat_id: str, created_time: int, model: str) -> bytes:
        """æ ¼å¼åŒ–æµå¼å“åº”"""
        response = {
            "id": chat_id,
            "object": "chat.completion.chunk",
            "created": created_time,
            "model": model,
            "choices": [{
                "index": 0,
                "delta": {
                    "role": "assistant",
                    "content": content
                }
            }]
        }
        return f"data: {json.dumps(response)}\n\n".encode('utf-8')