# DeepClaude 项目实施部署指南

本文档详细说明了 DeepClaude 项目的部署流程，包括环境准备、配置说明和多种部署方案。

## 目录

- [环境要求](#环境要求)
- [本地开发环境配置](#本地开发环境配置)
- [Docker 部署方案](#docker-部署方案)
- [云服务器部署方案](#云服务器部署方案)
- [Dify 部署方案](#dify-部署方案)
- [项目验证](#项目验证)
- [常见问题排查](#常见问题排查)
- [性能优化建议](#性能优化建议)

## 环境要求

### 系统要求

- 操作系统：Linux、macOS 或 Windows
- CPU：1核或更高
- 内存：2GB或更高
- 磁盘空间：至少500MB可用空间

### 基础环境

- Python 3.11 或更高版本
- uv 包管理器（推荐）或 pip
- Git（用于版本控制和代码获取）
- Docker（可选，用于容器化部署）

### 依赖包

项目主要依赖以下包（详见pyproject.toml）：

```toml
dependencies = [
    "aiohttp>=3.11.11",     # 异步HTTP客户端/服务器框架
    "colorlog>=6.9.0",      # 彩色日志输出
    "fastapi>=0.115.8",     # 现代化的Web框架
    "python-dotenv>=1.0.1", # 环境变量管理
    "tiktoken>=0.8.0",      # Token计数工具
    "uvicorn>=0.34.0",      # ASGI服务器
]
```

### 网络要求

- 稳定的互联网连接
- 能够访问以下API服务：
  - Anthropic API (claude-3系列模型)
  - DeepSeek API (deepseek-reasoner模型)
  - 或者 OpenRouter/OneAPI 等中转服务
- 建议使用HTTPS进行安全通信

## 本地开发环境配置

### 1. 安装 uv 包管理器

```bash
# 使用 pip 安装 uv
pip install uv

# 或使用 curl 安装（推荐）
curl -LsSf https://astral.sh/uv/install.sh | sh
```

### 2. 项目依赖安装

```bash
# 克隆项目
git clone https://github.com/ErlichLiu/DeepClaude.git
cd DeepClaude

# 使用 uv 创建虚拟环境并安装依赖
uv sync

# 激活虚拟环境
# Windows
.venv\Scripts\activate
# Linux/macOS
source .venv/bin/activate
```

### 3. 环境变量配置

1. 复制环境变量模板：
```bash
cp .env.example .env
```

2. 编辑 .env 文件，配置必要的环境变量：
```env
# API 访问控制
ALLOW_API_KEY=your_allow_api_key  # 设置访问API的密钥
ALLOW_ORIGINS="*"                # 允许的跨域来源

# DeepSeek API 配置
DEEPSEEK_API_KEY=your_deepseek_api_key
DEEPSEEK_API_URL=https://api.deepseek.com/v1/chat/completions
DEEPSEEK_MODEL=deepseek-reasoner
IS_ORIGIN_REASONING=true

# Claude API 配置
CLAUDE_API_KEY=your_claude_api_key
CLAUDE_MODEL=claude-3-5-sonnet-20241022
CLAUDE_PROVIDER=anthropic
CLAUDE_API_URL=https://api.anthropic.com/v1/messages

# 日志配置
LOG_LEVEL=INFO  # 可选：DEBUG, INFO, WARNING, ERROR
```

### 4. 启动本地服务

```bash
# 基本启动
uvicorn app.main:app --host 0.0.0.0 --port 8000

# 开发模式（自动重载）
uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
```

## Docker 部署方案

### 方案一：使用 docker-compose（推荐）

1. 确保已安装 Docker 和 Docker Compose

2. 创建 docker-compose.yml：
```yaml
services:
  deepclaude:
    image: ghcr.io/erlichliu/deepclaude:latest
    ports:
      - "8000:8000"
    environment:
      ALLOW_API_KEY: your_allow_api_key
      ALLOW_ORIGINS: "*"
      DEEPSEEK_API_KEY: your_deepseek_api_key
      DEEPSEEK_API_URL: https://api.deepseek.com/v1/chat/completions
      DEEPSEEK_MODEL: deepseek-reasoner
      IS_ORIGIN_REASONING: true
      CLAUDE_API_KEY: your_claude_api_key
      CLAUDE_MODEL: claude-3-5-sonnet-20241022
      CLAUDE_PROVIDER: anthropic
      CLAUDE_API_URL: https://api.anthropic.com/v1/messages
      LOG_LEVEL: INFO
    restart: always
```

3. 启动服务：
```bash
docker-compose up -d
```

### 方案二：手动构建镜像

```bash
# 构建镜像
docker build -t deepclaude:latest .

# 运行容器
docker run -d \
    -p 8000:8000 \
    -e ALLOW_API_KEY=your_allow_api_key \
    -e ALLOW_ORIGINS="*" \
    -e DEEPSEEK_API_KEY=your_deepseek_api_key \
    -e DEEPSEEK_API_URL=https://api.deepseek.com/v1/chat/completions \
    -e DEEPSEEK_MODEL=deepseek-reasoner \
    -e IS_ORIGIN_REASONING=true \
    -e CLAUDE_API_KEY=your_claude_api_key \
    -e CLAUDE_MODEL=claude-3-5-sonnet-20241022 \
    -e CLAUDE_PROVIDER=anthropic \
    -e CLAUDE_API_URL=https://api.anthropic.com/v1/messages \
    -e LOG_LEVEL=INFO \
    --restart always \
    deepclaude:latest
```

## 云服务器部署方案

### 腾讯云ECS部署

1. 创建并配置实例：
   - 选择 Ubuntu 20.04 LTS
   - 开放 8000 端口
   - 配置安全组

2. 安装基础环境：
```bash
# 更新系统
sudo apt update && sudo apt upgrade -y

# 安装必要工具
sudo apt install -y curl git

# 安装Docker
curl -fsSL https://get.docker.com | sh
sudo usermod -aG docker $USER

# 安装Docker Compose
sudo curl -L "https://github.com/docker/compose/releases/download/v2.24.5/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
sudo chmod +x /usr/local/bin/docker-compose
```

3. 部署应用：
```bash
# 创建项目目录
mkdir deepclaude && cd deepclaude

# 创建docker-compose.yml（内容同上）

# 启动服务
docker-compose up -d
```

4. 配置Nginx反向代理（可选）：
```nginx
server {
    listen 80;
    server_name your_domain.com;

    location / {
        proxy_pass http://localhost:8000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
}
```

## 项目验证

### 1. 服务状态检查

访问API文档：
```
http://localhost:8000/docs
```

### 2. API测试

```bash
curl -X POST "http://localhost:8000/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer your_allow_api_key" \
  -d '{
    "messages": [{"role": "user", "content": "Hello"}],
    "model": "deepclaude"
  }'
```

### 3. Chatbox配置

#### NextChat
1. **准备工作**
   - 确保已安装 Docker Desktop（Mac/Windows）或 Docker Engine（Linux）
   - 创建项目目录：`mkdir nextchat && cd nextchat`

2. **配置文件准备**
   - 创建 `.env` 文件：
     ```bash
     # NextChat 访问密码（必需）
     CODE=your_access_password
     # 允许的 API Key 列表，多个密钥用逗号分隔
     ALLOW_API_KEY=sk-allowed-key-1,sk-allowed-key-2
     # API 基础配置
     BASE_URL=http://127.0.0.1:8000/v1
     # 自定义模型配置
     CUSTOM_MODELS="+deepclaude,deepclaude=Deep Claude"
     # 可选配置
     TIMEOUT_MS=100000
     HIDE_USER_API_KEY=0
     ```

   - 创建 `docker-compose.yml` 文件：
     ```yaml
     version: '3'
     services:
       nextchat:
         image: yidadaa/chatgpt-next-web
         platform: linux/arm64  # M1/M2/M4 Mac 使用此配置
         container_name: nextchat
         ports:
           - "3000:3000"
         env_file:
           - .env
         restart: always
         volumes:
           - ./data:/app/data
     ```

3. **启动服务**
   ```bash
   docker pull yidadaa/chatgpt-next-web
   docker-compose up -d
   ```

4. **配置 NextChat**
   - 访问 `http://localhost:3000`
   - 输入在 `.env` 中设置的访问密码
   - 点击左下角设置图标
   - 在「API设置」中选择「自定义API」
   - 填写配置：
     - API Key: 使用 ALLOW_API_KEY 中设置的任一密钥
     - Base URL: http://127.0.0.1:8000/v1（本地部署）或你的服务器地址
     - 模型: deepclaude

5. **维护操作**
   ```bash
   # 查看日志
   docker logs -f nextchat
   # 更新服务
   docker-compose down
   docker pull yidadaa/chatgpt-next-web
   docker-compose up -d
   ```

#### ChatBox
1. 下载并安装 [ChatBox](https://chatboxai.app/zh)
2. 打开ChatBox设置
3. 添加自定义API：
   - 名称：DeepClaude
   - Base URL：http://your_server:8000/v1
   - API Key：你的ALLOW_API_KEY
   - 模型：deepclaude
4. 保存配置后即可在对话中选择DeepClaude模型

#### LobeChat
1. 访问 [LobeChat](https://lobechat.com/) 或部署自己的实例
2. 进入设置页面
3. 选择「Language Model」-「Add Custom Model」
4. 添加自定义模型：
   - 名称：DeepClaude
   - Endpoint：http://your_server:8000/v1
   - API Key：你的ALLOW_API_KEY
   - 模型：deepclaude
5. 保存后即可在会话中使用DeepClaude模型

> 注意：如果是服务器部署，请将 http://127.0.0.1:8000 替换为你的服务器地址

## 常见问题排查

1. 服务无法启动：
   - 检查端口占用
   - 验证环境变量配置
   - 查看日志：`docker-compose logs -f`

2. API调用失败：
   - 确认API Key正确
   - 检查网络连接
   - 验证请求格式

3. 性能问题：
   - 调整worker数量
   - 优化并发设置
   - 监控资源使用

## 性能优化建议

1. 生产环境配置：
   - 使用Nginx反向代理
   - 启用HTTPS
   - 配置日志轮转

2. 资源优化：
   - 调整uvicorn workers
   - 设置合理的超时时间
   - 使用CDN加速

3. 监控：
   - 配置健康检查
   - 设置资源告警
   - 实施日志分析
```

## Dify 部署方案

### 系统要求

- Docker Engine 20.10.0 或更高版本
- Docker Compose V2 或更高版本
- 最小硬件配置：
  - CPU: 2核心
  - 内存: 8GB
  - 存储空间: 20GB

### 部署步骤

#### 1. 获取源码

```bash
# 克隆项目仓库
git clone https://github.com/langgenius/dify.git
# 进入 docker 目录
cd dify/docker
```

#### 2. 配置环境变量

```bash
# 复制环境配置文件
cp .env.example .env
```

#### 3. 修改关键环境变量

编辑 `.env` 文件，设置以下重要参数：

```ini
# 核心配置
CONSOLE_URL=http://localhost:3000
API_URL=http://localhost:5001
APP_API_URL=http://localhost:5001

# 数据库配置
DB_HOST=postgres
DB_PORT=5432
DB_USERNAME=postgres
DB_PASSWORD=dify_pass
DB_DATABASE=dify

# Redis配置
REDIS_HOST=redis
REDIS_PORT=6379
REDIS_PASSWORD=dify_pass

# 向量数据库配置（使用 Weaviate）
VECTOR_STORE=weaviate
WEAVIATE_ENDPOINT=http://weaviate:8080
WEAVIATE_API_KEY=your-weaviate-api-key

# LLM 配置
# 支持 openai, azure_openai, anthropic
OPENAI_API_KEY=your-openai-api-key
OPENAI_API_BASE=https://api.openai.com/v1
OPENAI_API_MODEL=gpt-3.5-turbo

# 文件存储配置
STORAGE_TYPE=local
LOCAL_STORAGE_PATH=/app/api/storage
```

#### 4. 启动服务

```bash
# 拉取并构建镜像
docker compose pull

# 启动所有服务
docker compose up -d

# 查看服务状态
docker compose ps
```

#### 5. 初始化数据库

```bash
# 执行数据库迁移
docker compose exec api flask db upgrade

# 创建管理员账号
docker compose exec api flask create-admin
```

### 访问服务

- 控制台访问地址：`http://localhost:3000`
- API 服务地址：`http://localhost:5001`

### 常用维护命令

```bash
# 查看日志
docker compose logs -f

# 重启服务
docker compose restart

# 停止服务
docker compose down

# 更新版本
docker compose pull
docker compose down
docker compose up -d
```

### 数据持久化

默认的数据存储位置：
- PostgreSQL: `./volumes/postgres`
- Redis: `./volumes/redis`
- 文件存储: `./volumes/storage`

### 自定义模型配置

#### 1. 配置方式概述

Dify 支持两种主要的模型配置方式：
1. 预定义模型（Predefined Models）：使用现有的模型提供商
2. 自定义模型（Customizable Models）：集成您自己部署的模型

#### 2. 添加自定义模型

1. 在 `api/core/model_runtime/providers` 目录下创建新的提供商目录
2. 创建 `provider.yaml` 配置文件：

```yaml
provider: "your_provider_name"
name: "Your Provider Display Name"
description: "Your provider description"
provider_type: "custom"
supported_model_types:
  - "llm"
  - "text-embedding"  # 如果支持的话
  - "rerank"         # 如果支持的话
  - "speech2text"    # 如果支持的话

model_deployment:
  type: "customizable-model"
  default_model: ""

provider_credential_schema:
  credential_form_schema:
    - variable: "api_base"
      label: "API Base URL"
      type: "text"
      required: true
      placeholder: "https://your-model-api-endpoint"
    - variable: "api_key"
      label: "API Key"
      type: "secret"
      required: true
      placeholder: "Your API key"
```

3. 实现模型接口：

```python
from core.model_runtime.model_providers.__base.large_language_model import LargeLanguageModel
from core.model_runtime.model_providers.__base.model_provider import ModelProvider

class YourCustomProvider(ModelProvider):
    def __init__(self):
        super().__init__('your_provider_name')
    
    def get_llm_model(self) -> LargeLanguageModel:
        return YourCustomLLM(self)

class YourCustomLLM(LargeLanguageModel):
    def _chat_completion(self, messages: list, model: str, temperature: float, max_tokens: int, **kwargs):
        # 实现您的模型调用逻辑
        pass
```

```bash
# 查看日志
docker compose logs -f

# 重启服务
docker compose restart

# 停止服务
docker compose down

# 更新版本
docker compose pull
docker compose down
docker compose up -d
```

### 数据持久化

默认的数据存储位置：
- PostgreSQL: `./volumes/postgres`
- Redis: `./volumes/redis`
- 文件存储: `./volumes/storage`

### 自定义模型配置

#### 1. 配置方式概述

Dify 支持两种主要的模型配置方式：
1. 预定义模型（Predefined Models）：使用现有的模型提供商
2. 自定义模型（Customizable Models）：集成您自己部署的模型

#### 2. 添加自定义模型

1. 在 `api/core/model_runtime/providers` 目录下创建新的提供商目录
2. 创建 `provider.yaml` 配置文件：

```yaml
provider: "your_provider_name"
name: "Your Provider Display Name"
description: "Your provider description"
provider_type: "custom"
supported_model_types:
  - "llm"
  - "text-embedding"  # 如果支持的话
  - "rerank"         # 如果支持的话
  - "speech2text"    # 如果支持的话

model_deployment:
  type: "customizable-model"
  default_model: ""

provider_credential_schema:
  credential_form_schema:
    - variable: "api_base"
      label: "API Base URL"
      type: "text"
      required: true
      placeholder: "https://your-model-api-endpoint"
    - variable: "api_key"
      label: "API Key"
      type: "secret"
      required: true
      placeholder: "Your API key"
```

3. 实现模型接口：

```python
from core.model_runtime.model_providers.__base.large_language_model import LargeLanguageModel
from core.model_runtime.model_providers.__base.model_provider import ModelProvider

class YourCustomProvider(ModelProvider):
    def __init__(self):
        super().__init__('your_provider_name')
    
    def get_llm_model(self) -> LargeLanguageModel:
        return YourCustomLLM(self)

class YourCustomLLM(LargeLanguageModel):
    def _chat_completion(self, messages: list, model: str, temperature: float, max_tokens: int, **kwargs):
        # 实现您的模型调用逻辑
        pass
```

```bash
# 查看日志
docker compose logs -f

# 重启服务
docker compose restart

# 停止服务
docker compose down

# 更新版本
docker compose pull
docker compose down
docker compose up -d
```

### 数据持久化

默认的数据存储位置：
- PostgreSQL: `./volumes/postgres`
- Redis: `./volumes/redis`
- 文件存储: `./volumes/storage`

### 自定义模型配置

#### 1. 配置方式概述

Dify 支持两种主要的模型配置方式：
1. 预定义模型（Predefined Models）：使用现有的模型提供商
2. 自定义模型（Customizable Models）：集成您自己部署的模型

#### 2. 添加自定义模型

1. 在 `api/core/model_runtime/providers` 目录下创建新的提供商目录
2. 创建 `provider.yaml` 配置文件：

```yaml
provider: "your_provider_name"
name: "Your Provider Display Name"
description: "Your provider description"
provider_type: "custom"
supported_model_types:
  - "llm"
  - "text-embedding"  # 如果支持的话
  - "rerank"         # 如果支持的话
  - "speech2text"    # 如果支持的话

model_deployment:
  type: "customizable-model"
  default_model: ""

provider_credential_schema:
  credential_form_schema:
    - variable: "api_base"
      label: "API Base URL"
      type: "text"
      required: true
      placeholder: "https://your-model-api-endpoint"
    - variable: "api_key"
      label: "API Key"
      type: "secret"
      required: true
      placeholder: "Your API key"
```

3. 实现模型接口：

```python
from core.model_runtime.model_providers.__base.large_language_model import LargeLanguageModel
from core.model_runtime.model_providers.__base.model_provider import ModelProvider

class YourCustomProvider(ModelProvider):
    def __init__(self):
        super().__init__('your_provider_name')
    
    def get_llm_model(self) -> LargeLanguageModel:
        return YourCustomLLM(self)

class YourCustomLLM(LargeLanguageModel):
    def _chat_completion(self, messages: list, model: str, temperature: float, max_tokens: int, **kwargs):
        # 实现您的模型调用逻辑
        pass
```

```bash
# 查看日志
docker compose logs -f

# 重启服务
docker compose restart

# 停止服务
docker compose down

# 更新版本
docker compose pull
docker compose down
docker compose up -d
```

### 数据持久化

默认的数据存储位置：
- PostgreSQL: `./volumes/postgres`
- Redis: `./volumes/redis`
- 文件存储: `./volumes/storage`

### 自定义模型配置

#### 1. 配置方式概述

Dify 支持两种主要的模型配置方式：
1. 预定义模型（Predefined Models）：使用现有的模型提供商
2. 自定义模型（Customizable Models）：集成您自己部署的模型

#### 2. 添加自定义模型

1. 在 `api/core/model_runtime/providers` 目录下创建新的提供商目录
2. 创建 `provider.yaml` 配置文件：

```yaml
provider: "your_provider_name"
name: "Your Provider Display Name"
description: "Your provider description"
provider_type: "custom"
supported_model_types:
  - "llm"
  - "text-embedding"  # 如果支持的话
  - "rerank"         # 如果支持的话
  - "speech2text"    # 如果支持的话

model_deployment:
  type: "customizable-model"
  default_model: ""

provider_credential_schema:
  credential_form_schema:
    - variable: "api_base"
      label: "API Base URL"
      type: "text"
      required: true
      placeholder: "https://your-model-api-endpoint"
    - variable: "api_key"
      label: "API Key"
      type: "secret"
      required: true
      placeholder: "Your API key"
```

3. 实现模型接口：

```python
from core.model_runtime.model_providers.__base.large_language_model import LargeLanguageModel
from core.model_runtime.model_providers.__base.model_provider import ModelProvider

class YourCustomProvider(ModelProvider):
    def __init__(self):
        super().__init__('your_provider_name')
    
    def get_llm_model(self) -> LargeLanguageModel:
        return YourCustomLLM(self)

class YourCustomLLM(LargeLanguageModel):
    def _chat_completion(self, messages: list, model: str, temperature: float, max_tokens: int, **kwargs):
        # 实现您的模型调用逻辑
        pass
```

```bash
# 查看日志
docker compose logs -f

# 重启服务
docker compose restart

# 停止服务
docker compose down

# 更新版本
docker compose pull
docker compose down
docker compose up -d
```

### 数据持久化

默认的数据存储位置：
- PostgreSQL: `./volumes/postgres`
- Redis: `./volumes/redis`
- 文件存储: `./volumes/storage`

### 自定义模型配置

#### 1. 配置方式概述

Dify 支持两种主要的模型配置方式：
1. 预定义模型（Predefined Models）：使用现有的模型提供商
2. 自定义模型（Customizable Models）：集成您自己部署的模型

#### 2. 添加自定义模型

1. 在 `api/core/model_runtime/providers` 目录下创建新的提供商目录
2. 创建 `provider.yaml` 配置文件：

```yaml
provider: "your_provider_name"
name: "Your Provider Display Name"
description: "Your provider description"
provider_type: "custom"
supported_model_types:
  - "llm"
  - "text-embedding"  # 如果支持的话
  - "rerank"         # 如果支持的话
  - "speech2text"    # 如果支持的话

model_deployment:
  type: "customizable-model"
  default_model: ""

provider_credential_schema:
  credential_form_schema:
    - variable: "api_base"
      label: "API Base URL"
      type: "text"
      required: true
      placeholder: "https://your-model-api-endpoint"
    - variable: "api_key"
      label: "API Key"
      type: "secret"
      required: true
      placeholder: "Your API key"
```

3. 实现模型接口：

```python
from core.model_runtime.model_providers.__base.large_language_model import LargeLanguageModel
from core.model_runtime.model_providers.__base.model_provider import ModelProvider

class YourCustomProvider(ModelProvider):
    def __init__(self):
        super().__init__('your_provider_name')
    
    def get_llm_model(self) -> LargeLanguageModel:
        return YourCustomLLM(self)

class YourCustomLLM(LargeLanguageModel):
    def _chat_completion(self, messages: list, model: str, temperature: float, max_tokens: int, **kwargs):
        # 实现您的模型调用逻辑
        pass
```

```bash
# 查看日志
docker compose logs -f

# 重启服务
docker compose restart

# 停止服务
docker compose down

# 更新版本
docker compose pull
docker compose down
docker compose up -d
```

### 数据持久化

默认的数据存储位置：
- PostgreSQL: `./volumes/postgres`
- Redis: `./volumes/redis`
- 文件存储: `./volumes/storage`

### 自定义模型配置

#### 1. 配置方式概述

Dify 支持两种主要的模型配置方式：
1. 预定义模型（Predefined Models）：使用现有的模型提供商
2. 自定义模型（Customizable Models）：集成您自己部署的模型

#### 2. 添加自定义模型

1. 在 `api/core/model_runtime/providers` 目录下创建新的提供商目录
2. 创建 `provider.yaml` 配置文件：

```yaml
provider: "your_provider_name"
name: "Your Provider Display Name"
description: "Your provider description"
provider_type: "custom"
supported_model_types:
  - "llm"
  - "text-embedding"  # 如果支持的话
  - "rerank"         # 如果支持的话
  - "speech2text"    # 如果支持的话

model_deployment:
  type: "customizable-model"
  default_model: ""

provider_credential_schema:
  credential_form_schema:
    - variable: "api_base"
      label: "API Base URL"
      type: "text"
      required: true
      placeholder: "https://your-model-api-endpoint"
    - variable: "api_key"
      label: "API Key"
      type: "secret"
      required: true
      placeholder: "Your API key"
```

3. 实现模型接口：

```python
from core.model_runtime.model_providers.__base.large_language_model import LargeLanguageModel
from core.model_runtime.model_providers.__base.model_provider import ModelProvider

class YourCustomProvider(ModelProvider):
    def __init__(self):
        super().__init__('your_provider_name')
    
    def get_llm_model(self) -> LargeLanguageModel:
        return YourCustomLLM(self)

class YourCustomLLM(LargeLanguageModel):
    def _chat_completion(self, messages: list, model: str, temperature: float, max_tokens: int, **kwargs):
        # 实现您的模型调用逻辑
        pass
```

```bash
# 查看日志
docker compose logs -f

# 重启服务
docker compose restart

# 停止服务
docker compose down

# 更新版本
docker compose pull
docker compose down
docker compose up -d
```

### 数据持久化

默认的数据存储位置：
- PostgreSQL: `./volumes/postgres`
- Redis: `./volumes/redis`
- 文件存储: `./volumes/storage`

### 自定义模型配置

#### 1. 配置方式概述

Dify 支持两种主要的模型配置方式：
1. 预定义模型（Predefined Models）：使用现有的模型提供商
2. 自定义模型（Customizable Models）：集成您自己部署的模型

#### 2. 添加自定义模型

1. 在 `api/core/model_runtime/providers` 目录下创建新的提供商目录
2. 创建 `provider.yaml` 配置文件：

```yaml
provider: "your_provider_name"
name: "Your Provider Display Name"
description: "Your provider description"
provider_type: "custom"
supported_model_types:
  - "llm"
  - "text-embedding"  # 如果支持的话
  - "rerank"         # 如果支持的话
  - "speech2text"    # 如果支持的话

model_deployment:
  type: "customizable-model"
  default_model: ""

provider_credential_schema:
  credential_form_schema:
    - variable: "api_base"
      label: "API Base URL"
      type: "text"
      required: true
      placeholder: "https://your-model-api-endpoint"
    - variable: "api_key"
      label: "API Key"
      type: "secret"
      required: true
      placeholder: "Your API key"
```

3. 实现模型接口：

```python
from core.model_runtime.model_providers.__base.large_language_model import LargeLanguageModel
from core.model_runtime.model_providers.__base.model_provider import ModelProvider

class YourCustomProvider(ModelProvider):
    def __init__(self):
        super().__init__('your_provider_name')
    
    def get_llm_model(self) -> LargeLanguageModel:
        return YourCustomLLM(self)

class YourCustomLLM(LargeLanguageModel):
    def _chat_completion(self, messages: list, model: str, temperature: float, max_tokens: int, **kwargs):
        # 实现您的模型调用逻辑
        pass
```

```bash
# 查看日志
docker compose logs -f

# 重启服务
docker compose restart

# 停止服务
docker compose down

# 更新版本
docker compose pull
docker compose down
docker compose up -d
```

### 数据持久化

默认的数据存储位置：
- PostgreSQL: `./volumes/postgres`
- Redis: `./volumes/redis`
- 文件存储: `./volumes/storage`

### 自定义模型配置

#### 1. 配置方式概述

Dify 支持两种主要的模型配置方式：
1. 预定义模型（Predefined Models）：使用现有的模型提供商
2. 自定义模型（Customizable Models）：集成您自己部署的模型

#### 2. 添加自定义模型

1. 在 `api/core/model_runtime/providers` 目录下创建新的提供商目录
2. 创建 `provider.yaml` 配置文件：

```yaml
provider: "your_provider_name"
name: "Your Provider Display Name"
description: "Your provider description"
provider_type: "custom"
supported_model_types:
  - "llm"
  - "text-embedding"  # 如果支持的话
  - "rerank"         # 如果支持的话
  - "speech2text"    # 如果支持的话

model_deployment:
  type: "customizable-model"
  default_model: ""

provider_credential_schema:
  credential_form_schema:
    - variable: "api_base"
      label: "API Base URL"
      type: "text"
      required: true
      placeholder: "https://your-model-api-endpoint"
    - variable: "api_key"
      label: "API Key"
      type: "secret"
      required: true
      placeholder: "Your API key"
```

3. 实现模型接口：

```python
from core.model_runtime.model_providers.__base.large_language_model import LargeLanguageModel
from core.model_runtime.model_providers.__base.model_provider import ModelProvider

class YourCustomProvider(ModelProvider):
    def __init__(self):
        super().__init__('your_provider_name')
    
    def get_llm_model(self) -> LargeLanguageModel:
        return YourCustomLLM(self)

class YourCustomLLM(LargeLanguageModel):
    def _chat_completion(self, messages: list, model: str, temperature: float, max_tokens: int, **kwargs):
        # 实现您的模型调用逻辑
        pass
```

```bash
# 查看日志
docker compose logs -f

# 重启服务
docker compose restart

# 停止服务
docker compose down

# 更新版本
docker compose pull
docker compose down
docker compose up -d
```

### 数据持久化

默认的数据存储位置：
- PostgreSQL: `./volumes/postgres`
- Redis: `./volumes/redis`
- 文件存储: `./volumes/storage`

### 自定义模型配置

#### 1. 配置方式概述

Dify 支持两种主要的模型配置方式：
1. 预定义模型（Predefined Models）：使用现有的模型提供商
2. 自定义模型（Customizable Models）：集成您自己部署的模型

#### 2. 添加自定义模型

1. 在 `api/core/model_runtime/providers` 目录下创建新的提供商目录
2. 创建 `provider.yaml` 配置文件：

```yaml
provider: "your_provider_name"
name: "Your Provider Display Name"
description: "Your provider description"
provider_type: "custom"
supported_model_types:
  - "llm"
  - "text-embedding"  # 如果支持的话
  - "rerank"         # 如果支持的话
  - "speech2text"    # 如果支持的话

model_deployment:
  type: "customizable-model"
  default_model: ""

provider_credential_schema:
  credential_form_schema:
    - variable: "api_base"
      label: "API Base URL"
      type: "text"
      required: true
      placeholder: "https://your-model-api-endpoint"
    - variable: "api_key"
      label: "API Key"
      type: "secret"
      required: true
      placeholder: "Your API key"
```

3. 实现模型接口：

```python
from core.model_runtime.model_providers.__base.large_language_model import LargeLanguageModel
from core.model_runtime.model_providers.__base.model_provider import ModelProvider

class YourCustomProvider(ModelProvider):
    def __init__(self):
        super().__init__('your_provider_name')
    
    def get_llm_model(self) -> LargeLanguageModel:
        return YourCustomLLM(self)

class YourCustomLLM(LargeLanguageModel):
    def _chat_completion(self, messages: list, model: str, temperature: float, max_tokens: int, **kwargs):
        # 实现您的模型调用逻辑
        pass
```

```bash
# 查看日志
docker compose logs -f

# 重启服务
docker compose restart

# 停止服务
docker compose down

# 更新版本
docker compose pull
docker compose down
docker compose up -d
```

### 数据持久化

默认的数据存储位置：
- PostgreSQL: `./volumes/postgres`
- Redis: `./volumes/redis`
- 文件存储: `./volumes/storage`

### 自定义模型配置

#### 1. 配置方式概述

Dify 支持两种主要的模型配置方式：
1. 预定义模型（Predefined Models）：使用现有的模型提供商
2. 自定义模型（Customizable Models）：集成您自己部署的模型

#### 2. 添加自定义模型

1. 在 `api/core/model_runtime/providers` 目录下创建新的提供商目录
2. 创建 `provider.yaml` 配置文件：

```yaml
provider: "your_provider_name"
name: "Your Provider Display Name"
description: "Your provider description"
provider_type: "custom"
supported_model_types:
  - "llm"
  - "text-embedding"  # 如果支持的话
  - "rerank"         # 如果支持的话
  - "speech2text"    # 如果支持的话

model_deployment:
  type: "customizable-model"
  default_model: ""

provider_credential_schema:
  credential_form_schema:
    - variable: "api_base"
      label: "API Base URL"
      type: "text"
      required: true
      placeholder: "https://your-model-api-endpoint"
    - variable: "api_key"
      label: "API Key"
      type: "secret"
      required: true
      placeholder: "Your API key"
```

3. 实现模型接口：

```python
from core.model_runtime.model_providers.__base.large_language_model import LargeLanguageModel
from core.model_runtime.model_providers.__base.model_provider import ModelProvider

class YourCustomProvider(ModelProvider):
    def __init__(self):
        super().__init__('your_provider_name')
    
    def get_llm_model(self) -> LargeLanguageModel:
        return YourCustomLLM(self)

class YourCustomLLM(LargeLanguageModel):
    def _chat_completion(self, messages: list, model: str, temperature: float, max_tokens: int, **kwargs):
        # 实现您的模型调用逻辑
        pass
```

```bash
# 查看日志
docker compose logs -f

# 重启服务
docker compose restart

# 停止服务
docker compose down

# 更新版本
docker compose pull
docker compose down
docker compose up -d
```

### 数据持久化

默认的数据存储位置：
- PostgreSQL: `./volumes/postgres`
- Redis: `./volumes/redis`
- 文件存储: `./volumes/storage`

### 自定义模型配置

#### 1. 配置方式概述

Dify 支持两种主要的模型配置方式：
1. 预定义模型（Predefined Models）：使用现有的模型提供商
2. 自定义模型（Customizable Models）：集成您自己部署的模型

#### 2. 添加自定义模型

1. 在 `api/core/model_runtime/providers` 目录下创建新的提供商目录
2. 创建 `provider.yaml` 配置文件：

```yaml
provider: "your_provider_name"
name: "Your Provider Display Name"
description: "Your provider description"
provider_type: "custom"
supported_model_types:
  - "llm"
  - "text-embedding"  # 如果支持的话
  - "rerank"         # 如果支持的话
  - "speech2text"    # 如果支持的话

model_deployment:
  type: "customizable-model"
  default_model: ""

provider_credential_schema:
  credential_form_schema:
    - variable: "api_base"
      label: "API Base URL"
      type: "text"
      required: true
      placeholder: "https://your-model-api-endpoint"
    - variable: "api_key"
      label: "API Key"
      type: "secret"
      required: true
      placeholder: "Your API key"
```

3. 实现模型接口：

```python
from core.model_runtime.model_providers.__base.large_language_model import LargeLanguageModel
from core.model_runtime.model_providers.__base.model_provider import ModelProvider

class YourCustomProvider(ModelProvider):
    def __init__(self):
        super().__init__('your_provider_name')
    
    def get_llm_model(self) -> LargeLanguageModel:
        return YourCustomLLM(self)

class YourCustomLLM(LargeLanguageModel):
    def _chat_completion(self, messages: list, model: str, temperature: float, max_tokens: int, **kwargs):
        # 实现您的模型调用逻辑
        pass
```

```bash
# 查看日志
docker compose logs -f

# 重启服务
docker compose restart

# 停止服务
docker compose down

# 更新版本
docker compose pull
docker compose down
docker compose up -d
```

### 数据持久化

默认的数据存储位置：
- PostgreSQL: `./volumes/postgres`
- Redis: `./volumes/redis`
- 文件存储: `./volumes/storage`

### 自定义模型配置

#### 1. 配置方式概述

Dify 支持两种主要的模型配置方式：
1. 预定义模型（Predefined Models）：使用现有的模型提供商
2. 自定义模型（Customizable Models）：集成您自己部署的模型

#### 2. 添加自定义模型

1. 在 `api/core/model_runtime/providers` 目录下创建新的提供商目录
2. 创建 `provider.yaml` 配置文件：

```yaml
provider: "your_provider_name"
name: "Your Provider Display Name"
description: "Your provider description"
provider_type: "custom"
supported_model_types:
  - "llm"
  - "text-embedding"  # 如果支持的话
  - "rerank"         # 如果支持的话
  - "speech2text"    # 如果支持的话

model_deployment:
  type: "customizable-model"
  default_model: ""

provider_credential_schema:
  credential_form_schema:
    - variable: "api_base"
      label: "API Base URL"
      type: "text"
      required: true
      placeholder: "https://your-model-api-endpoint"
    - variable: "api_key"
      label: "API Key"
      type: "secret"
      required: true
      placeholder: "Your API key"
```

3. 实现模型接口：

```python
from core.model_runtime.model_providers.__base.large_language_model import LargeLanguageModel
from core.model_runtime.model_providers.__base.model_provider import ModelProvider

class YourCustomProvider(ModelProvider):
    def __init__(self):
        super().__init__('your_provider_name')
    
    def get_llm_model(self) -> LargeLanguageModel:
        return YourCustomLLM(self)

class YourCustomLLM(LargeLanguageModel):
    def _chat_completion(self, messages: list, model: str, temperature: float, max_tokens: int, **kwargs):
        # 实现您的模型调用逻辑
        pass
```

```bash
# 查看日志
docker compose logs -f

# 重启服务
docker compose restart

# 停止服务
docker compose down

# 更新版本
docker compose pull
docker compose down
docker compose up -d
```

### 数据持久化

默认的数据存储位置：
- PostgreSQL: `./volumes/postgres`
- Redis: `./volumes/redis`
- 文件存储: `./volumes/storage`

### 自定义模型配置

#### 1. 配置方式概述

Dify 支持两种主要的模型配置方式：
1. 预定义模型（Predefined Models）：使用现有的模型提供商
2. 自定义模型（Customizable Models）：集成您自己部署的模型

#### 2. 添加自定义模型

1. 在 `api/core/model_runtime/providers` 目录下创建新的提供商目录
2. 创建 `provider.yaml` 配置文件：

```yaml
provider: "your_provider_name"
name: "Your Provider Display Name"
description: "Your provider description"
provider_type: "custom"
supported_model_types:
  - "llm"
  - "text-embedding"  # 如果支持的话
  - "rerank"         # 如果支持的话
  - "speech2text"    # 如果支持的话

model_deployment:
  type: "customizable-model"
  default_model: ""

provider_credential_schema:
  credential_form_schema:
    - variable: "api_base"
      label: "API Base URL"
      type: "text"
      required: true
      placeholder: "https://your-model-api-endpoint"
    - variable: "api_key"
      label: "API Key"
      type: "secret"
      required: true
      placeholder: "Your API key"
```

3. 实现模型接口：

```python
from core.model_runtime.model_providers.__base.large_language_model import LargeLanguageModel
from core.model_runtime.model_providers.__base.model_provider import ModelProvider

class YourCustomProvider(ModelProvider):
    def __init__(self):
        super().__init__('your_provider_name')
    
    def get_llm_model(self) -> LargeLanguageModel:
        return YourCustomLLM(self)

class YourCustomLLM(LargeLanguageModel):
    def _chat_completion(self, messages: list, model: str, temperature: float, max_tokens: int, **kwargs):
        # 实现您的模型调用逻辑
        pass
```

```bash
# 查看日志
docker compose logs -f

# 重启服务
docker compose restart

# 停止服务
docker compose down

# 更新版本
docker compose pull
docker compose down
docker compose up -d
```

### 数据持久化

默认的数据存储位置：
- PostgreSQL: `./volumes/postgres`
- Redis: `./volumes/redis`
- 文件存储: `./volumes/storage`

### 自定义模型配置

#### 1. 配置方式概述

Dify 支持两种主要的模型配置方式：
1. 预定义模型（Predefined Models）：使用现有的模型提供商
2. 自定义模型（Customizable Models）：集成您自己部署的模型

#### 2. 添加自定义模型

1. 在 `api/core/model_runtime/providers` 目录下创建新的提供商目录
2. 创建 `provider.yaml` 配置文件：

```yaml
provider: "your_provider_name"
name: "Your Provider Display Name"
description: "Your provider description"
provider_type: "custom"
supported_model_types:
  - "llm"
  - "text-embedding"  # 如果支持的话
  - "rerank"         # 如果支持的话
  - "speech2text"    # 如果支持的话

model_deployment:
  type: "customizable-model"
  default_model: ""

provider_credential_schema:
  credential_form_schema:
    - variable: "api_base"
      label: "API Base URL"
      type: "text"
      required: true
      placeholder: "https://your-model-api-endpoint"
    - variable: "api_key"
      label: "API Key"
      type: "secret"
      required: true
      placeholder: "Your API key"
```

3. 实现模型接口：

```python
from core.model_runtime.model_providers.__base.large_language_model import LargeLanguageModel
from core.model_runtime.model_providers.__base.model_provider import ModelProvider

class YourCustomProvider(ModelProvider):
    def __init__(self):
        super().__init__('your_provider_name')
    
    def get_llm_model(self) -> LargeLanguageModel:
        return YourCustomLLM(self)

class YourCustomLLM(LargeLanguageModel):
    def _chat_completion(self, messages: list, model: str, temperature: float, max_tokens: int, **kwargs):
        # 实现您的模型调用逻辑
        pass
```

```bash
# 查看日志
docker compose logs -f

# 重启服务
docker compose restart

# 停止服务
docker compose down

# 更新版本
docker compose pull
docker compose down
docker compose up -d
```

### 数据持久化

默认的数据存储位置：
- PostgreSQL: `./volumes/postgres`
- Redis: `./volumes/redis`
- 文件存储: `./volumes/storage`

### 自定义模型配置

#### 1. 配置方式概述

Dify 支持两种主要的模型配置方式：
1. 预定义模型（Predefined Models）：使用现有的模型提供商
2. 自定义模型（Customizable Models）：集成您自己部署的模型

#### 2. 添加自定义模型

1. 在 `api/core/model_runtime/providers` 目录下创建新的提供商目录
2. 创建 `provider.yaml` 配置文件：

```yaml
provider: "your_provider_name"
name: "Your Provider Display Name"
description: "Your provider description"
provider_type: "custom"
supported_model_types:
  - "llm"
  - "text-embedding"  # 如果支持的话
  - "rerank"         # 如果支持的话
  - "speech2text"    # 如果支持的话

model_deployment:
  type: "customizable-model"
  default_model: ""

provider_credential_schema:
  credential_form_schema:
    - variable: "api_base"
      label: "API Base URL"
      type: "text"
      required: true
      placeholder: "https://your-model-api-endpoint"
    - variable: "api_key"
      label: "API Key"
      type: "secret"
      required: true
      placeholder: "Your API key"
```

3. 实现模型接口：

```python
from core.model_runtime.model_providers.__base.large_language_model import LargeLanguageModel
from core.model_runtime.model_providers.__base.model_provider import ModelProvider

class YourCustomProvider(ModelProvider):
    def __init__(self):
        super().__init__('your_provider_name')
    
    def get_llm_model(self) -> LargeLanguageModel:
        return YourCustomLLM(self)

class YourCustomLLM(LargeLanguageModel):
    def _chat_completion(self, messages: list, model: str, temperature: float, max_tokens: int, **kwargs):
        # 实现您的模型调用逻辑
        pass
```

```bash
# 查看日志
docker compose logs -f

# 重启服务
docker compose restart

# 停止服务
docker compose down

# 更新版本
docker compose pull
docker compose down
docker compose up -d
```

### 数据持久化

默认的数据存储位置：
- PostgreSQL: `./volumes/postgres`
- Redis: `./volumes/redis`
- 文件存储: `./volumes/storage`

### 自定义模型配置

#### 1. 配置方式概述

Dify 支持两种主要的模型配置方式：
1. 预定义模型（Predefined Models）：使用现有的模型提供商
2. 自定义模型（Customizable Models）：集成您自己部署的模型

#### 2. 添加自定义模型

1. 在 `api/core/model_runtime/providers` 目录下创建新的提供商目录
2. 创建 `provider.yaml` 配置文件：

```yaml
provider: "your_provider_name"
name: "Your Provider Display Name"
description: "Your provider description"
provider_type: "custom"
supported_model_types:
  - "llm"
  - "text-embedding"  # 如果支持的话
  - "rerank"         # 如果支持的话
  - "speech2text"    # 如果支持的话

model_deployment:
  type: "customizable-model"
  default_model: ""

provider_credential_schema:
  credential_form_schema:
    - variable: "api_base"
      label: "API Base URL"
      type: "text"
      required: true
      placeholder: "https://your-model-api-endpoint"
    - variable: "api_key"
      label: "API Key"
      type: "secret"
      required: true
      placeholder: "Your API key"
```

3. 实现模型接口：

```python
from core.model_runtime.model_providers.__base.large_language_model import LargeLanguageModel
from core.model_runtime.model_providers.__base.model_provider import ModelProvider

class YourCustomProvider(ModelProvider):
    def __init__(self):
        super().__init__('your_provider_name')
    
    def get_llm_model(self) -> LargeLanguageModel:
        return YourCustomLLM(self)

class YourCustomLLM(LargeLanguageModel):
    def _chat_completion(self, messages: list, model: str, temperature: float, max_tokens: int, **kwargs):
        # 实现您的模型调用逻辑
        pass
```

```bash
# 查看日志
docker compose logs -f

# 重启服务
docker compose restart

# 停止服务
docker compose down

# 更新版本
docker compose pull
docker compose down
docker compose up -d
```

### 数据持久化

默认的数据存储位置：
- PostgreSQL: `./volumes/postgres`
- Redis: `./volumes/redis`
- 文件存储: `./volumes/storage`

### 自定义模型配置

#### 1. 配置方式概述

Dify 支持两种主要的模型配置方式：
1. 预定义模型（Predefined Models）：使用现有的模型提供商
2. 自定义模型（Customizable Models）：集成您自己部署的模型

#### 2. 添加自定义模型

1. 在 `api/core/model_runtime/providers` 目录下创建新的提供商目录
2. 创建 `provider.yaml` 配置文件：

```yaml
provider: "your_provider_name"
name: "Your Provider Display Name"
description: "Your provider description"
provider_type: "custom"
supported_model_types:
  - "llm"
  - "text-embedding"  # 如果支持的话
  - "rerank"         # 如果支持的话
  - "speech2text"    # 如果支持的话

model_deployment:
  type: "customizable-model"
  default_model: ""

provider_credential_schema:
  credential_form_schema:
    - variable: "api_base"
      label: "API Base URL"
      type: "text"
      required: true
      placeholder: "https://your-model-api-endpoint"
    - variable: "api_key"
      label: "API Key"
      type: "secret"
      required: true
      placeholder: "Your API key"
```

3. 实现模型接口：

```python
from core.model_runtime.model_providers.__base.large_language_model import LargeLanguageModel
from core.model_runtime.model_providers.__base.model_provider import ModelProvider

class YourCustomProvider(ModelProvider):
    def __init__(self):
        super().__init__('your_provider_name')
    
    def get_llm_model(self) -> LargeLanguageModel:
        return YourCustomLLM(self)

class YourCustomLLM(LargeLanguageModel):
    def _chat_completion(self, messages: list, model: str, temperature: float, max_tokens: int, **kwargs):
        # 实现您的模型调用逻辑
        pass
```

```bash
# 查看日志
docker compose logs -f

# 重启服务
docker compose restart

# 停止服务
docker compose down

# 更新版本
docker compose pull
docker compose down
docker compose up -d
```

### 数据持久化

默认的数据存储位置：
- PostgreSQL: `./volumes/postgres`
- Redis: `./volumes/redis`
- 文件存储: `./volumes/storage`

### 自定义模型配置

#### 1. 配置方式概述

Dify 支持两种主要的模型配置方式：
1. 预定义模型（Predefined Models）：使用现有的模型提供商
2. 自定义模型（Customizable Models）：集成您自己部署的模型

#### 2. 添加自定义模型

1. 在 `api/core/model_runtime/providers` 目录下创建新的提供商目录
2. 创建 `provider.yaml` 配置文件：

```yaml
provider: "your_provider_name"
name: "Your Provider Display Name"
description: "Your provider description"
provider_type: "custom"
supported_model_types:
  - "llm"
  - "text-embedding"  # 如果支持的话
  - "rerank"         # 如果支持的话
  - "speech2text"    # 如果支持的话

model_deployment:
  type: "customizable-model"
  default_model: ""

provider_credential_schema:
  credential_form_schema:
    - variable: "api_base"
      label: "API Base URL"
      type: "text"
      required: true
      placeholder: "https://your-model-api-endpoint"
    - variable: "api_key"
      label: "API Key"
      type: "secret"
      required: true
      placeholder: "Your API key"
```

3. 实现模型接口：

```python
from core.model_runtime.model_providers.__base.large_language_model import LargeLanguageModel
from core.model_runtime.model_providers.__base.model_provider import ModelProvider

class YourCustomProvider(ModelProvider):
    def __init__(self):
        super().__init__('your_provider_name')
    
    def get_llm_model(self) -> LargeLanguageModel:
        return YourCustomLLM(self)

class YourCustomLLM(LargeLanguageModel):
    def _chat_completion(self, messages: list, model: str, temperature: float, max_tokens: int, **kwargs):
        # 实现您的模型调用逻辑
        pass
```

```bash
# 查看日志
docker compose logs -f

# 重启服务
docker compose restart

# 停止服务
docker compose down

# 更新版本
docker compose pull
docker compose down
docker compose up -d
```

### 数据持久化

默认的数据存储位置：
- PostgreSQL: `./volumes/postgres`
- Redis: `./volumes/redis`
- 文件存储: `./volumes/storage`

### 自定义模型配置

#### 1. 配置方式概述

Dify 支持两种主要的模型配置方式：
1. 预定义模型（Predefined Models）：使用现有的模型提供商
2. 自定义模型（Customizable Models）：集成您自己部署的模型

#### 2. 添加自定义模型

1. 在 `api/core/model_runtime/providers` 目录下创建新的提供商目录
2. 创建 `provider.yaml` 配置文件：

```yaml
provider: "your_provider_name"
name: "Your Provider Display Name"
description: "Your provider description"
provider_type: "custom"
supported_model_types:
  - "llm"
  - "text-embedding"  # 如果支持的话
  - "rerank"         # 如果支持的话
  - "speech2text"    # 如果支持的话

model_deployment:
  type: "customizable-model"
  default_model: ""

provider_credential_schema:
  credential_form_schema:
    - variable: "api_base"
      label: "API Base URL"
      type: "text"
      required: true
      placeholder: "https://your-model-api-endpoint"
    - variable: "api_key"
      label: "API Key"
      type: "secret"
      required: true
      placeholder: "Your API key"
```

3. 实现模型接口：

```python
from core.model_runtime.model_providers.__base.large_language_model import LargeLanguageModel
from core.model_runtime.model_providers.__base.model_provider import ModelProvider

class YourCustomProvider(ModelProvider):
    def __init__(self):
        super().__init__('your_provider_name')
    
    def get_llm_model(self) -> LargeLanguageModel:
        return YourCustomLLM(self)

class YourCustomLLM(LargeLanguageModel):
    def _chat_completion(self, messages: list, model: str, temperature: float, max_tokens: int, **kwargs):
        # 实现您的模型调用逻辑
        pass
```

```bash
# 查看日志
docker compose logs -f

# 重启服务
docker compose restart

# 停止服务
docker compose down

# 更新版本
docker compose pull
docker compose down
docker compose up -d
```

### 数据持久化

默认的数据存储位置：
- PostgreSQL: `./volumes/postgres`
- Redis: `./volumes/redis`
- 文件存储: `./volumes/storage`

### 自定义模型配置

#### 1. 配置方式概述

Dify 支持两种主要的模型配置方式：
1. 预定义模型（Predefined Models）：使用现有的模型提供商
2. 自定义模型（Customizable Models）：集成您自己部署的模型

#### 2. 添加自定义模型

1. 在 `api/core/model_runtime/providers` 目录下创建新的提供商目录
2. 创建 `provider.yaml` 配置文件：

```yaml
provider: "your_provider_name"
name: "Your Provider Display Name"
description: "Your provider description"
provider_type: "custom"
supported_model_types:
  - "llm"
  - "text-embedding"  # 如果支持的话
  - "rerank"         # 如果支持的话
  - "speech2text"    # 如果支持的话

model_deployment:
  type: "customizable-model"
  default_model: ""

provider_credential_schema:
  credential_form_schema:
    - variable: "api_base"
      label: "API Base URL"
      type: "text"
      required: true
      placeholder: "https://your-model-api-endpoint"
    - variable: "api_key"
      label: "API Key"
      type: "secret"
      required: true
      placeholder: "Your API key"
```

3. 实现模型接口：

```python
from core.model_runtime.model_providers.__base.large_language_model import LargeLanguageModel
from core.model_runtime.model_providers.__base.model_provider import ModelProvider

class YourCustomProvider(ModelProvider):
    def __init__(self):
        super().__init__('your_provider_name')
    
    def get_llm_model(self) -> LargeLanguageModel:
        return YourCustomLLM(self)

class YourCustomLLM(LargeLanguageModel):
    def _chat_completion(self, messages: list, model: str, temperature: float, max_tokens: int, **kwargs):
        # 实现您的模型调用逻辑
        pass
```

```bash
# 查看日志
docker compose logs -f

# 重启服务
docker compose restart

# 停止服务
docker compose down

# 更新版本
docker compose pull
docker compose down
docker compose up -d
```

### 数据持久化

默认的数据存储位置：
- PostgreSQL: `./volumes/postgres`
- Redis: `./volumes/redis`
- 文件存储: `./volumes/storage`

### 自定义模型配置

#### 1. 配置方式概述

Dify 支持两种主要的模型配置方式：
1. 预定义模型（Predefined Models）：使用现有的模型提供商
2. 自定义模型（Customizable Models）：集成您自己部署的模型

#### 2. 添加自定义模型

1. 在 `api/core/model_runtime/providers` 目录下创建新的提供商目录
2. 创建 `provider.yaml` 配置文件：

```yaml
provider: "your_provider_name"
name: "Your Provider Display Name"
description: "Your provider description"
provider_type: "custom"
supported_model_types:
  - "llm"
  - "text-embedding"  # 如果支持的话
  - "rerank"         # 如果支持的话
  - "speech2text"    # 如果支持的话

model_deployment:
  type: "customizable-model"
  default_model: ""

provider_credential_schema:
  credential_form_schema:
    - variable: "api_base"
      label: "API Base URL"
      type: "text"
      required: true
      placeholder: "https://your-model-api-endpoint"
    - variable: "api_key"
      label: "API Key"
      type: "secret"
      required: true
      placeholder: "Your API key"
```

3. 实现模型接口：

```python
from core.model_runtime.model_providers.__base.large_language_model import LargeLanguageModel
from core.model_runtime.model_providers.__base.model_provider import ModelProvider

class YourCustomProvider(ModelProvider):
    def __init__(self):
        super().__init__('your_provider_name')
    
    def get_llm_model(self) -> LargeLanguageModel:
        return YourCustomLLM(self)

class YourCustomLLM(LargeLanguageModel):
    def _chat_completion(self, messages: list, model: str, temperature: float, max_tokens: int, **kwargs):
        # 实现您的模型调用逻辑
        pass
```

```bash
# 查看日志
docker compose logs -f

# 重启服务
docker compose restart

# 停止服务
docker compose down

# 更新版本
docker compose pull
docker compose down
docker compose up -d
```

### 数据持久化

默认的数据存储位置：
- PostgreSQL: `./volumes/postgres`
- Redis: `./volumes/redis`
- 文件存储: `./volumes/storage`

### 自定义模型配置

#### 1. 配置方式概述

Dify 支持两种主要的模型配置方式：
1. 预定义模型（Predefined Models）：使用现有的模型提供商
2. 自定义模型（Customizable Models）：集成您自己部署的模型

#### 2. 添加自定义模型

1. 在 `api/core/model_runtime/providers` 目录下创建新的提供商目录
2. 创建 `provider.yaml` 配置文件：

```yaml
provider: "your_provider_name"
name: "Your Provider Display Name"
description: "Your provider description"
provider_type: "custom"
supported_model_types:
  - "llm"
  - "text-embedding"  # 如果支持的话
  - "rerank"         # 如果支持的话
  - "speech2text"    # 如果支持的话

model_deployment:
  type: "customizable-model"
  default_model: ""

provider_credential_schema:
  credential_form_schema:
    - variable: "api_base"
      label: "API Base URL"
      type: "text"
      required: true
      placeholder: "https://your-model-api-endpoint"
    - variable: "api_key"
      label: "API Key"
      type: "secret"
      required: true
      placeholder: "Your API key"
```

3. 实现模型接口：

```python
from core.model_runtime.model_providers.__base.large_language_model import LargeLanguageModel
from core.model_runtime.model_providers.__base.model_provider import ModelProvider

class YourCustomProvider(ModelProvider):
    def __init__(self):
        super().__init__('your_provider_name')
    
    def get_llm_model(self) -> LargeLanguageModel:
        return YourCustomLLM(self)

class YourCustomLLM(LargeLanguageModel):
    def _chat_completion(self, messages: list, model: str, temperature: float, max_tokens: int, **kwargs):
        # 实现您的模型调用逻辑
        pass
```

```bash
# 查看日志
docker compose logs -f

# 重启服务
docker compose restart

# 停止服务
docker compose down

# 更新版本
docker compose pull
docker compose down
docker compose up -d
```

### 数据持久化

默认的数据存储位置：
- PostgreSQL: `./volumes/postgres`
- Redis: `./volumes/redis`
- 文件存储: `./volumes/storage`

### 自定义模型配置

#### 1. 配置方式概述

Dify 支持两种主要的模型配置方式：
1. 预定义模型（Predefined Models）：使用现有的模型提供商
2. 自定义模型（Customizable Models）：集成您自己部署的模型

#### 2. 添加自定义模型

1. 在 `api/core/model_runtime/providers` 目录下创建新的提供商目录
2. 创建 `provider.yaml` 配置文件：

```yaml
provider: "your_provider_name"
name: "Your Provider Display Name"
description: "Your provider description"
provider_type: "custom"
supported_model_types:
  - "llm"
  - "text-embedding"  # 如果支持的话
  - "rerank"         # 如果支持的话
  - "speech2text"    # 如果支持的话

model_deployment:
  type: "customizable-model"
  default_model: ""

provider_credential_schema:
  credential_form_schema:
    - variable: "api_base"
      label: "API Base URL"
      type: "text"
      required: true
      placeholder: "https://your-model-api-endpoint"
    - variable: "api_key"
      label: "API Key"
      type: "secret"
      required: true
      placeholder: "Your API key"
```

3. 实现模型接口：

```python
from core.model_runtime.model_providers.__base.large_language_model import LargeLanguageModel
from core.model_runtime.model_providers.__base.model_provider import ModelProvider

class YourCustomProvider(ModelProvider):
    def __init__(self):
        super().__init__('your_provider_name')
    
    def get_llm_model(self) -> LargeLanguageModel:
        return YourCustomLLM(self)

class YourCustomLLM(LargeLanguageModel):
    def _chat_completion(self, messages: list, model: str, temperature: float, max_tokens: int, **kwargs):
        # 实现您的模型调用逻辑
        pass
```

```bash
# 查看日志
docker compose logs -f

# 重启服务
docker compose restart

# 停止服务
docker compose down

# 更新版本
docker compose pull
docker compose down
docker compose up -d
```

### 数据持久化

默认的数据存储位置：
- PostgreSQL: `./volumes/postgres`
- Redis: `./volumes/redis`
- 文件存储: `./volumes/storage`

### 自定义模型配置

#### 1. 配置方式概述

Dify 支持两种主要的模型配置方式：
1. 预定义模型（Predefined Models）：使用现有的模型提供商
2. 自定义模型（Customizable Models）：集成您自己部署的模型

#### 2. 添加自定义模型

1. 在 `api/core/model_runtime/providers` 目录下创建新的提供商目录
2. 创建 `provider.yaml` 配置文件：

```yaml
provider: "your_provider_name"
name: "Your Provider Display Name"
description: "Your provider description"
provider_type: "custom"
supported_model_types:
  - "llm"
  - "text-embedding"  # 如果支持的话
  - "rerank"         # 如果支持的话
  - "speech2text"    # 如果支持的话

model_deployment:
  type: "customizable-model"
  default_model: ""

provider_credential_schema:
  credential_form_schema:
    - variable: "api_base"
      label: "API Base URL"
      type: "text"
      required: true
      placeholder: "https://your-model-api-endpoint"
    - variable: "api_key"
      label: "API Key"
      type: "secret"
      required: true
      placeholder: "Your API key"
```

3. 实现模型接口：

```python
from core.model_runtime.model_providers.__base.large_language_model import LargeLanguageModel
from core.model_runtime.model_providers.__base.model_provider import ModelProvider

class YourCustomProvider(ModelProvider):
    def __init__(self):
        super().__init__('your_provider_name')
    
    def get_llm_model(self) -> LargeLanguageModel:
        return YourCustomLLM(self)

class YourCustomLLM(LargeLanguageModel):
    def _chat_completion(self, messages: list, model: str, temperature: float, max_tokens: int, **kwargs):
        # 实现您的模型调用逻辑
        pass
```

```bash
# 查看日志
docker compose logs -f

# 重启服务
docker compose restart

# 停止服务
docker compose down

# 更新版本
docker compose pull
docker compose down
docker compose up -d
```

### 数据持久化

默认的数据存储位置：
- PostgreSQL: `./volumes/postgres`
- Redis: `./volumes/redis`
- 文件存储: `./volumes/storage`

### 自定义模型配置

#### 1. 配置方式概述

Dify 支持两种主要的模型配置方式：
1. 预定义模型（Predefined Models）：使用现有的模型提供商
2. 自定义模型（Customizable Models）：集成您自己部署的模型

#### 2. 添加自定义模型

1. 在 `api/core/model_runtime/providers` 目录下创建新的提供商目录
2. 创建 `provider.yaml` 配置文件：

```yaml
provider: "your_provider_name"
name: "Your Provider Display Name"
description: "Your provider description"
provider_type: "custom"
supported_model_types:
  - "llm"
  - "text-embedding"  # 如果支持的话
  - "rerank"         # 如果支持的话
  - "speech2text"    # 如果支持的话

model_deployment:
  type: "customizable-model"
  default_model: ""

provider_credential_schema:
  credential_form_schema:
    - variable: "api_base"
      label: "API Base URL"
      type: "text"
      required: true
      placeholder: "https://your-model-api-endpoint"
    - variable: "api_key"
      label: "API Key"
      type: "secret"
      required: true
      placeholder: "Your API key"
```

3. 实现模型接口：

```python
from core.model_runtime.model_providers.__base.large_language_model import LargeLanguageModel
from core.model_runtime.model_providers.__base.model_provider import ModelProvider

class YourCustomProvider(ModelProvider):
    def __init__(self):
        super().__init__('your_provider_name')
    
    def get_llm_model(self) -> LargeLanguageModel:
        return YourCustomLLM(self)

class YourCustomLLM(LargeLanguageModel):
    def _chat_completion(self, messages: list, model: str, temperature: float, max_tokens: int, **kwargs):
        # 实现您的模型调用逻辑
        pass
```

```bash
# 查看日志
docker compose logs -f

# 重启服务
docker compose restart

# 停止服务
docker compose down

# 更新版本
docker compose pull
docker compose down
docker compose up -d
```

### 数据持久化

默认的数据存储位置：
- PostgreSQL: `./volumes/postgres`
- Redis: `./volumes/redis`
- 文件存储: `./volumes/storage`

### 自定义模型配置

#### 1. 配置方式概述

Dify 支持两种主要的模型配置方式：
1. 预定义模型（Predefined Models）：使用现有的模型提供商
2. 自定义模型（Customizable Models）：集成您自己部署的模型

#### 2. 添加自定义模型

1. 在 `api/core/model_runtime/providers` 目录下创建新的提供商目录
2. 创建 `provider.yaml` 配置文件：

```yaml
provider: "your_provider_name"
name: "Your Provider Display Name"
description: "Your provider description"
provider_type: "custom"
supported_model_types:
  - "llm"
  - "text-embedding"  # 如果支持