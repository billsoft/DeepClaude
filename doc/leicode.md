# é¡¹ç›®ç›®å½•ç»“æ„
```
.
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ main.py
â”œâ”€â”€ .cursor/
â”‚   â”œâ”€â”€ rules/
â”œâ”€â”€ .github/
â”‚   â”œâ”€â”€ workflows/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ clients/
â”‚   â”‚   â”œâ”€â”€ base_client.py
â”‚   â”‚   â”œâ”€â”€ claude_client.py
â”‚   â”‚   â”œâ”€â”€ deepseek_client.py
â”‚   â”‚   â”œâ”€â”€ ollama_r1.py
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ deepclaude/
â”‚   â”‚   â”œâ”€â”€ deepclaude.py
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ auth.py
â”‚   â”‚   â”œâ”€â”€ logger.py
â”‚   â”‚   â”œâ”€â”€ message_processor.py
â”œâ”€â”€ doc/
â”œâ”€â”€ test/
â”‚   â”œâ”€â”€ test_claude_client.py
â”‚   â”œâ”€â”€ test_deepclaude.py
â”‚   â”œâ”€â”€ test_deepseek_client.py
â”‚   â”œâ”€â”€ test_nvidia_deepseek.py
â”‚   â”œâ”€â”€ test_ollama_r1.py
```

# WebæœåŠ¡å™¨å±‚

## main.py
```python
import os
import uvicorn
from dotenv import load_dotenv
from app.utils.logger import logger
load_dotenv()
def main():
 host = os.getenv('HOST', '::')
 port = int(os.getenv('PORT', 2411))
 reload = os.getenv('RELOAD', 'false').lower() == 'true'
 uvicorn.run(
 'app.main:app',
 host=host,
 port=port,
 reload=reload,
 loop='uvloop' if os.name != 'nt' else 'asyncio'
 )
if __name__ == '__main__':
 main()```
______________________________

## .\main.py
```python
import os
import uvicorn
from dotenv import load_dotenv
from app.utils.logger import logger
load_dotenv()
def main():
 host = os.getenv('HOST', '::')
 port = int(os.getenv('PORT', 2411))
 reload = os.getenv('RELOAD', 'false').lower() == 'true'
 uvicorn.run(
 'app.main:app',
 host=host,
 port=port,
 reload=reload,
 loop='uvloop' if os.name != 'nt' else 'asyncio'
 )
if __name__ == '__main__':
 main()```
______________________________

## ...\app\main.py
```python
import os
import sys
from dotenv import load_dotenv
import uuid
import time
load_dotenv()
def setup_proxy():
 enable_proxy = os.getenv('ENABLE_PROXY', 'false').lower() == 'true'
 if enable_proxy:
 os.environ['HTTP_PROXY'] = os.getenv('HTTP_PROXY', '')
 os.environ['HTTPS_PROXY'] = os.getenv('HTTPS_PROXY', '')
 else:
 os.environ.pop('HTTP_PROXY', None)
 os.environ.pop('HTTPS_PROXY', None)
setup_proxy()
from fastapi import FastAPI, Depends, Request
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from app.utils.logger import logger
from app.utils.auth import verify_api_key
from app.deepclaude.deepclaude import DeepClaude
from fastapi.responses import JSONResponse
app = FastAPI(title="DeepClaude API")
ALLOW_ORIGINS = os.getenv("ALLOW_ORIGINS", "*")
CLAUDE_API_KEY = os.getenv("CLAUDE_API_KEY")
CLAUDE_MODEL = os.getenv("CLAUDE_MODEL")
CLAUDE_PROVIDER = os.getenv("CLAUDE_PROVIDER", "anthropic")
CLAUDE_API_URL = os.getenv("CLAUDE_API_URL", "https://api.anthropic.com/v1/messages")
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")
DEEPSEEK_API_URL = os.getenv("DEEPSEEK_API_URL")
DEEPSEEK_MODEL = os.getenv("DEEPSEEK_MODEL", "deepseek-ai/DeepSeek-R1")
DEEPSEEK_PROVIDER = os.getenv("DEEPSEEK_PROVIDER", "deepseek")
IS_ORIGIN_REASONING = os.getenv("IS_ORIGIN_REASONING", "false").lower() == "true"
REASONING_PROVIDER = os.getenv("REASONING_PROVIDER", "deepseek").lower()
allow_origins_list = ALLOW_ORIGINS.split(",") if ALLOW_ORIGINS else []
app.add_middleware(
 CORSMiddleware,
 allow_origins=allow_origins_list,
 allow_credentials=True,
 allow_methods=["*"],
 allow_headers=["*"],
)
OLLAMA_API_URL = os.getenv("OLLAMA_API_URL")
if REASONING_PROVIDER == 'ollama' and not OLLAMA_API_URL:
 logger.critical("ä½¿ç”¨ Ollama æ¨ç†æ—¶å¿…é¡»è®¾ç½® OLLAMA_API_URL")
 sys.exit(1)
if REASONING_PROVIDER == 'deepseek' and not DEEPSEEK_API_KEY:
 logger.critical("ä½¿ç”¨ DeepSeek æ¨ç†æ—¶å¿…é¡»è®¾ç½® DEEPSEEK_API_KEY")
 sys.exit(1)
if not CLAUDE_API_KEY:
 logger.critical("å¿…é¡»è®¾ç½® CLAUDE_API_KEY")
 sys.exit(1)
deep_claude = DeepClaude(
 claude_api_key=CLAUDE_API_KEY,
 claude_api_url=CLAUDE_API_URL,
 claude_provider=CLAUDE_PROVIDER,
 deepseek_api_key=DEEPSEEK_API_KEY,
 deepseek_api_url=DEEPSEEK_API_URL,
 deepseek_provider=DEEPSEEK_PROVIDER,
 ollama_api_url=OLLAMA_API_URL,
 is_origin_reasoning=IS_ORIGIN_REASONING
)
logger.debug("å½“å‰æ—¥å¿—çº§åˆ«ä¸º DEBUG")
logger.info("å¼€å§‹è¯·æ±‚")
@app.get("/", dependencies=[Depends(verify_api_key)])
async def root():
 logger.info("è®¿é—®äº†æ ¹è·¯å¾„")
 return {"message": "Welcome to DeepClaude API"}
@app.get("/v1/models")
async def list_models():
 models = [{
 "id": "deepclaude",
 "object": "model",
 "created": 1677610602,
 "owned_by": "deepclaude",
 "permission": [{
 "id": "modelperm-deepclaude",
 "object": "model_permission",
 "created": 1677610602,
 "allow_create_engine": False,
 "allow_sampling": True,
 "allow_logprobs": True,
 "allow_search_indices": False,
 "allow_view": True,
 "allow_fine_tuning": False,
 "organization": "*",
 "group": None,
 "is_blocking": False
 }],
 "root": "deepclaude",
 "parent": None
 }]
 return {"object": "list", "data": models}
@app.post("/v1/chat/completions", dependencies=[Depends(verify_api_key)])
async def chat_completions(request: Request):
 try:
 data = await request.json()
 if "messages" not in data:
 raise ValueError("Missing messages parameter")
 if data.get("stream", False):
 return StreamingResponse(
 deep_claude.chat_completions_with_stream(
 messages=data["messages"],
 chat_id=f"chatcmpl-{uuid.uuid4()}",
 created_time=int(time.time()),
 model=data.get("model", "deepclaude")
 ),
 media_type="text/event-stream",
 headers={
 "Cache-Control": "no-cache, no-transform",
 "Connection": "keep-alive",
 "Content-Type": "text/event-stream;charset=utf-8",
 "X-Accel-Buffering": "no",
 "Transfer-Encoding": "chunked",
 "Keep-Alive": "timeout=600"
 }
 )
 else:
 response = await deep_claude.chat_completions_without_stream(
 messages=data["messages"],
 model_arg=get_and_validate_params(data)
 )
 return JSONResponse(content=response)
 except ValueError as e:
 logger.warning(f"å‚æ•°éªŒè¯é”™è¯¯: {e}")
 return JSONResponse(
 status_code=400,
 content={"error": str(e)}
 )
 except Exception as e:
 logger.error(f"å¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
 return JSONResponse(
 status_code=500,
 content={"error": "Internal server error"}
 )
def get_and_validate_params(body: dict) -> tuple:
 temperature: float = body.get("temperature", 0.5)
 top_p: float = body.get("top_p", 0.9)
 presence_penalty: float = body.get("presence_penalty", 0.0)
 frequency_penalty: float = body.get("frequency_penalty", 0.0)
 stream: bool = body.get("stream", True)
 if "sonnet" in body.get("model", ""):
 if not isinstance(temperature, (float)) or temperature < 0.0 or temperature > 1.0:
 raise ValueError("Sonnet è®¾å®š temperature å¿…é¡»åœ¨ 0 åˆ° 1 ä¹‹é—´")
 return (temperature, top_p, presence_penalty, frequency_penalty, stream)```
______________________________

## ...\clients\base_client.py
```python
from typing import AsyncGenerator, Any, Tuple
import aiohttp
from app.utils.logger import logger
from abc import ABC, abstractmethod
import os
from dotenv import load_dotenv
import asyncio
load_dotenv()
class BaseClient(ABC):
 def __init__(self, api_key: str, api_url: str):
 self.api_key = api_key
 self.api_url = api_url
 def _get_proxy(self) -> str | None:
 use_proxy, proxy = self._get_proxy_config()
 return proxy if use_proxy else None
 @abstractmethod
 def _get_proxy_config(self) -> tuple[bool, str | None]:
 pass
 @abstractmethod
 async def stream_chat(self, messages: list, model: str, **kwargs) -> AsyncGenerator[tuple[str, str], None]:
 pass
 @abstractmethod
 async def get_reasoning(self, messages: list, model: str, **kwargs) -> AsyncGenerator[tuple[str, str], None]:
 pass
 @abstractmethod
 def _extract_reasoning(self, content: str) -> tuple[bool, str]:
 pass
 async def _make_request(self, headers: dict, data: dict) -> AsyncGenerator[bytes, None]:
 max_retries = 3
 retry_count = 0
 retry_codes = {429, 500, 502, 503, 504}
 while retry_count < max_retries:
 try:
 async with aiohttp.ClientSession() as session:
 async with session.post(
 self.api_url,
 headers=headers,
 json=data,
 proxy=self._get_proxy(),
 timeout=aiohttp.ClientTimeout(
 total=60,
 connect=10,
 sock_read=30
 )
 ) as response:
 if response.status != 200:
 error_msg = await response.text()
 logger.error(f"APIè¯·æ±‚å¤±è´¥: HTTP {response.status}\n{error_msg}")
 if response.status in retry_codes:
 retry_count += 1
 wait_time = min(2 ** retry_count, 32)
 logger.warning(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
 await asyncio.sleep(wait_time)
 continue
 raise Exception(f"HTTP {response.status}: {error_msg}")
 buffer = bytearray()
 async for chunk in response.content.iter_chunks():
 chunk_data = chunk[0]
 if not chunk_data:
 continue
 buffer.extend(chunk_data)
 while b"\n" in buffer:
 line, remainder = buffer.split(b"\n", 1)
 if line:
 yield line
 buffer = remainder
 if buffer:
 yield bytes(buffer)
 break
 except (aiohttp.ClientError, asyncio.TimeoutError) as e:
 retry_count += 1
 if retry_count >= max_retries:
 logger.error(f"è¯·æ±‚é‡è¯•æ¬¡æ•°è¶…è¿‡ä¸Šé™: {e}")
 raise
 wait_time = min(2 ** retry_count, 32)
 logger.warning(f"ç½‘ç»œé”™è¯¯ï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯•: {e}")
 await asyncio.sleep(wait_time)```
______________________________

## ...\clients\claude_client.py
```python
import json
from typing import AsyncGenerator
from app.utils.logger import logger
from .base_client import BaseClient
import os
class ClaudeClient(BaseClient):
 def __init__(self, api_key: str, api_url: str = "https://api.anthropic.com/v1/messages", provider: str = "anthropic"):
 super().__init__(api_key, api_url)
 self.provider = provider.lower()
 def _extract_reasoning(self, content: str) -> tuple[bool, str]:
 return False, ""
 def _get_proxy_config(self) -> tuple[bool, str | None]:
 enable_proxy = os.getenv('CLAUDE_ENABLE_PROXY', 'false').lower() == 'true'
 if enable_proxy:
 http_proxy = os.getenv('HTTP_PROXY')
 https_proxy = os.getenv('HTTPS_PROXY')
 if https_proxy or http_proxy:
 logger.info(f"Claude å®¢æˆ·ç«¯ä½¿ç”¨ä»£ç†: {https_proxy or http_proxy}")
 else:
 logger.warning("å·²å¯ç”¨ Claude ä»£ç†ä½†æœªè®¾ç½®ä»£ç†åœ°å€")
 return True, https_proxy or http_proxy
 logger.debug("Claude å®¢æˆ·ç«¯æœªå¯ç”¨ä»£ç†")
 return False, None
 def _prepare_headers(self) -> dict:
 headers = {
 "Content-Type": "application/json",
 "Accept": "text/event-stream",
 }
 if self.provider == "anthropic":
 headers.update({
 "x-api-key": self.api_key,
 "anthropic-version": "2023-06-01",
 "anthropic-beta": "messages-2023-12-15"
 })
 elif self.provider == "openrouter":
 headers["Authorization"] = f"Bearer {self.api_key}"
 elif self.provider == "oneapi":
 headers["Authorization"] = f"Bearer {self.api_key}"
 return headers
 def _prepare_request_data(self, messages: list, **kwargs) -> dict:
 data = {
 "model": kwargs.get("model", "claude-3-5-sonnet-20241022"),
 "messages": messages,
 "max_tokens": kwargs.get("max_tokens", 8192),
 "temperature": kwargs.get("temperature", 0.7),
 "top_p": kwargs.get("top_p", 0.9),
 "stream": True
 }
 logger.debug(f"Claudeè¯·æ±‚æ•°æ®: {messages}")
 return data
 async def stream_chat(self, messages: list, **kwargs) -> AsyncGenerator[dict, None]:
 try:
 headers = self._prepare_headers()
 data = self._prepare_request_data(messages, **kwargs)
 logger.debug(f"Claudeè¯·æ±‚æ•°æ®: {data}")
 async for chunk in self._make_request(headers, data):
 try:
 if chunk:
 text = chunk.decode('utf-8')
 if text.startswith('data: '):
 data = text[6:].strip()
 if data == '[DONE]':
 break
 response = json.loads(data)
 logger.debug(f"Claudeå“åº”æ•°æ®: {response}")
 if 'type' in response:
 if response['type'] == 'content_block_delta':
 content = response['delta'].get('text', '')
 if content:
 yield "content", content
 elif 'choices' in response:
 if response['choices'][0].get('delta', {}).get('content'):
 yield "content", response['choices'][0].get('delta', {}).get('content')
 except json.JSONDecodeError as e:
 logger.error(f"è§£æClaudeå“åº”å¤±è´¥: {e}")
 continue
 except Exception as e:
 logger.error(f"Claudeæµå¼è¯·æ±‚å¤±è´¥: {e}", exc_info=True)
 raise
 async def get_reasoning(self, messages: list, model: str, **kwargs) -> AsyncGenerator[tuple[str, str], None]:
 if kwargs.get('stream', True) == False:
 async for content_type, content in self.stream_chat(
 messages=messages,
 model=kwargs.get('model', 'claude-3-5-sonnet-20241022'),
 **kwargs
 ):
 all_content = content
 yield "answer", all_content
 return
 return```
______________________________

## ...\clients\deepseek_client.py
```python
import json
from typing import AsyncGenerator
from app.utils.logger import logger
from .base_client import BaseClient
import os
import logging
import re
class DeepSeekClient(BaseClient):
 def __init__(self, api_key: str, api_url: str = None, provider: str = None):
 self.provider = provider or os.getenv('DEEPSEEK_PROVIDER', 'deepseek')
 self.provider_configs = {
 'deepseek': {
 'url': 'https://api.deepseek.com/v1/chat/completions',
 'model': 'deepseek-reasoner'
 },
 'siliconflow': {
 'url': 'https://api.siliconflow.cn/v1/chat/completions',
 'model': 'deepseek-ai/DeepSeek-R1'
 },
 'nvidia': {
 'url': 'https://integrate.api.nvidia.com/v1/chat/completions',
 'model': 'deepseek-ai/deepseek-r1'
 }
 }
 if self.provider not in self.provider_configs:
 raise ValueError(f"ä¸æ”¯æŒçš„ provider: {self.provider}")
 config = self.provider_configs[self.provider]
 api_url = api_url or os.getenv('DEEPSEEK_API_URL') or config['url']
 super().__init__(api_key, api_url)
 self.default_model = config['model']
 self.reasoning_mode = os.getenv('DEEPSEEK_REASONING_MODE', 'auto').lower()
 self.is_origin_reasoning = os.getenv('IS_ORIGIN_REASONING', 'false').lower() == 'true'
 if self.is_origin_reasoning and self.reasoning_mode == 'auto':
 self.reasoning_mode = 'reasoning_field'
 self.early_content_threshold = int(os.getenv('DEEPSEEK_EARLY_THRESHOLD', '20'))
 self._content_buffer = ""
 self._reasoning_buffer = ""
 self._has_found_reasoning = False
 self._content_token_count = 0
 logger.debug(f"DeepSeekå®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ - æä¾›å•†: {self.provider}, æ¨¡å‹: {self.default_model}, æ¨ç†æ¨¡å¼: {self.reasoning_mode}")
 def _get_proxy_config(self) -> tuple[bool, str | None]:
 enable_proxy = os.getenv('DEEPSEEK_ENABLE_PROXY', 'false').lower() == 'true'
 if enable_proxy:
 http_proxy = os.getenv('HTTP_PROXY')
 https_proxy = os.getenv('HTTPS_PROXY')
 logger.info(f"DeepSeek å®¢æˆ·ç«¯ä½¿ç”¨ä»£ç†: {https_proxy or http_proxy}")
 return True, https_proxy or http_proxy
 logger.debug("DeepSeek å®¢æˆ·ç«¯æœªå¯ç”¨ä»£ç†")
 return False, None
 def _process_think_tag_content(self, content: str) -> tuple[bool, str]:
 has_start = "<think>" in content
 has_end = "</think>" in content
 if has_start and has_end:
 return True, content
 elif has_start:
 return False, content
 elif not has_start and not has_end:
 return False, content
 else:
 return True, content
 def _extract_reasoning(self, content: str | dict) -> tuple[bool, str]:
 logger.debug(f"æå–æ¨ç†å†…å®¹ï¼Œcontentç±»å‹: {type(content)}, æ¨ç†æ¨¡å¼: {self.reasoning_mode}")
 if isinstance(content, dict):
 logger.debug(f"å¤„ç†å­—å…¸ç±»å‹çš„æ¨ç†å†…å®¹: {str(content)[:100]}...")
 if "reasoning_content" in content:
 extracted = content["reasoning_content"]
 logger.debug(f"ä»reasoning_contentå­—æ®µæå–åˆ°æ¨ç†å†…å®¹: {str(extracted)[:50]}...")
 return True, extracted
 if "role" in content and content["role"] in ["reasoning", "thinking", "thought"]:
 if "content" in content:
 logger.debug(f"ä»æ€è€ƒè§’è‰²æå–åˆ°æ¨ç†å†…å®¹")
 return True, content["content"]
 if "content" in content:
 text_content = content["content"]
 if self.reasoning_mode in ['auto', 'think_tags'] and "<think>" in text_content:
 return self._extract_from_think_tags(text_content)
 if self.reasoning_mode in ['auto', 'any_content']:
 logger.debug(f"ä»»ä½•å†…å®¹æ¨¡å¼ï¼Œå°†æ™®é€šå†…å®¹è§†ä¸ºæ¨ç†: {text_content[:50]}...")
 return True, text_content
 if self.reasoning_mode == 'early_content' and self._content_token_count < self.early_content_threshold:
 self._content_token_count += 1
 logger.debug(f"æ—©æœŸå†…å®¹æ¨¡å¼ï¼Œå°†å†…å®¹è§†ä¸ºæ¨ç† (token {self._content_token_count}/{self.early_content_threshold})")
 return True, text_content
 if self.provider == 'nvidia' and self.reasoning_mode == 'auto':
 for field in ["thinking", "thought", "reasoning"]:
 if field in content:
 logger.debug(f"ä»NVIDIAç‰¹æ®Šå­—æ®µ{field}æå–åˆ°æ¨ç†å†…å®¹")
 return True, content[field]
 return False, ""
 elif isinstance(content, str):
 logger.debug(f"å¤„ç†å­—ç¬¦ä¸²ç±»å‹çš„æ¨ç†å†…å®¹: {content[:50]}...")
 if self.reasoning_mode in ['auto', 'think_tags']:
 self._content_buffer += content
 has_think, extracted = self._extract_from_buffered_think_tags()
 if has_think:
 return True, extracted
 if self.reasoning_mode == 'early_content' and self._content_token_count < self.early_content_threshold:
 self._content_token_count += 1
 logger.debug(f"æ—©æœŸå†…å®¹æ¨¡å¼ï¼Œå°†å†…å®¹è§†ä¸ºæ¨ç† (token {self._content_token_count}/{self.early_content_threshold})")
 return True, content
 if self.reasoning_mode in ['auto', 'any_content']:
 logger.debug(f"ä»»ä½•å†…å®¹æ¨¡å¼ï¼Œå°†å­—ç¬¦ä¸²å†…å®¹è§†ä¸ºæ¨ç†: {content[:50]}...")
 return True, content
 if self.reasoning_mode == 'auto' and self._is_potential_reasoning(content):
 logger.debug(f"æ ¹æ®å¯å‘å¼åˆ¤æ–­ï¼Œå°†å†…å®¹è§†ä¸ºæ¨ç†: {content[:50]}...")
 return True, content
 return False, ""
 logger.warning(f"æ— æ³•å¤„ç†çš„å†…å®¹ç±»å‹: {type(content)}")
 return False, ""
 def _is_potential_reasoning(self, text: str) -> bool:
 if self._has_found_reasoning:
 return True
 reasoning_patterns = [
 r'æˆ‘éœ€è¦æ€è€ƒ', r'è®©æˆ‘åˆ†æ', r'åˆ†æè¿™ä¸ªé—®é¢˜', r'æ€è·¯ï¼š', r'æ€è€ƒè¿‡ç¨‹',
 r'é¦–å…ˆ[ï¼Œ,]', r'ç¬¬ä¸€æ­¥', r'ç¬¬äºŒæ­¥', r'ç¬¬ä¸‰æ­¥', r'æ¥ä¸‹æ¥',
 r'ç®—æ³•æ€è·¯', r'è§£é¢˜æ€è·¯', r'è€ƒè™‘é—®é¢˜'
 ]
 for pattern in reasoning_patterns:
 if re.search(pattern, text):
 self._has_found_reasoning = True
 return True
 return False
 def _extract_from_buffered_think_tags(self) -> tuple[bool, str]:
 buffer = self._content_buffer
 if "<think>" not in buffer:
 return False, ""
 if "</think>" in buffer:
 start = buffer.find("<think>") + len("<think>")
 end = buffer.find("</think>")
 if start < end:
 extracted = buffer[start:end].strip()
 self._content_buffer = buffer[end + len("</think>"):]
 logger.debug(f"ä»ç¼“å†²åŒºä¸­çš„å®Œæ•´thinkæ ‡ç­¾æå–åˆ°æ¨ç†å†…å®¹: {extracted[:50]}...")
 return True, extracted
 elif len(buffer) > 1000 or buffer.count("\n") > 3:
 start = buffer.find("<think>") + len("<think>")
 extracted = buffer[start:].strip()
 self._content_buffer = buffer[-100:] if len(buffer) > 100 else buffer
 logger.debug(f"ä»ç¼“å†²åŒºä¸­çš„ä¸å®Œæ•´thinkæ ‡ç­¾æå–åˆ°æ¨ç†å†…å®¹: {extracted[:50]}...")
 return True, extracted
 return False, ""
 def _extract_from_think_tags(self, text: str) -> tuple[bool, str]:
 if not text or "<think>" not in text:
 return False, ""
 if "</think>" in text:
 start = text.find("<think>") + len("<think>")
 end = text.find("</think>")
 if start < end:
 extracted = text[start:end].strip()
 logger.debug(f"ä»å®Œæ•´thinkæ ‡ç­¾ä¸­æå–åˆ°æ¨ç†å†…å®¹: {extracted[:50]}...")
 return True, extracted
 else:
 start = text.find("<think>") + len("<think>")
 if start < len(text):
 extracted = text[start:].strip()
 logger.debug(f"ä»ä¸å®Œæ•´thinkæ ‡ç­¾ä¸­æå–åˆ°æ¨ç†å†…å®¹: {extracted[:50]}...")
 return True, extracted
 return False, ""
 def _extract_reasoning_from_text(self, text: str) -> tuple[bool, str]:
 return self._extract_from_think_tags(text)
 async def stream_chat(self, messages: list, model: str = None, model_arg: tuple = None) -> AsyncGenerator[tuple[str, str], None]:
 if not model:
 model = self.default_model
 if not model:
 raise ValueError("æœªæŒ‡å®šæ¨¡å‹ä¸”æ— é»˜è®¤æ¨¡å‹")
 headers = {
 "Authorization": f"Bearer {self.api_key}",
 "Content-Type": "application/json",
 "Accept": "text/event-stream",
 }
 data = {
 "model": model,
 "messages": messages,
 "stream": True,
 }
 if self.provider == 'nvidia':
 temperature = model_arg[0] if model_arg else 0.6
 top_p = model_arg[1] if model_arg else 0.7
 data.update({
 "temperature": temperature,
 "top_p": top_p,
 "max_tokens": 4096
 })
 logger.debug(f"å¼€å§‹æµå¼å¯¹è¯ï¼š{data}")
 self._content_buffer = ""
 self._reasoning_buffer = ""
 self._has_found_reasoning = False
 self._content_token_count = 0
 try:
 async for chunk in self._make_request(headers, data):
 chunk_str = chunk.decode('utf-8')
 if not chunk_str.strip():
 continue
 try:
 if chunk_str.startswith('data:'):
 chunk_str = chunk_str[5:].strip()
 if chunk_str == "[DONE]":
 continue
 data = json.loads(chunk_str)
 if not data or not data.get("choices") or not data["choices"][0].get("delta"):
 continue
 delta = data["choices"][0]["delta"]
 has_reasoning, reasoning = self._extract_reasoning(delta)
 if has_reasoning and reasoning:
 logger.debug(f"æ”¶åˆ°æ¨ç†å†…å®¹: {reasoning[:min(30, len(reasoning))]}...")
 self._reasoning_buffer += reasoning
 yield "reasoning", reasoning
 elif "content" in delta and delta["content"]:
 content = delta["content"]
 logger.debug(f"æ”¶åˆ°å›ç­”å†…å®¹: {content[:min(30, len(content))]}...")
 yield "content", content
 except json.JSONDecodeError:
 logger.warning(f"JSONè§£æé”™è¯¯: {chunk_str[:50]}...")
 continue
 except Exception as e:
 logger.error(f"æµå¼å¯¹è¯å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
 raise
 async def get_reasoning(self, messages: list, model: str, **kwargs) -> AsyncGenerator[tuple[str, str], None]:
 model_arg = kwargs.get('model_arg')
 headers = {
 "Authorization": f"Bearer {self.api_key}",
 "Content-Type": "application/json",
 "Accept": "text/event-stream",
 }
 supported_models = {
 'deepseek': ['deepseek-reasoner'],
 'siliconflow': ['deepseek-ai/DeepSeek-R1'],
 'nvidia': ['deepseek-ai/deepseek-r1']
 }
 if self.provider in supported_models and model not in supported_models[self.provider]:
 logger.warning(f"è¯·æ±‚çš„æ¨¡å‹ '{model}' å¯èƒ½ä¸è¢« {self.provider} æä¾›å•†æ”¯æŒï¼Œå°†ä½¿ç”¨é»˜è®¤æ¨¡å‹")
 model = supported_models[self.provider][0]
 data = {
 "model": model,
 "messages": messages,
 "stream": True,
 }
 if self.provider == 'nvidia':
 temperature = model_arg[0] if model_arg else 0.6
 top_p = model_arg[1] if model_arg else 0.7
 data.update({
 "temperature": temperature,
 "top_p": top_p,
 "max_tokens": 4096
 })
 logger.info(f"å¼€å§‹è·å–æ¨ç†å†…å®¹ï¼Œæ¨¡å‹: {model}ï¼Œæä¾›å•†: {self.provider}ï¼Œæ¨ç†æ¨¡å¼: {self.reasoning_mode}")
 logger.debug(f"æ¨ç†è¯·æ±‚æ•°æ®: {data}")
 self._content_buffer = ""
 self._reasoning_buffer = ""
 self._has_found_reasoning = False
 self._content_token_count = 0
 buffer = ""
 has_yielded_content = False
 is_first_chunk = True
 try:
 async for chunk in self._make_request(headers, data):
 try:
 chunk_str = chunk.decode('utf-8')
 if not chunk_str.strip():
 continue
 if is_first_chunk:
 logger.debug(f"é¦–ä¸ªå“åº”å—: {chunk_str}")
 is_first_chunk = False
 for line in chunk_str.splitlines():
 if not line.strip():
 continue
 if line.startswith("data: "):
 json_str = line[len("data: "):].strip()
 if json_str == "[DONE]":
 logger.debug("æ”¶åˆ°[DONE]æ ‡è®°")
 continue
 try:
 data = json.loads(json_str)
 if logger.isEnabledFor(logging.DEBUG):
 small_data = {k: v for k, v in data.items() if k != 'choices'}
 if 'choices' in data and data['choices']:
 small_data['choices_count'] = len(data['choices'])
 small_data['sample_delta'] = data['choices'][0].get('delta', {})
 logger.debug(f"è§£æJSONå“åº”: {small_data}")
 if not data or not data.get("choices") or not data["choices"][0].get("delta"):
 logger.debug(f"è·³è¿‡æ— æ•ˆæ•°æ®å—: {json_str[:50]}")
 continue
 delta = data["choices"][0]["delta"]
 has_reasoning, reasoning = self._extract_reasoning(delta)
 if has_reasoning and reasoning:
 logger.debug(f"è·å–åˆ°æ¨ç†å†…å®¹: {reasoning[:min(30, len(reasoning))]}...")
 self._reasoning_buffer += reasoning
 yield "reasoning", reasoning
 has_yielded_content = True
 elif "content" in delta and delta["content"]:
 content = delta["content"]
 logger.debug(f"è·å–åˆ°æ™®é€šå†…å®¹: {content[:min(30, len(content))]}...")
 yield "content", content
 has_yielded_content = True
 else:
 logger.debug(f"æ— æ³•æå–å†…å®¹ï¼Œdelta: {delta}")
 except json.JSONDecodeError as e:
 logger.warning(f"JSONè§£æé”™è¯¯: {e}, å†…å®¹: {json_str[:50]}...")
 buffer += json_str
 try:
 data = json.loads(buffer)
 logger.debug(f"ä»ç¼“å†²åŒºè§£æJSONæˆåŠŸ")
 buffer = ""
 if data and data.get("choices") and data["choices"][0].get("delta"):
 delta = data["choices"][0]["delta"]
 has_reasoning, reasoning = self._extract_reasoning(delta)
 if has_reasoning and reasoning:
 logger.debug(f"ä»ç¼“å†²åŒºè·å–åˆ°æ¨ç†å†…å®¹: {reasoning[:min(30, len(reasoning))]}...")
 self._reasoning_buffer += reasoning
 yield "reasoning", reasoning
 has_yielded_content = True
 elif "content" in delta and delta["content"]:
 content = delta["content"]
 logger.debug(f"ä»ç¼“å†²åŒºè·å–åˆ°æ™®é€šå†…å®¹: {content[:min(30, len(content))]}...")
 yield "content", content
 has_yielded_content = True
 except Exception as e:
 logger.debug(f"ç¼“å†²åŒºJSONè§£æå¤±è´¥: {e}")
 except Exception as e:
 logger.warning(f"å¤„ç†æ¨ç†å†…å®¹å—æ—¶å‘ç”Ÿé”™è¯¯: {e}")
 continue
 if not has_yielded_content and self._content_buffer:
 logger.info(f"å°è¯•ä»å†…å®¹ç¼“å†²åŒºä¸­æå–æ¨ç†å†…å®¹ï¼Œç¼“å†²åŒºå¤§å°: {len(self._content_buffer)}")
 has_reasoning, reasoning = self._extract_from_buffered_think_tags()
 if has_reasoning and reasoning:
 logger.debug(f"ä»æœ€ç»ˆç¼“å†²åŒºè·å–åˆ°æ¨ç†å†…å®¹: {reasoning[:min(30, len(reasoning))]}...")
 yield "reasoning", reasoning
 has_yielded_content = True
 elif self.reasoning_mode in ['auto', 'any_content', 'early_content']:
 logger.debug(f"å°†å‰©ä½™ç¼“å†²åŒºå†…å®¹ä½œä¸ºæ¨ç†è¾“å‡º")
 yield "reasoning", self._content_buffer
 has_yielded_content = True
 if not has_yielded_content:
 logger.warning("æœªèƒ½è·å–åˆ°ä»»ä½•æ¨ç†å†…å®¹æˆ–æ™®é€šå†…å®¹ï¼Œè¯·æ£€æŸ¥APIå“åº”æ ¼å¼")
 logger.warning(f"å·²å°è¯•çš„æ¨ç†æ¨¡å¼: {self.reasoning_mode}")
 logger.warning(f"ç¼“å†²åŒºçŠ¶æ€: å†…å®¹ç¼“å†²åŒºé•¿åº¦={len(self._content_buffer)}, æ¨ç†ç¼“å†²åŒºé•¿åº¦={len(self._reasoning_buffer)}")
 except Exception as e:
 logger.error(f"è·å–æ¨ç†å†…å®¹è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
 raise```
______________________________

## ...\clients\ollama_r1.py
```python
import os
import json
from typing import AsyncGenerator
from app.utils.logger import logger
from .base_client import BaseClient
import asyncio
class OllamaR1Client(BaseClient):
 def __init__(self, api_url: str = "http://localhost:11434"):
 if not api_url:
 raise ValueError("å¿…é¡»æä¾› Ollama API URL")
 if not api_url.endswith("/api/chat"):
 api_url = f"{api_url.rstrip('/')}/api/chat"
 super().__init__(api_key="", api_url=api_url)
 self.default_model = "deepseek-r1:32b"
 def _process_think_tag_content(self, content: str) -> tuple[bool, str]:
 has_start = "<think>" in content
 has_end = "</think>" in content
 if has_start and has_end:
 return True, content
 elif has_start:
 return False, content
 elif not has_start and not has_end:
 return False, content
 else:
 return True, content
 def _extract_reasoning(self, content: str) -> tuple[bool, str]:
 if "<think>" in content and "</think>" in content:
 start = content.find("<think>") + 7
 end = content.find("</think>")
 if start < end:
 return True, content[start:end].strip()
 return False, ""
 async def stream_chat(self, messages: list, model: str = "deepseek-r1:32b") -> AsyncGenerator[tuple[str, str], None]:
 if not messages:
 raise ValueError("æ¶ˆæ¯åˆ—è¡¨ä¸èƒ½ä¸ºç©º")
 headers = {
 "Content-Type": "application/json",
 }
 data = {
 "model": model,
 "messages": messages,
 "stream": True,
 "options": {
 "temperature": 0.7,
 "num_predict": 1024,
 }
 }
 logger.debug(f"å¼€å§‹æµå¼å¯¹è¯ï¼š{data}")
 try:
 current_content = ""
 async for chunk in self._make_request(headers, data):
 chunk_str = chunk.decode('utf-8')
 if not chunk_str.strip():
 continue
 try:
 response = json.loads(chunk_str)
 if "message" in response and "content" in response["message"]:
 content = response["message"]["content"]
 has_think_start = "<think>" in content
 has_think_end = "</think>" in content
 if has_think_start and not has_think_end:
 current_content = content
 elif has_think_end and current_content:
 full_content = current_content + content
 has_reasoning, reasoning = self._extract_reasoning(full_content)
 if has_reasoning:
 yield "reasoning", reasoning
 current_content = ""
 elif current_content:
 current_content += content
 else:
 yield "content", content
 if response.get("done"):
 if current_content:
 has_reasoning, reasoning = self._extract_reasoning(current_content)
 if has_reasoning:
 yield "reasoning", reasoning
 return
 except json.JSONDecodeError:
 continue
 except Exception as e:
 logger.error(f"æµå¼å¯¹è¯å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
 raise
 async def get_reasoning(self, messages: list, model: str = None, **kwargs) -> AsyncGenerator[tuple[str, str], None]:
 if model is None:
 model = self.default_model
 async for content_type, content in self.stream_chat(
 messages=messages,
 model=model
 ):
 if content_type == "reasoning":
 yield content_type, content
 def _get_proxy_config(self) -> tuple[bool, str | None]:
 enable_proxy = os.getenv('OLLAMA_ENABLE_PROXY', 'false').lower() == 'true'
 if enable_proxy:
 http_proxy = os.getenv('HTTP_PROXY')
 https_proxy = os.getenv('HTTPS_PROXY')
 if https_proxy or http_proxy:
 logger.info(f"Ollama å®¢æˆ·ç«¯ä½¿ç”¨ä»£ç†: {https_proxy or http_proxy}")
 else:
 logger.warning("å·²å¯ç”¨ Ollama ä»£ç†ä½†æœªè®¾ç½®ä»£ç†åœ°å€")
 return True, https_proxy or http_proxy
 logger.debug("Ollama å®¢æˆ·ç«¯æœªå¯ç”¨ä»£ç†")
 return False, None```
______________________________

## ...\clients\__init__.py
```python
from .base_client import BaseClient
from .deepseek_client import DeepSeekClient
from .claude_client import ClaudeClient
from .ollama_r1 import OllamaR1Client
__all__ = ['BaseClient', 'DeepSeekClient', 'ClaudeClient', 'OllamaR1Client']```
______________________________

## ...\deepclaude\deepclaude.py
```python
import json
import time
import tiktoken
import asyncio
from typing import AsyncGenerator
from app.utils.logger import logger
from app.clients import DeepSeekClient, ClaudeClient, OllamaR1Client
from app.utils.message_processor import MessageProcessor
import aiohttp
import os
from dotenv import load_dotenv
load_dotenv()
class DeepClaude:
 def __init__(self, **kwargs):
 self.claude_client = ClaudeClient(
 api_key=kwargs.get('claude_api_key'),
 api_url=kwargs.get('claude_api_url'),
 provider=kwargs.get('claude_provider')
 )
 self.provider = kwargs.get('deepseek_provider', 'deepseek')
 self.reasoning_providers = {
 'deepseek': lambda: DeepSeekClient(
 api_key=kwargs.get('deepseek_api_key'),
 api_url=kwargs.get('deepseek_api_url'),
 provider=kwargs.get('deepseek_provider')
 ),
 'ollama': lambda: OllamaR1Client(
 api_url=kwargs.get('ollama_api_url')
 )
 }
 self.min_reasoning_chars = int(os.getenv('MIN_REASONING_CHARS', '50'))
 self.max_retries = int(os.getenv('REASONING_MAX_RETRIES', '2'))
 self.reasoning_modes = os.getenv('REASONING_MODE_SEQUENCE', 'auto,think_tags,early_content,any_content').split(',')
 def _get_reasoning_provider(self):
 provider = os.getenv('REASONING_PROVIDER', 'deepseek').lower()
 if provider not in self.reasoning_providers:
 raise ValueError(f"ä¸æ”¯æŒçš„æ¨ç†æä¾›è€…: {provider}")
 return self.reasoning_providers[provider]()
 async def _handle_stream_response(self, response_queue: asyncio.Queue,
 chat_id: str, created_time: int, model: str) -> AsyncGenerator[bytes, None]:
 try:
 while True:
 item = await response_queue.get()
 if item is None:
 break
 content_type, content = item
 if content_type == "reasoning":
 response = {
 "id": chat_id,
 "object": "chat.completion.chunk",
 "created": created_time,
 "model": model,
 "is_reasoning": True,
 "choices": [{
 "index": 0,
 "delta": {
 "role": "assistant",
 "content": f"ğŸ¤” æ€è€ƒè¿‡ç¨‹:\n{content}\n",
 "reasoning": True
 }
 }]
 }
 elif content_type == "content":
 response = {
 "id": chat_id,
 "object": "chat.completion.chunk",
 "created": created_time,
 "model": model,
 "choices": [{
 "index": 0,
 "delta": {
 "role": "assistant",
 "content": f"\n\n---\næ€è€ƒå®Œæ¯•ï¼Œå¼€å§‹å›ç­”ï¼š\n\n{content}"
 }
 }]
 }
 yield f"data: {json.dumps(response)}\n\n".encode('utf-8')
 except Exception as e:
 logger.error(f"å¤„ç†æµå¼å“åº”æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
 raise
 async def _handle_api_error(self, e: Exception) -> str:
 if isinstance(e, aiohttp.ClientError):
 return "ç½‘ç»œè¿æ¥é”™è¯¯ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥"
 elif isinstance(e, asyncio.TimeoutError):
 return "è¯·æ±‚è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•"
 elif isinstance(e, ValueError):
 return f"å‚æ•°é”™è¯¯: {str(e)}"
 else:
 return f"æœªçŸ¥é”™è¯¯: {str(e)}"
 async def chat_completions_with_stream(self, messages: list, **kwargs):
 try:
 logger.info("å¼€å§‹æµå¼å¤„ç†è¯·æ±‚...")
 provider = self._get_reasoning_provider()
 reasoning_content = []
 thought_complete = False
 logger.info(f"æ€è€ƒè€…æä¾›å•†: {self.provider}")
 logger.info(f"æ€è€ƒæ¨¡å¼: {os.getenv('DEEPSEEK_REASONING_MODE', 'auto')}")
 try:
 reasoning_success = False
 is_first_reasoning = True
 yield self._format_stream_response(
 "å¼€å§‹æ€è€ƒé—®é¢˜...",
 content_type="reasoning",
 is_first_thought=True,
 **kwargs
 )
 is_first_reasoning = False
 for retry_count, reasoning_mode in enumerate(self.reasoning_modes):
 if reasoning_success:
 break
 if retry_count > 0:
 logger.info(f"å°è¯•ä½¿ç”¨ä¸åŒçš„æ¨ç†æ¨¡å¼: {reasoning_mode} (å°è¯• {retry_count+1}/{len(self.reasoning_modes)})")
 os.environ["DEEPSEEK_REASONING_MODE"] = reasoning_mode
 provider = self._get_reasoning_provider()
 yield self._format_stream_response(
 f"åˆ‡æ¢æ€è€ƒæ¨¡å¼: {reasoning_mode}...",
 content_type="reasoning",
 is_first_thought=False,
 **kwargs
 )
 thinking_kwargs = self._prepare_thinker_kwargs(kwargs)
 logger.info(f"ä½¿ç”¨æ€è€ƒæ¨¡å‹: {thinking_kwargs.get('model')}")
 try:
 async for content_type, content in provider.get_reasoning(
 messages=messages,
 **thinking_kwargs
 ):
 if content_type == "reasoning":
 reasoning_content.append(content)
 if len("".join(reasoning_content)) > self.min_reasoning_chars:
 reasoning_success = True
 yield self._format_stream_response(
 content,
 content_type="reasoning",
 is_first_thought=False,
 **kwargs
 )
 elif content_type == "content":
 logger.debug(f"æ”¶åˆ°å¸¸è§„å†…å®¹: {content[:50]}...")
 thought_complete = True
 if not reasoning_success and reasoning_mode in ['early_content', 'any_content']:
 logger.info("å°†å¸¸è§„å†…å®¹è½¬åŒ–ä¸ºæ¨ç†å†…å®¹")
 reasoning_content.append(f"åˆ†æ: {content}")
 yield self._format_stream_response(
 f"åˆ†æ: {content}",
 content_type="reasoning",
 is_first_thought=False,
 **kwargs
 )
 except Exception as reasoning_e:
 logger.error(f"ä½¿ç”¨æ¨¡å¼ {reasoning_mode} è·å–æ¨ç†å†…å®¹æ—¶å‘ç”Ÿé”™è¯¯: {reasoning_e}")
 yield self._format_stream_response(
 f"æ€è€ƒæ¨¡å¼ {reasoning_mode} å¤±è´¥ï¼Œå°è¯•å…¶ä»–æ–¹å¼...",
 content_type="reasoning",
 is_first_thought=False,
 **kwargs
 )
 continue
 logger.info(f"æ€è€ƒè¿‡ç¨‹{'æˆåŠŸ' if reasoning_success else 'å¤±è´¥'}ï¼Œå…±æ”¶é›† {len(reasoning_content)} ä¸ªæ€è€ƒç‰‡æ®µ")
 except Exception as e:
 logger.error(f"æ€è€ƒé˜¶æ®µå‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
 yield self._format_stream_response(
 f"æ€è€ƒè¿‡ç¨‹å‡ºé”™: {str(e)}ï¼Œå°è¯•ç»§ç»­...",
 content_type="reasoning",
 is_first_thought=False,
 **kwargs
 )
 if not reasoning_content or len("".join(reasoning_content)) < self.min_reasoning_chars:
 logger.warning(f"æœªè·å–åˆ°è¶³å¤Ÿçš„æ€è€ƒå†…å®¹ï¼Œå½“å‰å†…å®¹é•¿åº¦: {len(''.join(reasoning_content))}")
 if not reasoning_content or len("".join(reasoning_content)) < self.min_reasoning_chars // 2:
 logger.warning("æœªè·å–åˆ°æœ‰æ•ˆæ€è€ƒå†…å®¹ï¼Œä½¿ç”¨åŸå§‹é—®é¢˜ä½œä¸ºæ›¿ä»£")
 message_content = messages[-1]['content'] if messages and isinstance(messages[-1], dict) and 'content' in messages[-1] else "æœªèƒ½è·å–é—®é¢˜å†…å®¹"
 reasoning_content = [f"é—®é¢˜åˆ†æï¼š{message_content}"]
 yield self._format_stream_response(
 "æ— æ³•è·å–æ€è€ƒè¿‡ç¨‹ï¼Œå°†ç›´æ¥å›ç­”é—®é¢˜",
 content_type="reasoning",
 is_first_thought=True,
 **kwargs
 )
 yield self._format_stream_response(
 "\n\n---\næ€è€ƒå®Œæ¯•ï¼Œå¼€å§‹å›ç­”ï¼š\n\n",
 content_type="separator",
 is_first_thought=False,
 **kwargs
 )
 full_reasoning = "\n".join(reasoning_content)
 if 'content' in messages[-1]:
 original_question = messages[-1]['content']
 else:
 logger.warning("æ— æ³•ä»æ¶ˆæ¯ä¸­è·å–é—®é¢˜å†…å®¹")
 original_question = "æœªæä¾›é—®é¢˜å†…å®¹"
 prompt = self._format_claude_prompt(
 original_question,
 full_reasoning
 )
 logger.debug(f"å‘é€ç»™Claudeçš„æç¤ºè¯: {prompt[:500]}...")
 try:
 answer_begun = False
 async for content_type, content in self.claude_client.stream_chat(
 messages=[{"role": "user", "content": prompt}],
 **self._prepare_answerer_kwargs(kwargs)
 ):
 if content_type == "content" and content:
 if not answer_begun and content.strip():
 answer_begun = True
 yield self._format_stream_response(
 content,
 content_type="content",
 is_first_thought=False,
 **kwargs
 )
 except Exception as e:
 logger.error(f"å›ç­”é˜¶æ®µå‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
 yield self._format_stream_response(
 f"\n\nâš ï¸ è·å–å›ç­”æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}",
 content_type="error",
 is_first_thought=False,
 **kwargs
 )
 except Exception as e:
 error_msg = await self._handle_api_error(e)
 logger.error(f"æµå¼å¤„ç†é”™è¯¯: {error_msg}", exc_info=True)
 yield self._format_stream_response(
 f"é”™è¯¯: {error_msg}",
 content_type="error",
 is_first_thought=False,
 **kwargs
 )
 def _prepare_thinker_kwargs(self, kwargs: dict) -> dict:
 provider_type = os.getenv('REASONING_PROVIDER', 'deepseek').lower()
 if provider_type == 'ollama':
 model = "deepseek-r1:32b"
 else:
 model = os.getenv('DEEPSEEK_MODEL', 'deepseek-reasoner')
 if self.provider == 'deepseek':
 model = 'deepseek-reasoner'
 elif self.provider == 'siliconflow':
 model = 'deepseek-ai/DeepSeek-R1'
 elif self.provider == 'nvidia':
 model = 'deepseek-ai/deepseek-r1'
 return {
 'model': model,
 'temperature': kwargs.get('temperature', 0.7),
 'top_p': kwargs.get('top_p', 0.9)
 }
 def _prepare_answerer_kwargs(self, kwargs: dict) -> dict:
 return {
 'model': 'claude-3-5-sonnet-20241022',
 'temperature': kwargs.get('temperature', 0.7),
 'top_p': kwargs.get('top_p', 0.9)
 }
 def _chunk_content(self, content: str, chunk_size: int = 3) -> list[str]:
 return [content[i:i+chunk_size] for i in range(0, len(content), chunk_size)]
 def _format_claude_prompt(self, original_question: str, reasoning: str) -> str:
 return f
 async def chat_completions_without_stream(
 self,
 messages: list,
 model_arg: tuple[float, float, float, float],
 deepseek_model: str = "deepseek-reasoner",
 claude_model: str = "claude-3-5-sonnet-20241022"
 ) -> dict:
 logger.info("å¼€å§‹å¤„ç†è¯·æ±‚...")
 logger.debug(f"è¾“å…¥æ¶ˆæ¯: {messages}")
 logger.info("æ­£åœ¨è·å–æ¨ç†å†…å®¹...")
 try:
 reasoning = await self._get_reasoning_content(
 messages=messages,
 model=deepseek_model,
 model_arg=model_arg
 )
 except Exception as e:
 logger.error(f"è·å–æ¨ç†å†…å®¹å¤±è´¥: {e}")
 reasoning = "æ— æ³•è·å–æ¨ç†å†…å®¹"
 for reasoning_mode in self.reasoning_modes[1:]:
 try:
 logger.info(f"å°è¯•ä½¿ç”¨ä¸åŒçš„æ¨ç†æ¨¡å¼è·å–å†…å®¹: {reasoning_mode}")
 os.environ["DEEPSEEK_REASONING_MODE"] = reasoning_mode
 reasoning = await self._get_reasoning_content(
 messages=messages,
 model=deepseek_model,
 model_arg=model_arg
 )
 if reasoning and len(reasoning) > self.min_reasoning_chars:
 logger.info(f"ä½¿ç”¨æ¨ç†æ¨¡å¼ {reasoning_mode} æˆåŠŸè·å–æ¨ç†å†…å®¹")
 break
 except Exception as retry_e:
 logger.error(f"ä½¿ç”¨æ¨ç†æ¨¡å¼ {reasoning_mode} é‡è¯•å¤±è´¥: {retry_e}")
 logger.debug(f"è·å–åˆ°æ¨ç†å†…å®¹: {reasoning[:min(500, len(reasoning))]}...")
 combined_content = f
 claude_messages = [{"role": "user", "content": combined_content}]
 logger.info("æ­£åœ¨è·å– Claude å›ç­”...")
 try:
 full_content = ""
 async for content_type, content in self.claude_client.stream_chat(
 messages=claude_messages,
 model_arg=model_arg,
 model=claude_model,
 stream=False
 ):
 if content_type in ["answer", "content"]:
 logger.debug(f"è·å–åˆ° Claude å›ç­”: {content}")
 full_content += content
 return {
 "content": full_content,
 "role": "assistant"
 }
 except Exception as e:
 logger.error(f"è·å– Claude å›ç­”å¤±è´¥: {e}")
 raise
 async def _get_reasoning_content(self, messages: list, model: str, **kwargs) -> str:
 try:
 provider = self._get_reasoning_provider()
 reasoning_content = []
 async for content_type, content in provider.get_reasoning(
 messages=messages,
 model=model,
 model_arg=kwargs.get('model_arg')
 ):
 if content_type == "reasoning":
 reasoning_content.append(content)
 elif content_type == "content" and not reasoning_content:
 logger.info("æœªæ”¶é›†åˆ°æ¨ç†å†…å®¹ï¼Œå°†æ™®é€šå†…å®¹è§†ä¸ºæ¨ç†")
 reasoning_content.append(f"åˆ†æ: {content}")
 result = "\n".join(reasoning_content)
 if not result or len(result) < self.min_reasoning_chars:
 current_mode = os.getenv('DEEPSEEK_REASONING_MODE', 'auto')
 logger.warning(f"ä½¿ç”¨æ¨¡å¼ {current_mode} è·å–çš„æ¨ç†å†…å®¹ä¸è¶³ï¼Œå°è¯•åˆ‡æ¢æ¨¡å¼")
 for reasoning_mode in self.reasoning_modes:
 if reasoning_mode == current_mode:
 continue
 logger.info(f"å°è¯•ä½¿ç”¨æ¨ç†æ¨¡å¼: {reasoning_mode}")
 os.environ["DEEPSEEK_REASONING_MODE"] = reasoning_mode
 provider = self._get_reasoning_provider()
 reasoning_content = []
 async for content_type, content in provider.get_reasoning(
 messages=messages,
 model=model,
 model_arg=kwargs.get('model_arg')
 ):
 if content_type == "reasoning":
 reasoning_content.append(content)
 elif content_type == "content" and not reasoning_content:
 reasoning_content.append(f"åˆ†æ: {content}")
 retry_result = "\n".join(reasoning_content)
 if retry_result and len(retry_result) > self.min_reasoning_chars:
 logger.info(f"ä½¿ç”¨æ¨ç†æ¨¡å¼ {reasoning_mode} æˆåŠŸè·å–è¶³å¤Ÿçš„æ¨ç†å†…å®¹")
 return retry_result
 return result or "æ— æ³•è·å–è¶³å¤Ÿçš„æ¨ç†å†…å®¹"
 except Exception as e:
 logger.error(f"ä¸»è¦æ¨ç†æä¾›è€…å¤±è´¥: {e}")
 if hasattr(self, 'ollama_api_url'):
 logger.info("å°è¯•åˆ‡æ¢åˆ° Ollama æ¨ç†æä¾›è€…")
 try:
 provider = OllamaR1Client(self.ollama_api_url)
 reasoning_content = []
 async for content_type, content in provider.get_reasoning(
 messages=messages,
 model="deepseek-r1:32b"
 ):
 if content_type == "reasoning":
 reasoning_content.append(content)
 return "\n".join(reasoning_content)
 except Exception as e:
 logger.error(f"å¤‡ç”¨æ¨ç†æä¾›è€…ä¹Ÿå¤±è´¥: {e}")
 return "æ— æ³•è·å–æ¨ç†å†…å®¹"
 async def _retry_operation(self, operation, max_retries=3):
 for i in range(max_retries):
 try:
 return await operation()
 except Exception as e:
 if i == max_retries - 1:
 raise
 logger.warning(f"æ“ä½œå¤±è´¥ï¼Œæ­£åœ¨é‡è¯• ({i+1}/{max_retries}): {str(e)}")
 await asyncio.sleep(1 * (i + 1))
 def _validate_model_names(self, deepseek_model: str, claude_model: str):
 if not deepseek_model or not isinstance(deepseek_model, str):
 raise ValueError("æ— æ•ˆçš„ DeepSeek æ¨¡å‹åç§°")
 if not claude_model or not isinstance(claude_model, str):
 raise ValueError("æ— æ•ˆçš„ Claude æ¨¡å‹åç§°")
 def _validate_messages(self, messages: list) -> None:
 if not messages:
 raise ValueError("æ¶ˆæ¯åˆ—è¡¨ä¸èƒ½ä¸ºç©º")
 for msg in messages:
 if not isinstance(msg, dict):
 raise ValueError("æ¶ˆæ¯å¿…é¡»æ˜¯å­—å…¸æ ¼å¼")
 if "role" not in msg or "content" not in msg:
 raise ValueError("æ¶ˆæ¯å¿…é¡»åŒ…å« role å’Œ content å­—æ®µ")
 if msg["role"] not in ["user", "assistant", "system"]:
 raise ValueError(f"ä¸æ”¯æŒçš„æ¶ˆæ¯è§’è‰²: {msg['role']}")
 async def _get_reasoning_with_fallback(
 self,
 messages: list,
 model: str,
 model_arg: tuple[float, float, float, float] = None
 ) -> str:
 try:
 provider = self._get_reasoning_provider()
 reasoning_content = []
 for reasoning_mode in self.reasoning_modes:
 if reasoning_content and len("".join(reasoning_content)) > self.min_reasoning_chars:
 break
 logger.info(f"å°è¯•ä½¿ç”¨æ¨ç†æ¨¡å¼: {reasoning_mode}")
 os.environ["DEEPSEEK_REASONING_MODE"] = reasoning_mode
 provider = self._get_reasoning_provider()
 temp_content = []
 try:
 async for content_type, content in provider.get_reasoning(
 messages=messages,
 model=model,
 model_arg=model_arg
 ):
 if content_type == "reasoning":
 temp_content.append(content)
 elif content_type == "content" and not temp_content and reasoning_mode in ['early_content', 'any_content']:
 temp_content.append(f"åˆ†æ: {content}")
 if temp_content and len("".join(temp_content)) > len("".join(reasoning_content)):
 reasoning_content = temp_content
 except Exception as mode_e:
 logger.error(f"ä½¿ç”¨æ¨ç†æ¨¡å¼ {reasoning_mode} æ—¶å‘ç”Ÿé”™è¯¯: {mode_e}")
 continue
 return "".join(reasoning_content) or "æ— æ³•è·å–æ¨ç†å†…å®¹"
 except Exception as e:
 logger.error(f"ä¸»è¦æ¨ç†æä¾›è€…å¤±è´¥: {e}")
 if isinstance(provider, DeepSeekClient):
 logger.info("å°è¯•åˆ‡æ¢åˆ° Ollama æ¨ç†æä¾›è€…")
 provider = OllamaR1Client(self.ollama_api_url)
 reasoning_content = []
 async for content_type, content in provider.get_reasoning(
 messages=messages,
 model="deepseek-r1:32b"
 ):
 if content_type == "reasoning":
 reasoning_content.append(content)
 return "".join(reasoning_content)
 raise
 def _validate_config(self):
 provider = os.getenv('REASONING_PROVIDER', 'deepseek').lower()
 if provider == 'deepseek':
 if not self.deepseek_api_key:
 raise ValueError("ä½¿ç”¨ DeepSeek æ—¶å¿…é¡»æä¾› API KEY")
 if not self.deepseek_api_url:
 raise ValueError("ä½¿ç”¨ DeepSeek æ—¶å¿…é¡»æä¾› API URL")
 elif provider == 'ollama':
 if not self.ollama_api_url:
 raise ValueError("ä½¿ç”¨ Ollama æ—¶å¿…é¡»æä¾› API URL")
 def _format_stream_response(self, content: str, content_type: str = "content", **kwargs) -> bytes:
 response = {
 "id": kwargs.get("chat_id", f"chatcmpl-{int(time.time())}"),
 "object": "chat.completion.chunk",
 "created": kwargs.get("created_time", int(time.time())),
 "model": kwargs.get("model", "deepclaude"),
 "choices": [{
 "index": 0,
 "delta": {
 "content": content
 }
 }]
 }
 if content_type == "reasoning":
 response["choices"][0]["delta"]["reasoning"] = True
 response["is_reasoning"] = True
 is_first_thought = kwargs.get("is_first_thought", False)
 if is_first_thought and not content.startswith("ğŸ¤”"):
 response["choices"][0]["delta"]["content"] = f"ğŸ¤” {content}"
 elif content_type == "separator":
 response["is_separator"] = True
 elif content_type == "error":
 response["is_error"] = True
 response["choices"][0]["delta"]["content"] = f"âš ï¸ {content}"
 return f"data: {json.dumps(response)}\n\n".encode('utf-8')
 def _validate_kwargs(self, kwargs: dict) -> None:
 temperature = kwargs.get('temperature')
 if temperature is not None:
 if not isinstance(temperature, (int, float)) or temperature < 0 or temperature > 1:
 raise ValueError("temperature å¿…é¡»åœ¨ 0 åˆ° 1 ä¹‹é—´")
 top_p = kwargs.get('top_p')
 if top_p is not None:
 if not isinstance(top_p, (int, float)) or top_p < 0 or top_p > 1:
 raise ValueError("top_p å¿…é¡»åœ¨ 0 åˆ° 1 ä¹‹é—´")
 model = kwargs.get('model')
 if model and not isinstance(model, str):
 raise ValueError("model å¿…é¡»æ˜¯å­—ç¬¦ä¸²ç±»å‹")
 def _split_into_tokens(self, text: str) -> list[str]:
 return list(text)```
______________________________

## ...\deepclaude\__init__.py
```python
```
______________________________

## ...\utils\auth.py
```python
from fastapi import HTTPException, Header
from typing import Optional
import os
from dotenv import load_dotenv
from app.utils.logger import logger
logger.info(f"å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
logger.info("å°è¯•åŠ è½½.envæ–‡ä»¶...")
load_dotenv(override=True)
ALLOW_API_KEY = os.getenv("ALLOW_API_KEY")
logger.info(f"ALLOW_API_KEYç¯å¢ƒå˜é‡çŠ¶æ€: {'å·²è®¾ç½®' if ALLOW_API_KEY else 'æœªè®¾ç½®'}")
if not ALLOW_API_KEY:
 raise ValueError("ALLOW_API_KEY environment variable is not set")
logger.info(f"Loaded API key starting with: {ALLOW_API_KEY[:4] if len(ALLOW_API_KEY) >= 4 else ALLOW_API_KEY}")
async def verify_api_key(authorization: Optional[str] = Header(None)) -> None:
 if authorization is None:
 logger.warning("è¯·æ±‚ç¼ºå°‘Authorization header")
 raise HTTPException(
 status_code=401,
 detail="Missing Authorization header"
 )
 api_key = authorization.replace("Bearer ", "").strip()
 if api_key != ALLOW_API_KEY:
 logger.warning(f"æ— æ•ˆçš„APIå¯†é’¥: {api_key}")
 raise HTTPException(
 status_code=401,
 detail="Invalid API key"
 )
 logger.info("APIå¯†é’¥éªŒè¯é€šè¿‡")```
______________________________

## ...\utils\logger.py
```python
import os
import sys
import logging
from logging.handlers import RotatingFileHandler
import inspect
LOG_LEVEL = os.getenv('LOG_LEVEL', 'INFO').upper()
class DebuggableLogger(logging.Logger):
 def debug_stream(self, data, max_length=500):
 if self.isEnabledFor(logging.DEBUG):
 data_str = str(data)
 if len(data_str) > max_length:
 data_str = data_str[:max_length] + "... [æˆªæ–­]"
 frame = inspect.currentframe().f_back
 filename = os.path.basename(frame.f_code.co_filename)
 lineno = frame.f_lineno
 self.debug(f"[{filename}:{lineno}] æµå¼æ•°æ®: {data_str}")
 def debug_response(self, response, max_length=300):
 if self.isEnabledFor(logging.DEBUG):
 resp_str = str(response)
 if len(resp_str) > max_length:
 resp_str = resp_str[:max_length] + "... [æˆªæ–­]"
 frame = inspect.currentframe().f_back
 filename = os.path.basename(frame.f_code.co_filename)
 lineno = frame.f_lineno
 self.debug(f"[{filename}:{lineno}] APIå“åº”: {resp_str}")
logging.setLoggerClass(DebuggableLogger)
logger = logging.getLogger('deepclaude')
log_level = getattr(logging, LOG_LEVEL, logging.INFO)
logger.setLevel(log_level)
console_handler = logging.StreamHandler(sys.stdout)
console_handler.setLevel(log_level)
formatter = logging.Formatter(
 '[%(asctime)s] [%(levelname)s] [%(filename)s:%(lineno)d] %(message)s',
 datefmt='%Y-%m-%d %H:%M:%S'
)
console_handler.setFormatter(formatter)
logger.addHandler(console_handler)
if os.getenv('LOG_TO_FILE', 'false').lower() == 'true':
 log_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 'logs')
 os.makedirs(log_dir, exist_ok=True)
 file_handler = RotatingFileHandler(
 os.path.join(log_dir, 'deepclaude.log'),
 maxBytes=10*1024*1024,
 backupCount=5
 )
 file_handler.setLevel(log_level)
 file_handler.setFormatter(formatter)
 logger.addHandler(file_handler)
logger.info(f"æ—¥å¿—çº§åˆ«è®¾ç½®ä¸º: {LOG_LEVEL}")
if log_level <= logging.DEBUG:
 logger.debug("è°ƒè¯•æ¨¡å¼å·²å¼€å¯")```
______________________________

## ...\utils\message_processor.py
```python
from typing import List, Dict
from app.utils.logger import logger
class MessageProcessor:
 @staticmethod
 def convert_to_deepseek_format(messages: List[Dict]) -> List[Dict]:
 processed = []
 temp_content = []
 current_role = None
 for msg in messages:
 role = msg.get("role", "")
 content = msg.get("content", "")
 if not content:
 continue
 if role == "system":
 if processed and processed[0]["role"] == "system":
 processed[0]["content"] += f"\n{content}"
 else:
 processed.insert(0, {"role": "system", "content": content})
 continue
 if role == current_role:
 temp_content.append(content)
 else:
 if temp_content:
 processed.append({
 "role": current_role,
 "content": "\n".join(temp_content)
 })
 temp_content = [content]
 current_role = role
 if temp_content:
 processed.append({
 "role": current_role,
 "content": "\n".join(temp_content)
 })
 final_messages = []
 for i, msg in enumerate(processed):
 if i > 0 and msg["role"] == final_messages[-1]["role"]:
 if msg["role"] == "user":
 final_messages.append({"role": "assistant", "content": "è¯·ç»§ç»­ã€‚"})
 else:
 final_messages.append({"role": "user", "content": "è¯·ç»§ç»­ã€‚"})
 final_messages.append(msg)
 logger.debug(f"è½¬æ¢åçš„æ¶ˆæ¯æ ¼å¼: {final_messages}")
 return final_messages
 @staticmethod
 def validate_messages(messages: List[Dict]) -> bool:
 if not messages:
 return False
 for i in range(1, len(messages)):
 if messages[i]["role"] == messages[i-1]["role"]:
 return False
 return True```
______________________________

## ...\test\test_claude_client.py
```python
import os
import sys
import asyncio
from dotenv import load_dotenv
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
sys.path.insert(0, project_root)
from app.clients.claude_client import ClaudeClient
from app.utils.logger import logger
load_dotenv()
async def test_claude_stream():
 api_key = os.getenv("CLAUDE_API_KEY")
 api_url = os.getenv("CLAUDE_API_URL", "https://api.anthropic.com/v1/messages")
 provider = os.getenv("CLAUDE_PROVIDER", "anthropic")
 logger.info(f"API URL: {api_url}")
 logger.info(f"API Key æ˜¯å¦å­˜åœ¨: {bool(api_key)}")
 logger.info(f"Provider: {provider}")
 enable_proxy = os.getenv('CLAUDE_ENABLE_PROXY', 'false').lower() == 'true'
 if enable_proxy:
 proxy = os.getenv('HTTPS_PROXY') or os.getenv('HTTP_PROXY')
 logger.info(f"ä»£ç†å·²å¯ç”¨: {proxy}")
 else:
 logger.info("ä»£ç†æœªå¯ç”¨")
 if not api_key:
 logger.error("è¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½® CLAUDE_API_KEY")
 return
 messages = [
 {"role": "user", "content": "é™µæ°´å¥½ç©å˜›?"}
 ]
 client = ClaudeClient(api_key, api_url, provider)
 try:
 logger.info("å¼€å§‹æµ‹è¯• Claude æµå¼è¾“å‡º...")
 async for content_type, content in client.stream_chat(
 messages=messages,
 model_arg=(0.7, 0.9, 0, 0),
 model="claude-3-5-sonnet-20241022"
 ):
 if content_type == "answer":
 logger.info(f"æ”¶åˆ°å›ç­”å†…å®¹: {content}")
 except Exception as e:
 logger.error(f"æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}", exc_info=True)
 logger.error(f"é”™è¯¯ç±»å‹: {type(e)}")
def main():
 asyncio.run(test_claude_stream())
if __name__ == "__main__":
 main()```
______________________________

## ...\test\test_deepclaude.py
```python
import os
async def test_reasoning_fallback():
 deepclaude = DeepClaude(...)
 messages = [{"role": "user", "content": "æµ‹è¯•é—®é¢˜"}]
 os.environ['REASONING_PROVIDER'] = 'deepseek'
 reasoning = await deepclaude._get_reasoning_with_fallback(
 messages=messages,
 model="deepseek-reasoner"
 )
 assert reasoning```
______________________________

## ...\test\test_deepseek_client.py
```python
import os
import sys
import asyncio
import argparse
from dotenv import load_dotenv
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
sys.path.insert(0, project_root)
from app.clients.deepseek_client import DeepSeekClient
from app.utils.logger import logger
def parse_args():
 parser = argparse.ArgumentParser(description='æµ‹è¯• DeepSeek å®¢æˆ·ç«¯')
 parser.add_argument('--reasoning-mode', type=str, choices=['auto', 'reasoning_field', 'think_tags', 'any_content'],
 default=os.getenv('DEEPSEEK_REASONING_MODE', 'auto'),
 help='æ¨ç†å†…å®¹æå–æ¨¡å¼')
 parser.add_argument('--provider', type=str, choices=['deepseek', 'siliconflow', 'nvidia'],
 default=os.getenv('DEEPSEEK_PROVIDER', 'deepseek'),
 help='APIæä¾›å•†')
 parser.add_argument('--model', type=str,
 default=os.getenv('DEEPSEEK_MODEL', 'deepseek-reasoner'),
 help='æ¨¡å‹åç§°')
 parser.add_argument('--question', type=str, default='1+1ç­‰äºå‡ ?',
 help='æµ‹è¯•é—®é¢˜')
 parser.add_argument('--debug', action='store_true',
 help='å¯ç”¨è°ƒè¯•æ¨¡å¼')
 return parser.parse_args()
async def test_deepseek_stream(args):
 api_key = os.getenv("DEEPSEEK_API_KEY")
 api_url = os.getenv("DEEPSEEK_API_URL", "https://api.siliconflow.cn/v1/chat/completions")
 os.environ["DEEPSEEK_REASONING_MODE"] = args.reasoning_mode
 os.environ["DEEPSEEK_PROVIDER"] = args.provider
 logger.info("=== DeepSeek å®¢æˆ·ç«¯æµ‹è¯•å¼€å§‹ ===")
 logger.info(f"API URL: {api_url}")
 logger.info(f"API Key æ˜¯å¦å­˜åœ¨: {bool(api_key)}")
 logger.info(f"æä¾›å•†: {args.provider}")
 logger.info(f"æ¨ç†æ¨¡å¼: {args.reasoning_mode}")
 logger.info(f"ä½¿ç”¨æ¨¡å‹: {args.model}")
 logger.info(f"æµ‹è¯•é—®é¢˜: {args.question}")
 if not api_key:
 logger.error("è¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½® DEEPSEEK_API_KEY")
 return
 messages = [
 {"role": "user", "content": args.question}
 ]
 client = DeepSeekClient(api_key, api_url, provider=args.provider)
 try:
 logger.info("å¼€å§‹æµ‹è¯• DeepSeek æµå¼è¾“å‡º...")
 logger.debug(f"å‘é€æ¶ˆæ¯: {messages}")
 reasoning_buffer = []
 content_buffer = []
 reasoning_count = 0
 content_count = 0
 async for content_type, content in client.get_reasoning(
 messages=messages,
 model=args.model
 ):
 if content_type == "reasoning":
 reasoning_count += 1
 reasoning_buffer.append(content)
 if len(''.join(reasoning_buffer)) >= 50 or any(p in content for p in 'ã€‚ï¼Œï¼ï¼Ÿ.!?'):
 logger.info(f"æ¨ç†è¿‡ç¨‹ï¼ˆ{reasoning_count}ï¼‰ï¼š{''.join(reasoning_buffer)}")
 reasoning_buffer = []
 elif content_type == "content":
 content_count += 1
 content_buffer.append(content)
 if len(''.join(content_buffer)) >= 50 or any(p in content for p in 'ã€‚ï¼Œï¼ï¼Ÿ.!?'):
 logger.info(f"æ™®é€šå†…å®¹ï¼ˆ{content_count}ï¼‰ï¼š{''.join(content_buffer)}")
 content_buffer = []
 if reasoning_buffer:
 logger.info(f"æ¨ç†è¿‡ç¨‹ï¼ˆæœ€ç»ˆï¼‰ï¼š{''.join(reasoning_buffer)}")
 if content_buffer:
 logger.info(f"æ™®é€šå†…å®¹ï¼ˆæœ€ç»ˆï¼‰ï¼š{''.join(content_buffer)}")
 logger.info(f"æµ‹è¯•å®Œæˆ - æ”¶åˆ° {reasoning_count} ä¸ªæ¨ç†ç‰‡æ®µï¼Œ{content_count} ä¸ªæ™®é€šå†…å®¹ç‰‡æ®µ")
 if reasoning_count == 0:
 logger.warning("æœªæ”¶åˆ°ä»»ä½•æ¨ç†å†…å®¹ï¼è¯·æ£€æŸ¥ä»¥ä¸‹è®¾ç½®:")
 logger.warning(f"1. æ¨ç†æ¨¡å¼æ˜¯å¦æ­£ç¡®ï¼š{args.reasoning_mode}")
 logger.warning(f"2. APIæä¾›å•† {args.provider} æ˜¯å¦æ”¯æŒæ¨ç†åŠŸèƒ½")
 logger.warning(f"3. æ¨¡å‹ {args.model} æ˜¯å¦æ”¯æŒæ¨ç†è¾“å‡º")
 logger.info("=== DeepSeek å®¢æˆ·ç«¯æµ‹è¯•å®Œæˆ ===")
 except Exception as e:
 logger.error(f"æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}", exc_info=True)
 logger.error(f"é”™è¯¯ç±»å‹: {type(e)}")
def main():
 args = parse_args()
 if args.debug:
 os.environ['LOG_LEVEL'] = 'DEBUG'
 load_dotenv()
 asyncio.run(test_deepseek_stream(args))
if __name__ == "__main__":
 main()```
______________________________

## ...\test\test_nvidia_deepseek.py
```python
import os
import sys
import asyncio
from dotenv import load_dotenv
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
sys.path.insert(0, project_root)
from app.clients.deepseek_client import DeepSeekClient
from app.utils.logger import logger
load_dotenv()
async def test_nvidia_deepseek_stream():
 api_key = os.getenv("DEEPSEEK_API_KEY")
 api_url = os.getenv("DEEPSEEK_API_URL")
 logger.info("=== NVIDIA DeepSeek å®¢æˆ·ç«¯æµ‹è¯•å¼€å§‹ ===")
 logger.info(f"API URL: {api_url}")
 logger.info(f"API Key æ˜¯å¦å­˜åœ¨: {bool(api_key)}")
 if not api_key:
 logger.error("è¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½® DEEPSEEK_API_KEY")
 return
 messages = [
 {"role": "user", "content": "Which number is larger, 9.11 or 9.8?"}
 ]
 client = DeepSeekClient(
 api_key=api_key,
 api_url=api_url,
 provider="nvidia"
 )
 try:
 logger.info("å¼€å§‹æµ‹è¯• NVIDIA DeepSeek æµå¼è¾“å‡º...")
 logger.debug(f"å‘é€æ¶ˆæ¯: {messages}")
 reasoning_buffer = []
 content_buffer = []
 async for content_type, content in client.stream_chat(
 messages=messages,
 model="deepseek-ai/deepseek-r1"
 ):
 if content_type == "reasoning":
 reasoning_buffer.append(content)
 if len(''.join(reasoning_buffer)) >= 50 or any(p in content for p in 'ã€‚ï¼Œï¼ï¼Ÿ.!?'):
 logger.debug(f"æ¨ç†è¿‡ç¨‹ï¼š{''.join(reasoning_buffer)}")
 reasoning_buffer = []
 elif content_type == "content":
 content_buffer.append(content)
 if len(''.join(content_buffer)) >= 50 or any(p in content for p in 'ã€‚ï¼Œï¼ï¼Ÿ.!?'):
 logger.info(f"æœ€ç»ˆç­”æ¡ˆï¼š{''.join(content_buffer)}")
 content_buffer = []
 if reasoning_buffer:
 logger.debug(f"æ¨ç†è¿‡ç¨‹ï¼š{''.join(reasoning_buffer)}")
 if content_buffer:
 logger.info(f"æœ€ç»ˆç­”æ¡ˆï¼š{''.join(content_buffer)}")
 logger.info("=== NVIDIA DeepSeek å®¢æˆ·ç«¯æµ‹è¯•å®Œæˆ ===")
 except Exception as e:
 logger.error(f"æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}", exc_info=True)
def main():
 asyncio.run(test_nvidia_deepseek_stream())
if __name__ == "__main__":
 main()```
______________________________

## ...\test\test_ollama_r1.py
```python
import os
import sys
import asyncio
from dotenv import load_dotenv
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
sys.path.insert(0, project_root)
from app.clients.ollama_r1 import OllamaR1Client
from app.utils.logger import logger
load_dotenv()
os.environ['LOG_LEVEL'] = 'DEBUG'
async def test_ollama_stream():
 api_url = os.getenv("OLLAMA_API_URL", "http://192.168.100.81:11434/api/chat")
 logger.info(f"API URL: {api_url}")
 client = OllamaR1Client(api_url)
 try:
 messages = [
 {"role": "user", "content": "9.9å’Œ9.11è°å¤§?"}
 ]
 logger.info("å¼€å§‹æµ‹è¯• Ollama R1 æµå¼è¾“å‡º...")
 logger.debug(f"å‘é€æ¶ˆæ¯: {messages}")
 async for msg_type, content in client.stream_chat(messages):
 if msg_type == "reasoning":
 logger.info(f"æ¨ç†è¿‡ç¨‹: {content}")
 else:
 logger.info(f"æœ€ç»ˆç­”æ¡ˆ: {content}")
 except Exception as e:
 logger.error(f"æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
 raise
async def test_ollama_connection():
 api_url = os.getenv("OLLAMA_API_URL")
 assert api_url, "OLLAMA_API_URL æœªè®¾ç½®"
 client = OllamaR1Client(api_url)
 messages = [{"role": "user", "content": "æµ‹è¯•è¿æ¥"}]
 try:
 async for _, _ in client.stream_chat(messages):
 pass
 return True
 except Exception as e:
 logger.error(f"Ollama è¿æ¥æµ‹è¯•å¤±è´¥: {e}")
 return False
if __name__ == "__main__":
 asyncio.run(test_ollama_stream())```
______________________________

# é…ç½®æ–‡ä»¶

## .\Dockerfile
```dockerfile
# ä½¿ç”¨ Python 3.11 slim ç‰ˆæœ¬ä½œä¸ºåŸºç¡€é•œåƒ
# slimç‰ˆæœ¬æ˜¯ä¸€ä¸ªè½»é‡çº§çš„Pythoné•œåƒï¼ŒåªåŒ…å«è¿è¡ŒPythonåº”ç”¨æ‰€å¿…éœ€çš„ç»„ä»¶
# ç›¸æ¯”å®Œæ•´ç‰ˆé•œåƒä½“ç§¯æ›´å°ï¼Œæ›´é€‚åˆéƒ¨ç½²ç”Ÿäº§ç¯å¢ƒ
FROM python:3.11-slim

# è®¾ç½®å·¥ä½œç›®å½•
# åœ¨å®¹å™¨å†…åˆ›å»º/appç›®å½•å¹¶å°†å…¶è®¾ç½®ä¸ºå·¥ä½œç›®å½•
# åç»­çš„æ“ä½œï¼ˆå¦‚COPYï¼‰å¦‚æœä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼Œéƒ½ä¼šåŸºäºè¿™ä¸ªç›®å½•
WORKDIR /app

# è®¾ç½®ç¯å¢ƒå˜é‡
# PYTHONUNBUFFERED=1ï¼šç¡®ä¿Pythonçš„è¾“å‡ºä¸ä¼šè¢«ç¼“å­˜ï¼Œå®æ—¶è¾“å‡ºæ—¥å¿—
# PYTHONDONTWRITEBYTECODE=1ï¼šé˜²æ­¢Pythonå°†pycæ–‡ä»¶å†™å…¥ç£ç›˜
ENV PYTHONUNBUFFERED=1 \
 PYTHONDONTWRITEBYTECODE=1

# å®‰è£…é¡¹ç›®ä¾èµ–åŒ…
# --no-cache-dirï¼šä¸ç¼“å­˜pipä¸‹è½½çš„åŒ…ï¼Œå‡å°‘é•œåƒå¤§å°
# æŒ‡å®šç²¾ç¡®çš„ç‰ˆæœ¬å·ä»¥ç¡®ä¿æ„å»ºçš„ä¸€è‡´æ€§å’Œå¯é‡ç°æ€§
# aiohttp: ç”¨äºå¼‚æ­¥HTTPè¯·æ±‚
# colorlog: ç”¨äºå½©è‰²æ—¥å¿—è¾“å‡º
# fastapi: Webæ¡†æ¶
# python-dotenv: ç”¨äºåŠ è½½.envç¯å¢ƒå˜é‡
# tiktoken: OpenAIçš„åˆ†è¯å™¨
# uvicorn: ASGIæœåŠ¡å™¨
RUN pip install --no-cache-dir \
 aiohttp==3.11.11 \
 colorlog==6.9.0 \
 fastapi==0.115.8 \
 python-dotenv==1.0.1 \
 tiktoken==0.8.0 \
 "uvicorn[standard]"

# å¤åˆ¶é¡¹ç›®æ–‡ä»¶
# å°†æœ¬åœ°çš„./appç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶å¤åˆ¶åˆ°å®¹å™¨ä¸­çš„/app/appç›®å½•
COPY ./app ./app

# æš´éœ²ç«¯å£
# å£°æ˜å®¹å™¨å°†ä½¿ç”¨1124ç«¯å£
EXPOSE 1124

# python -m uvicornï¼šé€šè¿‡Pythonæ¨¡å—çš„æ–¹å¼å¯åŠ¨uvicornæœåŠ¡å™¨
# app.main:appï¼šæŒ‡å®šFastAPIåº”ç”¨çš„å¯¼å…¥è·¯å¾„ï¼Œæ ¼å¼ä¸º"æ¨¡å—è·¯å¾„:åº”ç”¨å®ä¾‹å˜é‡å"
# --host 0.0.0.0ï¼šå…è®¸æ¥è‡ªä»»ä½•IPçš„è®¿é—®ï¼ˆä¸ä»…ä»…æ˜¯localhostï¼‰
# --port 1124ï¼šæŒ‡å®šæœåŠ¡å™¨ç›‘å¬çš„ç«¯å£å·
CMD ["python", "-m", "uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "1124"]
```
______________________________
